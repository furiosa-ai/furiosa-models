{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Furiosa Models Furiosa Models provides DNN models including quantized pre-trained weights, model metadata, and runtime configurations for FuriosaAI SDK and NPU. Basically, all models are specifically optimized for FuriosaAI NPU, but the models are based on standard ONNX format. You can feel free to use all models for even CPU and GPU. Available Models Model Task Size Accuracy Latency (NPU) Latency (CPU) ResNet50 Image Classification 25M 76.002% SSDMobileNet Object Detection 7.2M mAP 0.228 SSDResNet34 Object Detection 20M mAP 0.220 YOLOv5M Object Detection 21M mAP 0.280 YOLOv5L Object Detection 46M mAP 0.295 Installation You can quickly install Furiosa Models by using pip as following: pip install 'furiosa-sdk[models]' Or you can build from the source code as following: git clone https://github.com/furiosa-ai/furiosa-models pip install . Usage You can simply access each model as following: from furiosa.models.vision import ResNet50 model = ResNet50 . load () Each model in available models also provides the details including how to access the model, input and output tensors, and pre/post processings. If you want to learn more about Furiosa SDK, you can refer to Furiosa SDK - Tutorial and Code Examples Also, you can learn about Blocking and Non-blocking APIs if you want to access the models with Asynchronous (AsyncIO) client library. See Also Furiosa Models - Github Furiosa SDK Documentation","title":"Overview"},{"location":"#furiosa-models","text":"Furiosa Models provides DNN models including quantized pre-trained weights, model metadata, and runtime configurations for FuriosaAI SDK and NPU. Basically, all models are specifically optimized for FuriosaAI NPU, but the models are based on standard ONNX format. You can feel free to use all models for even CPU and GPU.","title":"Furiosa Models"},{"location":"#available-models","text":"Model Task Size Accuracy Latency (NPU) Latency (CPU) ResNet50 Image Classification 25M 76.002% SSDMobileNet Object Detection 7.2M mAP 0.228 SSDResNet34 Object Detection 20M mAP 0.220 YOLOv5M Object Detection 21M mAP 0.280 YOLOv5L Object Detection 46M mAP 0.295","title":"Available Models"},{"location":"#installation","text":"You can quickly install Furiosa Models by using pip as following: pip install 'furiosa-sdk[models]' Or you can build from the source code as following: git clone https://github.com/furiosa-ai/furiosa-models pip install .","title":"Installation"},{"location":"#usage","text":"You can simply access each model as following: from furiosa.models.vision import ResNet50 model = ResNet50 . load () Each model in available models also provides the details including how to access the model, input and output tensors, and pre/post processings. If you want to learn more about Furiosa SDK, you can refer to Furiosa SDK - Tutorial and Code Examples Also, you can learn about Blocking and Non-blocking APIs if you want to access the models with Asynchronous (AsyncIO) client library.","title":"Usage"},{"location":"#see-also","text":"Furiosa Models - Github Furiosa SDK Documentation","title":"See Also"},{"location":"blocking_and_nonblocking_api/","text":"Blocking and Non-Blocking API Furiosa Model offers both blocking and non-blocking API for fetching models. Since the sizes of model images vary from tens to hundreds megabytes, fetching a single model takes some time (usually few seconds). If you fetch models within asynchronous code, you need to handle this kind of blocking calls. In the case, you can use Furiosa Model's non-blocking API. Blocking example from furiosa.models.vision import ResNet50 from furiosa.registry import Model model : Model = ResNet50 . load () print ( model . name ) print ( model . format ) print ( model . metadata . description ) Non-blocking example To use non-blocking API, you just need to import furiosa.models.vision.nonblocking instead of furiosa.models.vision . Creating your model should be executed within asyncio.run() block or async function with await keyword. All other API of models is the same as blocking API. import asyncio from furiosa.models.vision import ResNet50 from furiosa.registry import Model model : Model = asyncio . run ( ResNet50 . load_async ()) print ( model . name ) print ( model . format ) print ( model . metadata . description )","title":"Blocking and Non-Blocking API"},{"location":"blocking_and_nonblocking_api/#blocking-and-non-blocking-api","text":"Furiosa Model offers both blocking and non-blocking API for fetching models. Since the sizes of model images vary from tens to hundreds megabytes, fetching a single model takes some time (usually few seconds). If you fetch models within asynchronous code, you need to handle this kind of blocking calls. In the case, you can use Furiosa Model's non-blocking API.","title":"Blocking and Non-Blocking API"},{"location":"blocking_and_nonblocking_api/#blocking-example","text":"from furiosa.models.vision import ResNet50 from furiosa.registry import Model model : Model = ResNet50 . load () print ( model . name ) print ( model . format ) print ( model . metadata . description )","title":"Blocking example"},{"location":"blocking_and_nonblocking_api/#non-blocking-example","text":"To use non-blocking API, you just need to import furiosa.models.vision.nonblocking instead of furiosa.models.vision . Creating your model should be executed within asyncio.run() block or async function with await keyword. All other API of models is the same as blocking API. import asyncio from furiosa.models.vision import ResNet50 from furiosa.registry import Model model : Model = asyncio . run ( ResNet50 . load_async ()) print ( model . name ) print ( model . format ) print ( model . metadata . description )","title":"Non-blocking example"},{"location":"changelog/","text":"Changelog All notable changes to this project will be documented in this file. The format is based on keep a changelog . [Unreleased] [0.0.3] Added Changed Furiosa Model now uses blocking API by default instead of non-blocking API [0.0.2] [0.0.1]","title":"Changelog"},{"location":"changelog/#changelog","text":"All notable changes to this project will be documented in this file. The format is based on keep a changelog .","title":"Changelog"},{"location":"changelog/#unreleased","text":"","title":"[Unreleased]"},{"location":"changelog/#003","text":"","title":"[0.0.3]"},{"location":"changelog/#added","text":"","title":"Added"},{"location":"changelog/#changed","text":"Furiosa Model now uses blocking API by default instead of non-blocking API","title":"Changed"},{"location":"changelog/#002","text":"","title":"[0.0.2]"},{"location":"changelog/#001","text":"","title":"[0.0.1]"},{"location":"native_postprocessor/","text":"Some models include the native post-processing implementations, optimized for FuriosaAI Warboy and Intel/AMD CPUs. Basically, furiosa-models includes pre/post-processing implementation in Python for each model. They are reference implementations and can work with CPU and other accelerators like GPU. The native post processor is implemented in Rust and C++, and works with only FuriosaAI NPU. The implementation is designed to leverage FuriosaAI NPU's characteristics even for post-processing and maximize the latency and throughput by using the characteristics of modern CPU architecture, such as CPU cache, SIMD instructions and CPU pipelining. Table 1. Models that support native-postprocessors and their benchmark Model Latency (Python) Latency (Native) ResNet50 SSDMobileNet SSDResNet34 Usage To use native post processor, please pass use_native_post=True when a model is initialized. After then, you need to initialize NativePostProcessor . To evaluate the postprocessing results, please call NativePostProcessor.eval() . The following is an example to use native post processor for SSDMobileNet . from furiosa.models.vision import SSDMobileNet from furiosa.models.vision.ssd_mobilenet import NativePostProcessor , preprocess from furiosa.runtime import session model = SSDMobileNet . load ( use_native_post = True ) postprocessor = NativePostProcessor ( model ) with session . create ( model . enf ) as sess : image , context = preprocess ([ \"tests/assets/cat.jpg\" ]) output = sess . run ( image ) . numpy () postprocessor . eval ( output , context = context [ 0 ])","title":"NPU-optimized Postprocessor"},{"location":"native_postprocessor/#usage","text":"To use native post processor, please pass use_native_post=True when a model is initialized. After then, you need to initialize NativePostProcessor . To evaluate the postprocessing results, please call NativePostProcessor.eval() . The following is an example to use native post processor for SSDMobileNet . from furiosa.models.vision import SSDMobileNet from furiosa.models.vision.ssd_mobilenet import NativePostProcessor , preprocess from furiosa.runtime import session model = SSDMobileNet . load ( use_native_post = True ) postprocessor = NativePostProcessor ( model ) with session . create ( model . enf ) as sess : image , context = preprocess ([ \"tests/assets/cat.jpg\" ]) output = sess . run ( image ) . numpy () postprocessor . eval ( output , context = context [ 0 ])","title":"Usage"},{"location":"models/resnet50_v1.5/","text":"ResNet50 v1.5 ResNet50 v1.5 backbone model trained on ImageNet (224x224). This model has been used since MLCommons v0.5. Usage Using Furiosa SDK from furiosa.models.vision import ResNet50 , resnet50 from furiosa.runtime import session model = ResNet50 . load () with session . create ( model . enf ) as sess : image = resnet50 . preprocess ( \"tests/assets/cat.jpg\" ) output = sess . run ( image ) . numpy () resnet50 . postprocess ( output ) Model inputs The input is a 3-channel image of 224x224 (height, width). Data Type: numpy.float32 Tensor Shape: [1, 3, 224, 224] Memory Layout: NCHW Optimal Batch Size: <= 8 Outputs The output is a numpy.float32 tensor with the shape ( [1,] ), including a class id. postprocess() can transform the class id to a single label. Source This model is originated from ResNet50 v1.5 in ONNX available at MLCommons - Supported Models .","title":"ResNet50 v1.5"},{"location":"models/resnet50_v1.5/#resnet50-v15","text":"ResNet50 v1.5 backbone model trained on ImageNet (224x224). This model has been used since MLCommons v0.5.","title":"ResNet50 v1.5"},{"location":"models/resnet50_v1.5/#usage","text":"","title":"Usage"},{"location":"models/resnet50_v1.5/#using-furiosa-sdk","text":"from furiosa.models.vision import ResNet50 , resnet50 from furiosa.runtime import session model = ResNet50 . load () with session . create ( model . enf ) as sess : image = resnet50 . preprocess ( \"tests/assets/cat.jpg\" ) output = sess . run ( image ) . numpy () resnet50 . postprocess ( output )","title":"Using Furiosa SDK"},{"location":"models/resnet50_v1.5/#model-inputs","text":"The input is a 3-channel image of 224x224 (height, width). Data Type: numpy.float32 Tensor Shape: [1, 3, 224, 224] Memory Layout: NCHW Optimal Batch Size: <= 8","title":"Model inputs"},{"location":"models/resnet50_v1.5/#outputs","text":"The output is a numpy.float32 tensor with the shape ( [1,] ), including a class id. postprocess() can transform the class id to a single label.","title":"Outputs"},{"location":"models/resnet50_v1.5/#source","text":"This model is originated from ResNet50 v1.5 in ONNX available at MLCommons - Supported Models .","title":"Source"},{"location":"models/ssd_mobilenet/","text":"SSD MobileNet v1 SSD MobileNet v1 backbone model trained on COCO (300x300). This model has been used since MLCommons v0.5. Usage Using Furiosa SDK from furiosa.models.vision import SSDMobileNet from furiosa.models.vision.ssd_mobilenet import postprocess , preprocess from furiosa.runtime import session model = SSDMobileNet . load () with session . create ( model . enf ) as sess : image , context = preprocess ([ \"tests/assets/cat.jpg\" ]) output = sess . run ( image ) . numpy () postprocess ( output , batch_preproc_params = context ) NPU-optimized postprocessor support This model supports the NPU-optimized post-processing implementation. To learn more about this, please read NPU-optimized Postprocessor . A usage example of native postprocessor for SSD MobileNet from furiosa.models.vision import SSDMobileNet from furiosa.models.vision.ssd_mobilenet import NativePostProcessor , preprocess from furiosa.runtime import session model = SSDMobileNet . load ( use_native_post = True ) postprocessor = NativePostProcessor ( model ) with session . create ( model . enf ) as sess : image , context = preprocess ([ \"tests/assets/cat.jpg\" ]) output = sess . run ( image ) . numpy () postprocessor . eval ( output , context = context [ 0 ]) Model inputs The input is a 3-channel image of 300x300 (height, width). Data Type: numpy.float32 Tensor Shape: [1, 3, 300, 300] Memory Layout: NCHW Optimal Batch Size: <= 8 Outputs The outputs are 12 numpy.float32 tensors in various shapes as the following. You can refer to postprocess() function to learn how to decode boxes, classes, and confidence scores. Tensor Shape Data Type Data Type Description 0 (1, 273, 19, 19) float32 NCHW 1 (1, 12, 19, 19) float32 NCHW 2 (1, 546, 10, 10) float32 NCHW 3 (1, 24, 10, 10) float32 NCHW 4 (1, 546, 5, 5) float32 NCHW 5 (1, 24, 5, 5) float32 NCHW 6 (1, 546, 3, 3) float32 NCHW 7 (1, 24, 3, 3) float32 NCHW 8 (1, 546, 2, 2) float32 NCHW 9 (1, 24, 2, 2) float32 NCHW 10 (1, 546, 1, 1) float32 NCHW 11 (1, 24, 1, 1) float32 NCHW Source This model is originated from SSD MobileNet v1 in ONNX available at MLCommons - Supported Models .","title":"SSD MobileNet v1"},{"location":"models/ssd_mobilenet/#ssd-mobilenet-v1","text":"SSD MobileNet v1 backbone model trained on COCO (300x300). This model has been used since MLCommons v0.5.","title":"SSD MobileNet v1"},{"location":"models/ssd_mobilenet/#usage","text":"","title":"Usage"},{"location":"models/ssd_mobilenet/#using-furiosa-sdk","text":"from furiosa.models.vision import SSDMobileNet from furiosa.models.vision.ssd_mobilenet import postprocess , preprocess from furiosa.runtime import session model = SSDMobileNet . load () with session . create ( model . enf ) as sess : image , context = preprocess ([ \"tests/assets/cat.jpg\" ]) output = sess . run ( image ) . numpy () postprocess ( output , batch_preproc_params = context )","title":"Using Furiosa SDK"},{"location":"models/ssd_mobilenet/#npu-optimized-postprocessor-support","text":"This model supports the NPU-optimized post-processing implementation. To learn more about this, please read NPU-optimized Postprocessor . A usage example of native postprocessor for SSD MobileNet from furiosa.models.vision import SSDMobileNet from furiosa.models.vision.ssd_mobilenet import NativePostProcessor , preprocess from furiosa.runtime import session model = SSDMobileNet . load ( use_native_post = True ) postprocessor = NativePostProcessor ( model ) with session . create ( model . enf ) as sess : image , context = preprocess ([ \"tests/assets/cat.jpg\" ]) output = sess . run ( image ) . numpy () postprocessor . eval ( output , context = context [ 0 ])","title":"NPU-optimized postprocessor support"},{"location":"models/ssd_mobilenet/#model-inputs","text":"The input is a 3-channel image of 300x300 (height, width). Data Type: numpy.float32 Tensor Shape: [1, 3, 300, 300] Memory Layout: NCHW Optimal Batch Size: <= 8","title":"Model inputs"},{"location":"models/ssd_mobilenet/#outputs","text":"The outputs are 12 numpy.float32 tensors in various shapes as the following. You can refer to postprocess() function to learn how to decode boxes, classes, and confidence scores. Tensor Shape Data Type Data Type Description 0 (1, 273, 19, 19) float32 NCHW 1 (1, 12, 19, 19) float32 NCHW 2 (1, 546, 10, 10) float32 NCHW 3 (1, 24, 10, 10) float32 NCHW 4 (1, 546, 5, 5) float32 NCHW 5 (1, 24, 5, 5) float32 NCHW 6 (1, 546, 3, 3) float32 NCHW 7 (1, 24, 3, 3) float32 NCHW 8 (1, 546, 2, 2) float32 NCHW 9 (1, 24, 2, 2) float32 NCHW 10 (1, 546, 1, 1) float32 NCHW 11 (1, 24, 1, 1) float32 NCHW","title":"Outputs"},{"location":"models/ssd_mobilenet/#source","text":"This model is originated from SSD MobileNet v1 in ONNX available at MLCommons - Supported Models .","title":"Source"},{"location":"models/ssd_resnet34/","text":"SSD ResNet34 SSD ResNet34 backbone model trained on COCO (1200x1200). This model has been used since MLCommons v0.5. Usage Using Furiosa SDK Model inputs The input is a 3-channel image of 300x300 (height, width). Data Type: numpy.float32 Tensor Shape: [1, 3, 1200, 1200] Memory Layout: NCHW Optimal Batch Size: <= 1 Outputs The outputs are 12 numpy.float32 tensors in various shapes as the following. You can refer to postprocess() function to learn how to decode boxes, classes, and confidence scores. Tensor Shape Data Type Data Type Description 0 (1, 324, 50, 50) float32 NCHW 1 (1, 486, 25, 25) float32 NCHW 2 (1, 486, 13, 13) float32 NCHW 3 (1, 486, 7, 7) float32 NCHW 4 (1, 324, 3, 3) float32 NCHW 5 (1, 324, 3, 3) float32 NCHW 6 (1, 16, 50, 50) float32 NCHW 7 (1, 24, 25, 25) float32 NCHW 8 (1, 24, 13, 13) float32 NCHW 9 (1, 24, 7, 7) float32 NCHW 10 (1, 16, 3, 3) float32 NCHW 11 (1, 16, 3, 3) float32 NCHW Source This model is originated from SSD ResNet34 in ONNX available at MLCommons - Supported Models .","title":"SSD ResNet34"},{"location":"models/ssd_resnet34/#ssd-resnet34","text":"SSD ResNet34 backbone model trained on COCO (1200x1200). This model has been used since MLCommons v0.5.","title":"SSD ResNet34"},{"location":"models/ssd_resnet34/#usage","text":"","title":"Usage"},{"location":"models/ssd_resnet34/#using-furiosa-sdk","text":"","title":"Using Furiosa SDK"},{"location":"models/ssd_resnet34/#model-inputs","text":"The input is a 3-channel image of 300x300 (height, width). Data Type: numpy.float32 Tensor Shape: [1, 3, 1200, 1200] Memory Layout: NCHW Optimal Batch Size: <= 1","title":"Model inputs"},{"location":"models/ssd_resnet34/#outputs","text":"The outputs are 12 numpy.float32 tensors in various shapes as the following. You can refer to postprocess() function to learn how to decode boxes, classes, and confidence scores. Tensor Shape Data Type Data Type Description 0 (1, 324, 50, 50) float32 NCHW 1 (1, 486, 25, 25) float32 NCHW 2 (1, 486, 13, 13) float32 NCHW 3 (1, 486, 7, 7) float32 NCHW 4 (1, 324, 3, 3) float32 NCHW 5 (1, 324, 3, 3) float32 NCHW 6 (1, 16, 50, 50) float32 NCHW 7 (1, 24, 25, 25) float32 NCHW 8 (1, 24, 13, 13) float32 NCHW 9 (1, 24, 7, 7) float32 NCHW 10 (1, 16, 3, 3) float32 NCHW 11 (1, 16, 3, 3) float32 NCHW","title":"Outputs"},{"location":"models/ssd_resnet34/#source","text":"This model is originated from SSD ResNet34 in ONNX available at MLCommons - Supported Models .","title":"Source"}]}