{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Furiosa Models","text":"<p><code>furiosa-models</code> is an open model zoo project for FuriosaAI NPU. It provides a set of public pre-trained, pre-quantized models for learning and demo purposes or for developing your applications.</p> <p><code>furiosa-models</code> also includes pre-packaged post/processing utilities, compiler configurations optimized for FuriosaAI NPU. However, all models are standard ONNX or tflite models, and they can run even on CPU and GPU as well.</p>"},{"location":"#releases","title":"Releases","text":"<ul> <li>v0.8.0 - 2022-11-10</li> </ul>"},{"location":"#online-documentation","title":"Online Documentation","text":"<p>If you are new, you can start from Getting Started. You can also find the latest online documents,  including programming guides, API references, and examples from the followings:</p> <ul> <li>Furiosa Models - Latest Documentation</li> <li>Model object</li> <li>Model List</li> <li>Command Tool</li> <li>Furiosa SDK - Tutorial and Code Examples</li> </ul>"},{"location":"#model-list","title":"Model List","text":"<p>The table summarizes all models available in <code>furiosa-models</code>. If you visit each model link,  you can find details about loading a model, their input and output tensors, pre/post processings, and usage examples.</p> Model Task Size Accuracy ResNet50 Image Classification 25M 76.002% (ImageNet1K-val) SSDMobileNet Object Detection 7.2M mAP 0.228 (COCO 2017-val) SSDResNet34 Object Detection 20M mAP 0.220 (COCO 2017-val) YOLOv5M Object Detection 21M mAP 0.280 (Bdd100k-val)* YOLOv5L Object Detection 46M mAP 0.295 (Bdd100k-val)* <p>*: The accuracy of the yolov5 f32 model trained with bdd100k-val dataset, is mAP 0.295 (for yolov5m) and mAP 0.316 (for yolov5l).</p>"},{"location":"#see-also","title":"See Also","text":"<ul> <li>Furiosa Models - Latest Documentation</li> <li>Furiosa Models - Github</li> <li>Furiosa SDK Documentation</li> </ul>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#unreleased","title":"[unreleased]","text":""},{"location":"changelog/#added","title":"Added","text":""},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li>Changed packaging tool from setuptools-rust to hatch #102</li> </ul>"},{"location":"changelog/#deprecated","title":"Deprecated","text":""},{"location":"changelog/#removed","title":"Removed","text":"<ul> <li>MLPerf postprocess rust bindings #102</li> <li>Cpp postprocessor(s) #102</li> </ul>"},{"location":"changelog/#fixed","title":"Fixed","text":""},{"location":"changelog/#080-2022-11-10","title":"[0.8.0 - 2022-11-10]","text":""},{"location":"changelog/#new-features","title":"New Features","text":"<ul> <li>Add ResNet50 model</li> <li>Add SSD ResNet34 model</li> <li>Add SSD MobileNet model</li> <li>Add YOLOv5l model</li> <li>Add YOLOv5m model</li> </ul>"},{"location":"changelog/#improvements","title":"Improvements","text":"<ul> <li>Add native postprocessing implementation for ResNet50 #42</li> <li>Add native postprocessing implementation for SSD ResNet34 #45</li> <li>Add native postprocessing implementation for SSD MobileNet #16</li> <li>Refactor Model API to use classmethods to load models #66</li> <li>Make Model Zoo's ABC Model #78</li> <li>Retain only necessary python dependencies #81</li> <li>Improve enf_generator.sh and add enf, dfg models for e2e tests #33</li> <li>Add mkdocstrings to make API references #22</li> </ul>"},{"location":"changelog/#tasks","title":"Tasks","text":"<ul> <li>Regresstion Test #61</li> <li>Attach Tekton CI #41</li> <li>Yolov5 L/M e2e testing code #22</li> <li>Change the documentation layout and add resnet34, resnet50, mobilenet details #33</li> <li>Replace maturin with setuptools-rust #26</li> </ul>"},{"location":"changelog/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Resolve DVC directly #80</li> </ul>"},{"location":"command_tool/","title":"Command Tool","text":"<p>We provide a simple command line too called <code>furiosa-models</code> to allow users to  evaluate or run quickly one of models with FuriosaAI NPU. </p>"},{"location":"command_tool/#installing","title":"Installing","text":"<p>To install <code>furiosa-models</code> command, please refer to Installing.  Then, <code>furiosa-models</code> command will be available.</p>"},{"location":"command_tool/#synopsis","title":"Synopsis","text":"<pre><code>furiosa-models [-h] {list, desc, bench} ...\n</code></pre> <p><code>furiosa-models</code> command has three subcommands: <code>list</code>, <code>desc</code>, and <code>bench</code>.</p>"},{"location":"command_tool/#subcommand-list","title":"Subcommand: list","text":"<p><code>list</code> subcommand prints out the list of models with attributes.  You will be able to figure out what models are available. </p> <p>Example <pre><code>$ furiosa-models list\n\n+--------------+------------------------------+----------------------+-------------------------+\n|  Model name  |      Model description       |      Task type       | Available postprocesses |\n+--------------+------------------------------+----------------------+-------------------------+\n|   ResNet50   |   MLCommons ResNet50 model   | Image Classification |      Python, Rust       |\n| SSDMobileNet | MLCommons MobileNet v1 model |   Object Detection   |    Python, Rust, Cpp    |\n| SSDResNet34  | MLCommons SSD ResNet34 model |   Object Detection   |    Python, Rust, Cpp    |\n|   YOLOv5l    |      YOLOv5 Large model      |   Object Detection   |         Python          |\n|   YOLOv5m    |     YOLOv5 Medium model      |   Object Detection   |         Python          |\n+--------------+------------------------------+----------------------+-------------------------+\n</code></pre></p>"},{"location":"command_tool/#subcommand-bench","title":"Subcommand: bench","text":"<p><code>bench</code> subcommand runs a specific model with a given path where the input sample data are located. It will print out the performance benchmark results like QPS.</p> <p>Example <pre><code>$ furiosa-models bench ResNet50 ./samples\n\nRunning 10 input samples ...\n----------------------------------------------------------------------\nWARN: the benchmark results may depend on the number of input samples,\nsizes of the images, and a machine where this benchmark is running.\n----------------------------------------------------------------------\n\n----------------------------------------------------------------------\nPreprocess -&gt; Inference -&gt; Postprocess\n----------------------------------------------------------------------\nTotal elapsed time: 1.04471 sec\nQPS: 9.57201\nAvg. elapsed time / sample: 104.47126 ms\n\n----------------------------------------------------------------------\nInference -&gt; Postprocess\n----------------------------------------------------------------------\nTotal elapsed time: 24.98687 ms\nQPS: 400.21017\nAvg. elapsed time / sample: 2.49869 ms\n\n----------------------------------------------------------------------\nInference\n----------------------------------------------------------------------\nTotal elapsed time: 558.66595 us\nQPS: 17899.78418\nAvg. elapsed time / sample: 55.86660 us\n</code></pre></p>"},{"location":"command_tool/#subcommand-desc","title":"Subcommand: desc","text":"<p><code>desc</code> subcommand shows the details of a specific model.</p> <p>Example <pre><code>$ furiosa-models desc ResNet50 \nfamily: ResNet\nformat: onnx\nmetadata:\n  description: ResNet50 v1.5 int8 ImageNet-1K\n  publication:\n    authors: null\n    date: null\n    publisher: null\n    title: null\n    url: https://arxiv.org/abs/1512.03385.pdf\nname: ResNet50\nversion: v1.5\ntask type: Image Classification\navailable postprocess versions: Python, Rust\n</code></pre></p>"},{"location":"developers-guide/","title":"Developer's guide","text":"<p>This documentation is for developers who want to contribute to Furiosa Models.</p>"},{"location":"developers-guide/#release-guide","title":"Release guide","text":""},{"location":"developers-guide/#select-the-appropriate-commit","title":"Select the appropriate commit","text":"<p>First, you need to select the proper release candidate commit to release. That revision must pass all CI tests and be polished.</p>"},{"location":"developers-guide/#create-a-dedicated-release-branch","title":"Create a dedicated release branch","text":"<p>Create a release branch named <code>branch-&lt;release version w/o v prefix&gt;</code> from that commit.</p>"},{"location":"developers-guide/#set-the-version","title":"Set the version","text":"<p>Furiosa Models' main branch is for developers, so it set to <code>&lt;next-release-version&gt;.dev0</code>. The version information is located in <code>./furiosa/models/__init__.py</code>. Please set it with the appropriate one.</p>"},{"location":"developers-guide/#alter-absolute-links-in-documentation","title":"Alter absolute links in documentation","text":"<p>Since <code>README.md</code> can appear in many places, including GitHub, the rendered documentation, and the index page of PyPI, there are a few absolute URL links (e.g., <code>[Getting Started](https://furiosa-ai.github.io/furiosa-models/v0.8.0/getting_started/)</code>). Please change it to appropriate links with the proper version.</p>"},{"location":"developers-guide/#create-release-tag","title":"Create release tag","text":"<p>Create a git tag when you publish a release to keep track of revisions in your releases. You can create and push a git tag with the following commands.</p> <pre><code>git tag &lt;release version&gt;  # e.g. git tag v0.8.2\ngit push --tags &lt;appropriate remote&gt;  # e.g. git push --tags upstream\n</code></pre>"},{"location":"developers-guide/#test-building-wheels","title":"Test building wheels","text":"<p>We're using <code>flit</code> as our packaging tool, so please test that you can generate a proper wheel using the <code>flit build</code> command before releasing. You may want to install <code>flit</code> first, please install it with <code>pip install flit</code>.</p>"},{"location":"developers-guide/#publish-to-pypi","title":"Publish to PyPI","text":"<p>Now you can publish the package with <code>flit publish</code> command. \ud83c\udf89</p>"},{"location":"getting_started/","title":"Getting Started","text":"<p>This documentation explains how to install furiosa-models, how to use available models in furiosa-models, and how to explore the documents.</p>"},{"location":"getting_started/#prerequisites","title":"Prerequisites","text":"<p><code>furiosa-models</code> can be installed on various Linux distributions, but it has been tested on the followings:</p> <ul> <li>CentOS 7 or higher</li> <li>Debian buster or higher</li> <li>Ubuntu 18.04 or higher</li> </ul> <p>The following packages should be installed, but the followings are installed by default in most systems. So, only when you have any dependency issue, you need to install the following packages:</p> <ul> <li>libstdc++6</li> <li>libgomp</li> </ul>"},{"location":"getting_started/#installing","title":"Installing","text":"<p>You can quickly install Furiosa Models by using <code>pip</code> as following:</p> <pre><code>pip install 'furiosa-models'\n</code></pre> Building from Source Code (click to see) <p>Or you can build from the source code as following:</p> <p>Building <code>furiosa-models</code> requires additional prerequisites:</p> <ul> <li>rust-toolchain (please refer to rustup)</li> </ul> <pre><code>git clone https://github.com/furiosa-ai/furiosa-models\npip install .\n</code></pre>"},{"location":"getting_started/#quick-example-and-guides","title":"Quick example and Guides","text":"<p>You can simply load a model and run through furiosa-sdk as the following:</p> <p>Info</p> <p>If you want to learn more about the installation of furiosa-sdk and how to use it, please follow the followings:</p> <ul> <li>Driver, Firmware, and Runtime Installation</li> <li>Python SDK Installation and User Guide</li> <li>Furiosa SDK - Tutorial and Code Examples</li> </ul> <pre><code>from furiosa.models.vision import SSDMobileNet\nfrom furiosa.runtime import session\nimage = [\"tests/assets/cat.jpg\"]\nmobilenet = SSDMobileNet.load()\nwith session.create(mobilenet) as sess:\ninputs, contexts = mobilenet.preprocess(image)\noutputs = sess.run(inputs).numpy()\nmobilenet.postprocess(outputs, contexts)\n</code></pre> <p>This example does:</p> <ol> <li>Loads the SSDMobileNet model</li> <li>Creates a <code>session</code> which is the main class of Furiosa Runtime which actually loads an ONNX/tflite model to NPU and run inferences</li> <li>Runs an inference with pre/post processings.</li> </ol> <p>A <code>Model</code> instance is a Python object, including model artifacts, metadata, and its pre/postprocessors. You can learn more about <code>Model</code> object at Model object.</p> <p>Also, you can find all available models at  Available Models. Each model page includes the details of the model, input and output tensors, and pre/post processings,  and API reference.</p> <p>If you want to learn more about <code>furiosa.runtime.session</code> in Furiosa Runtime, please refer to Furiosa SDK - Tutorial and Code Examples.</p>"},{"location":"model_object/","title":"Model object","text":"<p>In <code>furiosa-models</code> project, <code>Model</code> is the first class object, and it represents a neural network model.  This document explains what <code>Model</code> object offers and their usages.</p>"},{"location":"model_object/#loading-a-pre-trained-model","title":"Loading a pre-trained model","text":"<p>To load a pre-trained neural-network model, you need to call <code>load()</code> method. Since the sizes of pre-trained model weights vary from tens to hundreds megabytes,  the model images are not included in Python package. When <code>load()</code> method is called, a pre-trained model will be  fetched over network. It takes some time (usually few seconds) depending on models and environments.  Once the model images are fetched, they will be cached on a local disk.</p> <p>Non-blocking API <code>load_async()</code> also is available, and it can be used  if your application is running through asynchronous executors (e.g., asyncio).</p> Blocking APINon-blocking API <pre><code>from furiosa.models.types import Model\nfrom furiosa.models.vision import ResNet50\nmodel: Model = ResNet50.load()\n</code></pre> <pre><code>import asyncio\nfrom furiosa.models.types import Model\nfrom furiosa.models.vision import ResNet50\nmodel: Model = asyncio.run(ResNet50.load_async())\n</code></pre> <p></p>"},{"location":"model_object/#accessing-artifacts-and-metadata","title":"Accessing artifacts and metadata","text":"<p>A <code>Model</code> object includes model artifacts, such as ONNX, tflite, DFG, and ENF.</p> <p>DFG and ENF are FuriosaAI Compiler specific formats. Both formats are used for pre-compiled binary, and they are used to skip compilation times that take up to minutes. In addition, a <code>Model</code> object has various metadata. The followings are all attributes belonging to a single <code>Model</code> object.</p>"},{"location":"model_object/#furiosaregistrymodel","title":"<code>furiosa.registry.Model</code>","text":"<p>Represent the artifacts and metadata of a neural network model</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>a name of this model</p> <code>format</code> <code>Format</code> <p>the binary format type of model source; e.g., ONNX, tflite</p> <code>source</code> <code>bytes</code> <p>a source binary in ONNX or tflite. It can be used for compiling this model with a custom compiler configuration.</p> <code>dfg</code> <code>Optional[bytes]</code> <p>an intermediate representation of furiosa-compiler. Native post processor implementation uses dfg binary. Users don't need to use <code>dfg</code> directly.</p> <code>enf</code> <code>Optional[bytes]</code> <p>the executable binary for furiosa runtime and NPU</p> <code>version</code> <code>Optional[str]</code> <p>model version</p> <code>inputs</code> <code>Optional[List[ModelTensor]]</code> <p>data type and shape of input tensors</p> <code>outputs</code> <code>Optional[List[ModelTensor]]</code> <p>data type and shape of output tensors</p> <code>compiler_config</code> <code>Optional[Dict]</code> <p>a pre-defined compiler option</p> Source code in <code>furiosa/registry/model.py</code> <pre><code>class Model(BaseModel):\n\"\"\"Represent the artifacts and metadata of a neural network model\n    Attributes:\n        name: a name of this model\n        format: the binary format type of model source; e.g., ONNX, tflite\n        source: a source binary in ONNX or tflite. It can be used for compiling this model\n            with a custom compiler configuration.\n        dfg: an intermediate representation of furiosa-compiler. Native post processor implementation uses dfg binary.\n            Users don't need to use `dfg` directly.\n        enf: the executable binary for furiosa runtime and NPU\n        version: model version\n        inputs: data type and shape of input tensors\n        outputs: data type and shape of output tensors\n        compiler_config: a pre-defined compiler option\n    \"\"\"\n# class Config(BaseConfig):\n#     # Non pydantic attribute allowed\n#     # https://pydantic-docs.helpmanual.io/usage/types/#arbitrary-types-allowed\n#     arbitrary_types_allowed = True\nname: str\nsource: bytes = Field(repr=False)\nformat: Format\ndfg: Optional[bytes] = Field(repr=False)\nenf: Optional[bytes] = Field(repr=False)\nfamily: Optional[str] = None\nversion: Optional[str] = None\nmetadata: Optional[Metadata] = None\ninputs: Optional[List[ModelTensor]] = []\noutputs: Optional[List[ModelTensor]] = []\ncompiler_config: Optional[Dict] = None\n</code></pre>"},{"location":"model_object/#inferencing-with-session-api","title":"Inferencing with Session API","text":"<p>To load a model to FuriosaAI NPU, you need to create a session instance with a <code>Model</code> object  through Furiosa SDK. As we mentioned above, even a single <code>Model</code> object has multiple model artifacts, such as  a ONNX model and an ENF (FuriosaAI's compiled program binary).</p> <p>If an <code>Model</code> object is passed to <code>session.create()</code>, Session API chooses the ENF (FuriosaAI's Executable NPU Format)  by default. In this case, <code>session.create()</code> doesn't involve any compilation because it uses the pre-compiled ENF binary.</p> <p>Info</p> <p>If you want to learn more about the installation of furiosa-sdk and how to use it, please follow the followings:</p> <ul> <li>Driver, Firmware, and Runtime Installation</li> <li>Python SDK Installation and User Guide</li> <li>Furiosa SDK - Tutorial and Code Examples</li> </ul> <p>Users still can compile source models like ONNX or tflite if passing <code>Model.source</code> to <code>session.create()</code>.  Compiling models will take some time up to minutes, but it allows to specify batch size and compiler configs,  leading to more optimizations depending on user use-cases. To learn more about <code>Model.source</code>,  please refer to Accessing artifacts and metadata.</p> <p></p> <p>Example</p> Using ENF binaryUsing ONNX model <pre><code>from furiosa.models.vision import SSDMobileNet\nfrom furiosa.runtime import session\nimage = [\"tests/assets/cat.jpg\"]\nmobilenet = SSDMobileNet.load()\nwith session.create(mobilenet) as sess:\ninputs, contexts = mobilenet.preprocess(image)\noutputs = sess.run(inputs).numpy()\nmobilenet.postprocess(outputs, contexts)\n</code></pre> <pre><code>from furiosa.models.vision import SSDMobileNet\nfrom furiosa.runtime import session\nimages = [\"tests/assets/cat.jpg\", \"tests/assets/cat.jpg\"]\nmobilenet = SSDMobileNet.load()\nwith session.create(mobilenet.source, batch_size=2) as sess:\ninputs, contexts = mobilenet.preprocess(images)\noutputs = sess.run(inputs).numpy()\nmobilenet.postprocess(outputs, contexts=contexts)\n</code></pre>"},{"location":"model_object/#prepostprocessing","title":"Pre/Postprocessing","text":"<p>There are gaps between model input/outputs and user applications' desired input and output data. In general, inputs and outputs of a neural network model are tensors. In applications,  user sample data are images in standard formats like PNG or JPEG, and  users also need to convert the output tensors to struct data for user applications.</p> <p>A <code>Model</code> object also provides both <code>preprocess()</code> and <code>postprocess()</code> methods.  They are utilities to convert easily user inputs to the model's input tensors and output tensors  to struct data which can be easily accessible by applications.  If using pre-built pre/postprocessing methods, users can quickly start using <code>furiosa-models</code>. </p> <p>In sum, typical steps of a single inference is as the following, as also shown at examples.</p> <ol> <li>Call <code>preprocess()</code> with user inputs (e.g., image files)</li> <li>Pass an output of <code>preprocess()</code> to <code>Session.run()</code></li> <li>Pass the output of the model to <code>postprocess()</code></li> </ol> <p>Info</p> <p>Default postprocessing implementations are in Python. However, some models have the native postprocessing implemented in Rust and C++ and optimized for FuriosaAI Warboy and Intel/AMD CPUs. Python implementations can run on CPU and GPU as well, whereas the native postprocessor implementations works with only FuriosaAI NPU.  Native implementations are designed to leverage FuriosaAI NPU's characteristics even for post-processing and maximize the latency and throughput by using modern CPU architecture,  such as CPU cache, SIMD instructions and CPU pipelining. According to our benchmark, the native implementations show at most 70% lower latency.</p> <p>To use native post processor, please pass <code>use_native=True</code> to <code>Model.load()</code> or <code>Model.load_async()</code>. The following is an example to use native post processor for SSDMobileNet. You can find more details of each mode page.</p> <p>Example</p> <pre><code>from furiosa.models.vision import SSDMobileNet\nfrom furiosa.runtime import session\nimage = [\"tests/assets/cat.jpg\"]\nmobilenet = SSDMobileNet.load(use_native=True)\nwith session.create(mobilenet) as sess:\ninputs, contexts = mobilenet.preprocess(image)\noutputs = sess.run(inputs).numpy()\nmobilenet.postprocess(outputs, contexts[0])\n</code></pre>"},{"location":"model_object/#see-also","title":"See Also","text":"<ul> <li>Furiosa SDK Documentation</li> </ul>"},{"location":"models/resnet50_v1.5/","title":"ResNet50 v1.5","text":"<p>ResNet50 v1.5 backbone model trained on ImageNet (224x224). This model has been used since MLCommons v0.5.</p>"},{"location":"models/resnet50_v1.5/#overall","title":"Overall","text":"<ul> <li>Framework: PyTorch</li> <li>Model format: ONNX</li> <li>Model task: Image classification</li> <li>Source: This model is originated from ResNet50 v1.5 in ONNX available at MLCommons - Supported Models.</li> </ul>"},{"location":"models/resnet50_v1.5/#_1","title":"ResNet50 v1.5","text":"<p>Usages</p> Python PostprocessorNative Postprocessor <pre><code>from furiosa.models.vision import ResNet50\nfrom furiosa.runtime import session\nimage = \"tests/assets/cat.jpg\"\nresnet50 = ResNet50.load()\nwith session.create(resnet50) as sess:\ninputs, _ = resnet50.preprocess(image)\noutputs = sess.run(inputs).numpy()\nresnet50.postprocess(outputs)\n</code></pre> <pre><code>from furiosa.models.vision import ResNet50\nfrom furiosa.runtime import session\nimage = \"tests/assets/cat.jpg\"\nresnet50 = ResNet50.load(use_native=True)\nwith session.create(resnet50) as sess:\ninputs, _ = resnet50.preprocess(image)\noutputs = sess.run(inputs).numpy()\nresnet50.postprocess(outputs)\n</code></pre>"},{"location":"models/resnet50_v1.5/#inputs","title":"Inputs","text":"<p>The input is a 3-channel image of 224x224 (height, width).</p> <ul> <li>Data Type: <code>numpy.float32</code></li> <li>Tensor Shape: <code>[1, 3, 224, 224]</code></li> <li>Memory Format: NCHW, where:<ul> <li>N - batch size</li> <li>C - number of channels</li> <li>H - image height</li> <li>W - image width</li> </ul> </li> <li>Color Order: BGR</li> <li>Optimal Batch Size (minimum: 1): &lt;= 8</li> </ul>"},{"location":"models/resnet50_v1.5/#outputs","title":"Outputs","text":"<p>The output is a <code>numpy.float32</code> tensor with the shape (<code>[1,]</code>), including a class id. <code>postprocess()</code> transforms the class id to a label string.</p>"},{"location":"models/resnet50_v1.5/#prepostprocessing","title":"Pre/Postprocessing","text":"<p><code>furiosa.models.vision.ResNet50</code> class provides <code>preprocess</code> and <code>postprocess</code> methods that  convert input images to input tensors and the model outputs to labels respectively.  You can find examples at ResNet50 Usage.</p>"},{"location":"models/resnet50_v1.5/#furiosamodelsvisionresnet50preprocess","title":"<code>furiosa.models.vision.ResNet50.preprocess</code>","text":"<p>Convert an input image to a model input tensor</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Union[str, npt.ArrayLike]</code> <p>A path of an image or an image loaded as a numpy array in BGR order.</p> required <p>Returns:</p> Type Description <code>Tuple[np.array, None]</code> <p>The first element of tuple is a numpy array. To learn more about the output of preprocess, please refer to Inputs.</p>"},{"location":"models/resnet50_v1.5/#furiosamodelsvisionresnet50postprocess","title":"<code>furiosa.models.vision.ResNet50.postprocess</code>","text":""},{"location":"models/resnet50_v1.5/#native-postprocessor","title":"Native Postprocessor","text":"<p>This class provides another version of the postprocessing implementation which is highly optimized for NPU. The implementation leverages the NPU IO architecture and runtime.</p> <p>To use this implementation, when this model is loaded, the parameter <code>use_native=True</code> should be passed to <code>load()</code> or <code>load_aync()</code>. The following is an example:</p> <p>Example</p> <pre><code>from furiosa.models.vision import ResNet50\nfrom furiosa.runtime import session\nimage = \"tests/assets/cat.jpg\"\nresnet50 = ResNet50.load(use_native=True)\nwith session.create(resnet50) as sess:\ninputs, _ = resnet50.preprocess(image)\noutputs = sess.run(inputs).numpy()\nresnet50.postprocess(outputs)\n</code></pre>"},{"location":"models/ssd_mobilenet/","title":"SSD MobileNet v1","text":"<p>SSD MobileNet v1 backbone model trained on COCO (300x300). This model has been used since MLCommons v0.5.</p>"},{"location":"models/ssd_mobilenet/#overall","title":"Overall","text":"<ul> <li>Framework: PyTorch</li> <li>Model format: ONNX</li> <li>Model task: Object detection</li> <li>Source: This model is originated from SSD MobileNet v1 in ONNX available at MLCommons - Supported Models.</li> </ul>"},{"location":"models/ssd_mobilenet/#_1","title":"SSD MobileNet v1","text":"<p>Usages</p> Python PostprocessorNative Postprocessor <pre><code>from furiosa.models.vision import SSDMobileNet\nfrom furiosa.runtime import session\nimage = [\"tests/assets/cat.jpg\"]\nmobilenet = SSDMobileNet.load()\nwith session.create(mobilenet) as sess:\ninputs, contexts = mobilenet.preprocess(image)\noutputs = sess.run(inputs).numpy()\nmobilenet.postprocess(outputs, contexts)\n</code></pre> <pre><code>from furiosa.models.vision import SSDMobileNet\nfrom furiosa.runtime import session\nimage = [\"tests/assets/cat.jpg\"]\nmobilenet = SSDMobileNet.load(use_native=True)\nwith session.create(mobilenet) as sess:\ninputs, contexts = mobilenet.preprocess(image)\noutputs = sess.run(inputs).numpy()\nmobilenet.postprocess(outputs, contexts[0])\n</code></pre>"},{"location":"models/ssd_mobilenet/#inputs","title":"Inputs","text":"<p>The input is a 3-channel image of 300x300 (height, width).</p> <ul> <li>Data Type: <code>numpy.float32</code></li> <li>Tensor Shape: <code>[1, 3, 300, 300]</code></li> <li>Memory Format: NCHW, where:<ul> <li>N - batch size</li> <li>C - number of channels</li> <li>H - image height</li> <li>W - image width</li> </ul> </li> <li>Color Order: RGB</li> <li>Optimal Batch Size (minimum: 1): &lt;= 8</li> </ul>"},{"location":"models/ssd_mobilenet/#outputs","title":"Outputs","text":"<p>The outputs are 12 <code>numpy.float32</code> tensors in various shapes as the following. You can refer to <code>postprocess()</code> function to learn how to decode boxes, classes, and confidence scores.</p> Tensor Shape Data Type Data Type Description 0 (1, 273, 19, 19) float32 NCHW 1 (1, 12, 19, 19) float32 NCHW 2 (1, 546, 10, 10) float32 NCHW 3 (1, 24, 10, 10) float32 NCHW 4 (1, 546, 5, 5) float32 NCHW 5 (1, 24, 5, 5) float32 NCHW 6 (1, 546, 3, 3) float32 NCHW 7 (1, 24, 3, 3) float32 NCHW 8 (1, 546, 2, 2) float32 NCHW 9 (1, 24, 2, 2) float32 NCHW 10 (1, 546, 1, 1) float32 NCHW 11 (1, 24, 1, 1) float32 NCHW"},{"location":"models/ssd_mobilenet/#prepostprocessing","title":"Pre/Postprocessing","text":"<p><code>furiosa.models.vision.SSDMobileNet</code> class provides <code>preprocess</code> and <code>postprocess</code> methods. <code>preprocess</code> method converts input images to input tensors, and <code>postprocess</code> method converts  model output tensors to a list of bounding boxes, scores and labels.  You can find examples at SSDMobileNet Usage.</p>"},{"location":"models/ssd_mobilenet/#furiosamodelsvisionssdmobilenetpreprocess","title":"<code>furiosa.models.vision.SSDMobileNet.preprocess</code>","text":"<p>Preprocess input images to a batch of input tensors.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>Sequence[Union[str, np.ndarray]]</code> <p>A list of paths of image files (e.g., JPEG, PNG) or a stacked image loaded as a numpy array in BGR order or gray order.</p> required <p>Returns:</p> Type Description <code>Tuple[npt.ArrayLike, List[Dict[str, Any]]]</code> <p>The first element is 3-channel images of 300x300 in NCHW format, and the second element is a list of context about the original image metadata. Please learn more about the outputs of preprocess (i.e., model inputs), please refer to Inputs.</p>"},{"location":"models/ssd_mobilenet/#furiosamodelsvisionssdmobilenetpostprocess","title":"<code>furiosa.models.vision.SSDMobileNet.postprocess</code>","text":"<p>Convert the outputs of this model to a list of bounding boxes, scores and labels</p> <p>Parameters:</p> Name Type Description Default <code>model_outputs</code> <code>Sequence[numpy.ndarray]</code> <p>the outputs of the model. To learn more about the output of model, please refer to Outputs.</p> required <code>contexts</code> <code>Sequence[Dict[str, Any]]</code> <p>context coming from <code>preprocess()</code></p> required <p>Returns:</p> Type Description <code>List[List[ObjectDetectionResult]]</code> <p>Detected Bounding Box and its score and label represented as <code>ObjectDetectionResult</code>. To learn more about <code>ObjectDetectionResult</code>, 'Definition of ObjectDetectionResult' can be found below.</p> Definitions of ObjectDetectionResult and LtrbBoundingBox Source code in <code>furiosa/models/vision/postprocess.py</code> <pre><code>@dataclass\nclass LtrbBoundingBox:\nleft: float\ntop: float\nright: float\nbottom: float\ndef __iter__(self) -&gt; Iterator[float]:\nreturn iter([self.left, self.top, self.right, self.bottom])\n</code></pre> Source code in <code>furiosa/models/vision/postprocess.py</code> <pre><code>@dataclass\nclass ObjectDetectionResult:\nboundingbox: LtrbBoundingBox\nscore: float\nlabel: str\nindex: int\n</code></pre>"},{"location":"models/ssd_mobilenet/#native-postprocessor","title":"Native Postprocessor","text":"<p>This class provides another version of the postprocessing implementation which is highly optimized for NPU. The implementation leverages the NPU IO architecture and runtime.</p> <p>To use this implementation, when this model is loaded, the parameter <code>use_native=True</code> should be passed to <code>load()</code> or <code>load_aync()</code>. The following is an example:</p> <p>Example</p> <pre><code>from furiosa.models.vision import SSDMobileNet\nfrom furiosa.runtime import session\nimage = [\"tests/assets/cat.jpg\"]\nmobilenet = SSDMobileNet.load(use_native=True)\nwith session.create(mobilenet) as sess:\ninputs, contexts = mobilenet.preprocess(image)\noutputs = sess.run(inputs).numpy()\nmobilenet.postprocess(outputs, contexts[0])\n</code></pre>"},{"location":"models/ssd_resnet34/","title":"SSD ResNet34","text":"<p>SSD ResNet34 backbone model trained on COCO (1200x1200). This model has been used since MLCommons v0.5.</p>"},{"location":"models/ssd_resnet34/#overall","title":"Overall","text":"<ul> <li>Framework: PyTorch</li> <li>Model format: ONNX</li> <li>Model task: Object detection</li> <li>Source: This model is originated from SSD ResNet34 in ONNX available at MLCommons - Supported Models.</li> </ul>"},{"location":"models/ssd_resnet34/#_1","title":"SSD ResNet34","text":"<p>Usages</p> Python PostprocessorNative Postprocessor <pre><code>from furiosa.models.vision import SSDResNet34\nfrom furiosa.runtime import session\nresnet34 = SSDResNet34.load()\nwith session.create(resnet34) as sess:\nimage, contexts = resnet34.preprocess([\"tests/assets/cat.jpg\"])\noutput = sess.run(image).numpy()\nresnet34.postprocess(output, contexts=contexts)\n</code></pre> <pre><code>from furiosa.models.vision import SSDResNet34\nfrom furiosa.runtime import session\nresnet34 = SSDResNet34.load(use_native=True)\nwith session.create(resnet34) as sess:\nimage, contexts = resnet34.preprocess([\"tests/assets/cat.jpg\"])\noutput = sess.run(image).numpy()\nresnet34.postprocessor(output, contexts=contexts[0])\n</code></pre>"},{"location":"models/ssd_resnet34/#inputs","title":"Inputs","text":"<p>The input is a 3-channel image of 300x300 (height, width).</p> <ul> <li>Data Type: <code>numpy.float32</code></li> <li>Tensor Shape: <code>[1, 3, 1200, 1200]</code></li> <li>Memory Format: NCHW, where<ul> <li>N - batch size</li> <li>C - number of channels</li> <li>H - image height</li> <li>W - image width</li> </ul> </li> <li>Color Order: RGB</li> <li>Optimal Batch Size: 1</li> </ul>"},{"location":"models/ssd_resnet34/#outputs","title":"Outputs","text":"<p>The outputs are 12 <code>numpy.float32</code> tensors in various shapes as the following.  You can refer to <code>postprocess()</code> function to learn how to decode boxes, classes, and confidence scores.</p> Tensor Shape Data Type Data Type Description 0 (1, 324, 50, 50) float32 NCHW 1 (1, 486, 25, 25) float32 NCHW 2 (1, 486, 13, 13) float32 NCHW 3 (1, 486, 7, 7) float32 NCHW 4 (1, 324, 3, 3) float32 NCHW 5 (1, 324, 3, 3) float32 NCHW 6 (1, 16, 50, 50) float32 NCHW 7 (1, 24, 25, 25) float32 NCHW 8 (1, 24, 13, 13) float32 NCHW 9 (1, 24, 7, 7) float32 NCHW 10 (1, 16, 3, 3) float32 NCHW 11 (1, 16, 3, 3) float32 NCHW"},{"location":"models/ssd_resnet34/#prepostprocessing","title":"Pre/Postprocessing","text":"<p><code>furiosa.models.vision.SSDResNet34</code> class provides <code>preprocess</code> and <code>postprocess</code> methods. <code>preprocess</code> method converts input images to input tensors, and <code>postprocess</code> method converts  model output tensors to a list of bounding boxes, scores and labels.  You can find examples at SSDResNet34 Usage.</p>"},{"location":"models/ssd_resnet34/#furiosamodelsvisionssdresnet34preprocess","title":"<code>furiosa.models.vision.SSDResNet34.preprocess</code>","text":"<p>Preprocess input images to a batch of input tensors</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>Sequence[Union[str, np.ndarray]]</code> <p>A list of paths of image files (e.g., JPEG, PNG) or a stacked image loaded as a numpy array in BGR order or gray order.</p> required <p>Returns:</p> Type Description <code>Tuple[npt.ArrayLike, List[Dict[str, Any]]]</code> <p>The first element is a list of 3-channel images of 1200x1200 in NCHW format, and the second element is a list of context about the original image metadata. Please learn more about the outputs of preprocess (i.e., model inputs), please refer to Inputs.</p>"},{"location":"models/ssd_resnet34/#furiosamodelsvisionssdresnet34postprocess","title":"<code>furiosa.models.vision.SSDResNet34.postprocess</code>","text":"<p>Convert the outputs of this model to a list of bounding boxes, scores and labels</p> <p>Parameters:</p> Name Type Description Default <code>model_outputs</code> <code>Sequence[np.ndarray]</code> <p>the outputs of the model. To learn more about the output of model, please refer to Outputs.</p> required <code>contexts</code> <code>Sequence[Dict[str, Any]]</code> <p>context coming from <code>preprocess()</code></p> required <p>Returns:</p> Type Description <code>List[List[ObjectDetectionResult]]</code> <p>Detected Bounding Box and its score and label represented as <code>ObjectDetectionResult</code>. To learn more about <code>ObjectDetectionResult</code>, 'Definition of ObjectDetectionResult' can be found below.</p> Definition of ObjectDetectionResult and LtrbBoundingBox Source code in <code>furiosa/models/vision/postprocess.py</code> <pre><code>@dataclass\nclass LtrbBoundingBox:\nleft: float\ntop: float\nright: float\nbottom: float\ndef __iter__(self) -&gt; Iterator[float]:\nreturn iter([self.left, self.top, self.right, self.bottom])\n</code></pre> Source code in <code>furiosa/models/vision/postprocess.py</code> <pre><code>@dataclass\nclass ObjectDetectionResult:\nboundingbox: LtrbBoundingBox\nscore: float\nlabel: str\nindex: int\n</code></pre>"},{"location":"models/ssd_resnet34/#native-postprocessor","title":"Native Postprocessor","text":"<p>This class provides another version of the postprocessing implementation which is highly optimized for NPU. The implementation leverages the NPU IO architecture and runtime.</p> <p>To use this implementation, when this model is loaded, the parameter <code>use_native=True</code> should be passed to <code>load()</code> or <code>load_aync()</code>. The following is an example:</p> <p>Example</p> <pre><code>from furiosa.models.vision import SSDResNet34\nfrom furiosa.runtime import session\nresnet34 = SSDResNet34.load(use_native=True)\nwith session.create(resnet34) as sess:\nimage, contexts = resnet34.preprocess([\"tests/assets/cat.jpg\"])\noutput = sess.run(image).numpy()\nresnet34.postprocessor(output, contexts=contexts[0])\n</code></pre>"},{"location":"models/yolov5l/","title":"YOLOv5L","text":"<p>YOLOv5 is the one of the most popular object detection models. You can find more details at https://github.com/ultralytics/yolov5.</p>"},{"location":"models/yolov5l/#overall","title":"Overall","text":"<ul> <li>Framework: PyTorch</li> <li>Model format: ONNX</li> <li>Model task: Object Detection</li> <li>Source: https://github.com/ultralytics/yolov5</li> </ul>"},{"location":"models/yolov5l/#_1","title":"YOLOv5L","text":"<p>Usage</p> <pre><code>import cv2\nimport numpy as np\nfrom furiosa.models.vision import YOLOv5l\nfrom furiosa.runtime import session\nyolov5l = YOLOv5l.load()\nwith session.create(yolov5l) as sess:\nimage = cv2.imread(\"tests/assets/yolov5-test.jpg\")\ninputs, contexts = yolov5l.preprocess([image])\noutput = sess.run(np.expand_dims(inputs[0], axis=0)).numpy()\nyolov5l.postprocess(output, contexts=contexts)\n</code></pre>"},{"location":"models/yolov5l/#inputs","title":"Inputs","text":"<p>The input is a 3-channel image of 640, 640 (height, width).</p> <ul> <li>Data Type: <code>numpy.uint8</code></li> <li>Tensor Shape: <code>[1, 640, 640, 3]</code></li> <li>Memory Format: NHWC, where<ul> <li>N - batch size</li> <li>H - image height</li> <li>W - image width</li> <li>C - number of channels</li> </ul> </li> <li>Color Order: RGB</li> <li>Optimal Batch Size (minimum: 1): &lt;= 2</li> </ul>"},{"location":"models/yolov5l/#outputs","title":"Outputs","text":"<p>The outputs are 3 <code>numpy.float32</code> tensors in various shapes as the following.  You can refer to <code>postprocess()</code> function to learn how to decode boxes, classes, and confidence scores.</p> Tensor Shape Data Type Data Type Description 0 (1, 45, 80, 80) float32 NCHW 1 (1, 45, 40, 40) float32 NCHW 2 (1, 45, 20, 20) float32 NCHW"},{"location":"models/yolov5l/#prepostprocessing","title":"Pre/Postprocessing","text":"<p><code>furiosa.models.vision.YOLOv5l</code> class provides <code>preprocess</code> and <code>postprocess</code> methods. <code>preprocess</code> method converts input images to input tensors, and <code>postprocess</code> method converts  model output tensors to a list of bounding boxes, scores and labels.  You can find examples at YOLOv5l Usage.</p>"},{"location":"models/yolov5l/#furiosamodelsvisionyolov5lpreprocess","title":"<code>furiosa.models.vision.YOLOv5l.preprocess</code>","text":"<p>Preprocess input images to a batch of input tensors</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>Sequence[Union[str, np.ndarray]]</code> <p>Color images have (NCHW: Batch, Channel, Height, Width) dimensions.</p> required <code>color_format</code> <code>str</code> <p>'bgr' (default) or 'rgb'</p> <code>'bgr'</code> <p>Returns:</p> Type Description <code>Tuple[np.ndarray, List[Dict[str, Any]]]</code> <p>a pre-processed image, scales and padded sizes(width,height) per images. The first element is a stacked numpy array containing a batch of images. To learn more about the outputs of preprocess (i.e., model inputs), please refer to YOLOv5l Inputs or YOLOv5m Inputs.</p> <p>The second element is a list of dict objects about the original images. Each dict object has the following keys. 'scale' key of the returned dict has a rescaled ratio per width(=target/width) and height(=target/height), and the 'pad' key has padded width and height pixels. Specially, the last dictionary element of returing tuple will be passed to postprocessing as a parameter to calculate predicted coordinates on normalized coordinates back to an input image coordinator.</p>"},{"location":"models/yolov5l/#furiosamodelsvisionyolov5lpostprocess","title":"<code>furiosa.models.vision.YOLOv5l.postprocess</code>","text":"<p>Convert the outputs of this model to a list of bounding boxes, scores and labels</p> <p>Parameters:</p> Name Type Description Default <code>model_outputs</code> <code>Sequence[np.ndarray]</code> <p>P3/8, P4/16, P5/32 features from yolov5l model. To learn more about the outputs of preprocess (i.e., model inputs), please refer to YOLOv5l Outputs or YOLOv5m Outputs.</p> required <code>contexts</code> <code>Sequence[Dict[str, Any]]</code> <p>A configuration for each image generated by the preprocessor. For example, it could be the reduction ratio of the image, the actual image width and height.</p> required <code>conf_thres</code> <code>float</code> <p>Confidence score threshold. The default to 0.25</p> <code>0.25</code> <code>iou_thres</code> <code>float</code> <p>IoU threshold value for the NMS processing. The default to 0.45.</p> <code>0.45</code> <p>Returns:</p> Type Description <code>List[List[ObjectDetectionResult]]</code> <p>Detected Bounding Box and its score and label represented as <code>ObjectDetectionResult</code>. The details of <code>ObjectDetectionResult</code> can be found below.</p> Definition of ObjectDetectionResult and LtrbBoundingBox Source code in <code>furiosa/models/vision/postprocess.py</code> <pre><code>@dataclass\nclass LtrbBoundingBox:\nleft: float\ntop: float\nright: float\nbottom: float\ndef __iter__(self) -&gt; Iterator[float]:\nreturn iter([self.left, self.top, self.right, self.bottom])\n</code></pre> Source code in <code>furiosa/models/vision/postprocess.py</code> <pre><code>@dataclass\nclass ObjectDetectionResult:\nboundingbox: LtrbBoundingBox\nscore: float\nlabel: str\nindex: int\n</code></pre>"},{"location":"models/yolov5m/","title":"YOLOv5m","text":"<p>YOLOv5 is the one of the most popular object detection models. You can find more details at https://github.com/ultralytics/yolov5.</p>"},{"location":"models/yolov5m/#overall","title":"Overall","text":"<ul> <li>Framework: PyTorch</li> <li>Model format: ONNX</li> <li>Model task: Object Detection</li> <li>Source: https://github.com/ultralytics/yolov5.</li> </ul>"},{"location":"models/yolov5m/#_1","title":"YOLOv5m","text":"<p>Usage</p> <pre><code>import cv2\nimport numpy as np\nfrom furiosa.models.vision import YOLOv5m\nfrom furiosa.runtime import session\nyolov5m = YOLOv5m.load()\nwith session.create(yolov5m) as sess:\nimage = cv2.imread(\"tests/assets/yolov5-test.jpg\")\ninputs, contexts = yolov5m.preprocess([image])\noutput = sess.run(np.expand_dims(inputs[0], axis=0)).numpy()\nyolov5m.postprocess(output, contexts=contexts)\n</code></pre>"},{"location":"models/yolov5m/#inputs","title":"Inputs","text":"<p>The input is a 3-channel image of 640, 640 (height, width).</p> <ul> <li>Data Type: <code>numpy.uint8</code></li> <li>Tensor Shape: <code>[1, 640, 640, 3]</code></li> <li>Memory Format: NHWC, where<ul> <li>N - batch size</li> <li>H - image height</li> <li>W - image width</li> <li>C - number of channels</li> </ul> </li> <li>Color Order: RGB</li> <li>Optimal Batch Size (minimum: 1): &lt;= 4</li> </ul>"},{"location":"models/yolov5m/#outputs","title":"Outputs","text":"<p>The outputs are 3 <code>numpy.float32</code> tensors in various shapes as the following.  You can refer to <code>postprocess()</code> function to learn how to decode boxes, classes, and confidence scores.</p> Tensor Shape Data Type Data Type Description 0 (1, 45, 80, 80) float32 NCHW 1 (1, 45, 40, 40) float32 NCHW 2 (1, 45, 20, 20) float32 NCHW"},{"location":"models/yolov5m/#prepostprocessing","title":"Pre/Postprocessing","text":"<p><code>furiosa.models.vision.YOLOv5m</code> class provides <code>preprocess</code> and <code>postprocess</code> methods. <code>preprocess</code> method converts input images to input tensors, and <code>postprocess</code> method converts  model output tensors to a list of bounding boxes, scores and labels.  You can find examples at YOLOv5m Usage.</p>"},{"location":"models/yolov5m/#furiosamodelsvisionyolov5mpreprocess","title":"<code>furiosa.models.vision.YOLOv5m.preprocess</code>","text":"<p>Preprocess input images to a batch of input tensors</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>Sequence[Union[str, np.ndarray]]</code> <p>Color images have (NCHW: Batch, Channel, Height, Width) dimensions.</p> required <code>color_format</code> <code>str</code> <p>'bgr' (default) or 'rgb'</p> <code>'bgr'</code> <p>Returns:</p> Type Description <code>Tuple[np.ndarray, List[Dict[str, Any]]]</code> <p>a pre-processed image, scales and padded sizes(width,height) per images. The first element is a stacked numpy array containing a batch of images. To learn more about the outputs of preprocess (i.e., model inputs), please refer to YOLOv5l Inputs or YOLOv5m Inputs.</p> <p>The second element is a list of dict objects about the original images. Each dict object has the following keys. 'scale' key of the returned dict has a rescaled ratio per width(=target/width) and height(=target/height), and the 'pad' key has padded width and height pixels. Specially, the last dictionary element of returing tuple will be passed to postprocessing as a parameter to calculate predicted coordinates on normalized coordinates back to an input image coordinator.</p>"},{"location":"models/yolov5m/#furiosamodelsvisionyolov5mpostprocess","title":"<code>furiosa.models.vision.YOLOv5m.postprocess</code>","text":"<p>Convert the outputs of this model to a list of bounding boxes, scores and labels</p> <p>Parameters:</p> Name Type Description Default <code>model_outputs</code> <code>Sequence[np.ndarray]</code> <p>P3/8, P4/16, P5/32 features from yolov5l model. To learn more about the outputs of preprocess (i.e., model inputs), please refer to YOLOv5l Outputs or YOLOv5m Outputs.</p> required <code>contexts</code> <code>Sequence[Dict[str, Any]]</code> <p>A configuration for each image generated by the preprocessor. For example, it could be the reduction ratio of the image, the actual image width and height.</p> required <code>conf_thres</code> <code>float</code> <p>Confidence score threshold. The default to 0.25</p> <code>0.25</code> <code>iou_thres</code> <code>float</code> <p>IoU threshold value for the NMS processing. The default to 0.45.</p> <code>0.45</code> <p>Returns:</p> Type Description <code>List[List[ObjectDetectionResult]]</code> <p>Detected Bounding Box and its score and label represented as <code>ObjectDetectionResult</code>. The details of <code>ObjectDetectionResult</code> can be found below.</p> Definition of ObjectDetectionResult and LtrbBoundingBox Source code in <code>furiosa/models/vision/postprocess.py</code> <pre><code>@dataclass\nclass LtrbBoundingBox:\nleft: float\ntop: float\nright: float\nbottom: float\ndef __iter__(self) -&gt; Iterator[float]:\nreturn iter([self.left, self.top, self.right, self.bottom])\n</code></pre> Source code in <code>furiosa/models/vision/postprocess.py</code> <pre><code>@dataclass\nclass ObjectDetectionResult:\nboundingbox: LtrbBoundingBox\nscore: float\nlabel: str\nindex: int\n</code></pre>"}]}