{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Furiosa Models","text":"<p><code>furiosa-models</code> is an open model zoo project for FuriosaAI NPU. It provides a set of public pre-trained, pre-quantized models for learning and demo purposes or for developing your applications.</p> <p><code>furiosa-models</code> also includes pre-packaged post/processing utilities, compiler configurations optimized for FuriosaAI NPU. However, all models are standard ONNX or tflite models, and they can run even on CPU and GPU as well.</p>"},{"location":"#releases","title":"Releases","text":"<ul> <li>v0.10.0 - 2023-08-28</li> <li>v0.9.1 - 2023-05-26</li> <li>v0.9.0 - 2023-05-12</li> <li>v0.8.0 - 2022-11-10</li> </ul>"},{"location":"#online-documentation","title":"Online Documentation","text":"<p>If you are new, you can start from Getting Started. You can also find the latest online documents, including programming guides, API references, and examples from the followings:</p> <ul> <li>Furiosa Models - Latest Documentation</li> <li>Model Object</li> <li>Model List</li> <li>Command Line Tool</li> <li>Furiosa SDK - Tutorial and Code Examples</li> </ul>"},{"location":"#model-list","title":"Model List","text":"<p>The table summarizes all models available in <code>furiosa-models</code>. If you visit each model link, you can find details about loading a model, their input and output tensors, pre/post processings, and usage examples.</p> Model Task Size Accuracy ResNet50 Image Classification 25M 75.618% (ImageNet1K-val) EfficientNetB0 Image Classification 6.4M 72.44% (ImageNet1K-val) EfficientNetV2-S Image Classification 26M 83.532% (ImageNet1K-val) SSDMobileNet Object Detection 7.2M mAP 0.232 (COCO 2017-val) SSDResNet34 Object Detection 20M mAP 0.220 (COCO 2017-val) YOLOv5M Object Detection 21M mAP 0.272 (Bdd100k-val)* YOLOv5L Object Detection 46M mAP 0.284 (Bdd100k-val)* <p>*: The accuracy of the yolov5 f32 model trained with bdd100k-val dataset, is mAP 0.295 (for yolov5m) and mAP 0.316 (for yolov5l).</p>"},{"location":"#see-also","title":"See Also","text":"<ul> <li>Furiosa Models - Latest Documentation</li> <li>Furiosa Models - Github</li> <li>Furiosa SDK Documentation</li> </ul>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#0100-2023-08-28","title":"[0.10.0 - 2023-08-28]","text":""},{"location":"changelog/#new-features","title":"New Features","text":"<ul> <li>Provide 1pe artifacts too #158</li> </ul>"},{"location":"changelog/#improvements","title":"Improvements","text":"<ul> <li>Resolve artifact binaries lazily #155</li> <li>Added <code>ruff</code> linter #160</li> <li>Automatically build documentation on comment #162</li> <li>Upgrade <code>pydantic</code> library version to <code>2.0.0</code> #166</li> <li>Added <code>model.resolve_all()</code> to resolve all lazily loaded fields at once #166</li> <li>Added local artifact binary cache #166</li> </ul>"},{"location":"changelog/#removed","title":"Removed","text":"<ul> <li>Removed unused <code>timm</code> dependency #149</li> <li>Breaking: Now uses default Python initializer instead of <code>model.load()</code> #166</li> <li>Breaking: <code>model.enf</code> field has been removed #166</li> <li>Breaking: <code>model.source</code>, <code>model.calib_yaml</code> fields have been renamed #166</li> </ul>"},{"location":"changelog/#091-2023-05-26","title":"[0.9.1 - 2023-05-26]","text":""},{"location":"changelog/#090-2023-05-12","title":"[0.9.0 - 2023-05-12]","text":""},{"location":"changelog/#new-features_1","title":"New Features","text":"<ul> <li>Add EfficientNetB0 model #121</li> <li>Add EfficientNetV2-S model #130</li> <li>Set default target as Warboy's production revision (B0) #125</li> <li>Provide calibration ranges for every model #144</li> </ul>"},{"location":"changelog/#improvements_1","title":"Improvements","text":"<ul> <li>Removed <code>Quantize</code> external operators #144</li> <li>Detailed error messages for model file fetching #144</li> <li>ENF generator can do the jobs parallelly #144</li> <li>Removed furiosa.registry dependency #144</li> <li>Faster import for furiosa.models #117</li> <li>Replace yolov5's box decode implementation in Rust #109</li> <li>Remove Cpp postprocessor implementations #102</li> <li>Change packaging tool from setuptools-rust to flit #109</li> </ul>"},{"location":"changelog/#removed_1","title":"Removed","text":"<ul> <li>Truncated models and corresponding postprocesses #144</li> <li>Breaking: drop support of directly passing Model to session.create() #144</li> </ul>"},{"location":"changelog/#tasks","title":"Tasks","text":"<ul> <li>Release guide for developers #129</li> <li>Report regression test's result with PR comment #110</li> </ul>"},{"location":"changelog/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Fix CLI to properly report net inference time #112</li> <li>Update certain model sources with valid onnx #120</li> </ul>"},{"location":"changelog/#080-2022-11-10","title":"[0.8.0 - 2022-11-10]","text":""},{"location":"changelog/#new-features_2","title":"New Features","text":"<ul> <li>Add ResNet50 model</li> <li>Add SSD ResNet34 model</li> <li>Add SSD MobileNet model</li> <li>Add YOLOv5l model</li> <li>Add YOLOv5m model</li> </ul>"},{"location":"changelog/#improvements_2","title":"Improvements","text":"<ul> <li>Add native postprocessing implementation for ResNet50 #42</li> <li>Add native postprocessing implementation for SSD ResNet34 #45</li> <li>Add native postprocessing implementation for SSD MobileNet #16</li> <li>Refactor Model API to use classmethods to load models #66</li> <li>Make Model Zoo's ABC Model #78</li> <li>Retain only necessary python dependencies #81</li> <li>Improve enf_generator.sh and add enf, dfg models for e2e tests #33</li> <li>Add mkdocstrings to make API references #22</li> </ul>"},{"location":"changelog/#tasks_1","title":"Tasks","text":"<ul> <li>Regresstion Test #61</li> <li>Attach Tekton CI #41</li> <li>Yolov5 L/M e2e testing code #22</li> <li>Change the documentation layout and add resnet34, resnet50, mobilenet details #33</li> <li>Replace maturin with setuptools-rust #26</li> </ul>"},{"location":"changelog/#bug-fixes_1","title":"Bug Fixes","text":"<ul> <li>Resolve DVC directly #80</li> </ul>"},{"location":"command_line_tool/","title":"Command Line Tool","text":"<p>We provide a simple command line tool called <code>furiosa-models</code> to allow users to evaluate or run quickly one of models with FuriosaAI NPU.</p>"},{"location":"command_line_tool/#installing","title":"Installing","text":"<p>To install <code>furiosa-models</code> command, please refer to Installing. Then, <code>furiosa-models</code> command will be available.</p>"},{"location":"command_line_tool/#synopsis","title":"Synopsis","text":"<pre><code>furiosa-models [-h] {list, desc, bench} ...\n</code></pre> <p><code>furiosa-models</code> command has three subcommands: <code>list</code>, <code>desc</code>, and <code>bench</code>.</p>"},{"location":"command_line_tool/#subcommand-list","title":"Subcommand: <code>list</code>","text":"<p><code>list</code> subcommand prints out the list of models with attributes. You will be able to figure out what models are available.</p> <p>Example <pre><code>$ furiosa-models list\n\n+-----------------+------------------------------+----------------------+-------------------------+\n|   Model name    |      Model description       |      Task type       | Available postprocesses |\n+-----------------+------------------------------+----------------------+-------------------------+\n|    ResNet50     |   MLCommons ResNet50 model   | Image Classification |         Python          |\n|  SSDMobileNet   | MLCommons MobileNet v1 model |   Object Detection   |      Python, Rust       |\n|   SSDResNet34   | MLCommons SSD ResNet34 model |   Object Detection   |      Python, Rust       |\n|     YOLOv5l     |      YOLOv5 Large model      |   Object Detection   |         Python          |\n|     YOLOv5m     |     YOLOv5 Medium model      |   Object Detection   |         Python          |\n| EfficientNetB0  |    EfficientNet B0 model     | Image Classification |         Python          |\n| EfficientNetV2s |    EfficientNetV2-s model    | Image Classification |         Python          |\n+-----------------+------------------------------+----------------------+-------------------------+\n</code></pre></p>"},{"location":"command_line_tool/#subcommand-bench","title":"Subcommand: <code>bench</code>","text":"<p><code>bench</code> subcommand runs a specific model with a given path where the input sample data are located. It will print out the performance benchmark results like QPS.</p> <p>Example <pre><code>$ furiosa-models bench ResNet50 .\nlibfuriosa_hal.so --- v0.11.0, built @ 43c901f\nRunning 4 input samples ...\n----------------------------------------------------------------------\nWARN: the benchmark results may depend on the number of input samples,\nsizes of the images, and a machine where this benchmark is running.\n----------------------------------------------------------------------\n\n----------------------------------------------------------------------\nPreprocess -&gt; Inference -&gt; Postprocess\n----------------------------------------------------------------------\nTotal elapsed time: 618.86050 ms\nQPS: 6.46349\nAvg. elapsed time / sample: 154.71513 ms\n\n----------------------------------------------------------------------\nInference -&gt; Postprocess\n----------------------------------------------------------------------\nTotal elapsed time: 5.40490 ms\nQPS: 740.06865\nAvg. elapsed time / sample: 1.35123 ms\n\n----------------------------------------------------------------------\nInference\n----------------------------------------------------------------------\nTotal elapsed time: 5.05762 ms\nQPS: 790.88645\nAvg. elapsed time / sample: 1.26440 ms\n</code></pre></p>"},{"location":"command_line_tool/#subcommand-desc","title":"Subcommand: <code>desc</code>","text":"<p><code>desc</code> subcommand shows the details of a specific model.</p> <p>Example <pre><code>$ furiosa-models desc ResNet50\nfamily: ResNet\nformat: onnx\nmetadata:\n  description: ResNet50 v1.5 int8 ImageNet-1K\n  publication:\n    authors: null\n    date: null\n    publisher: null\n    title: null\n    url: https://arxiv.org/abs/1512.03385.pdf\nname: ResNet50\nversion: v1.5\ntask type: Image Classification\navailable postprocess versions: Python\n</code></pre></p>"},{"location":"developers-guide/","title":"Developer's Guide","text":"<p>This documentation is for developers who want to contribute to Furiosa Models.</p>"},{"location":"developers-guide/#release-guide","title":"Release guide","text":""},{"location":"developers-guide/#preparing-for-release","title":"Preparing for Release","text":"<ul> <li> Select a correct release tag to mark the release: <code>x.y.z</code></li> <li> Update the code in the <code>main</code> branch to reflect the next development version.<ul> <li> <code>__version__</code> field in <code>furiosa/models/__init__.py</code></li> </ul> </li> <li> Create a dedicated release branch on github for the new tag.</li> </ul>"},{"location":"developers-guide/#pre-release-tasks","title":"Pre-Release Tasks","text":"<p>Before releasing the new version, ensure that the following tasks are completed:</p> <ul> <li> Update the code in the release branch to the appropriate version.</li> <li> Generate a rendered documentation for the new release.</li> <li> Write a changelog that describes the changes made in the new release.</li> <li> Test the building of wheels by <code>flit build</code> to ensure the release is functional.</li> </ul>"},{"location":"developers-guide/#releasing-the-new-version","title":"Releasing the New Version","text":"<ul> <li> Publish the package to PyPI with\u00a0<code>flit publish</code> command. \ud83c\udf89</li> </ul>"},{"location":"getting_started/","title":"Getting Started","text":"<p>This documentation explains how to install furiosa-models, how to use available models in furiosa-models, and how to explore the documents.</p>"},{"location":"getting_started/#prerequisites","title":"Prerequisites","text":"<p><code>furiosa-models</code> can be installed on various Linux distributions, but it has been tested on the followings:</p> <ul> <li>CentOS 7 or higher</li> <li>Debian bullseye or higher</li> <li>Ubuntu 20.04 or higher</li> </ul> <p>The following packages should be installed, but the followings are installed by default in most systems. So, only when you have any dependency issue, you need to install the following packages:</p> <ul> <li>libstdc++6</li> <li>libgomp</li> </ul>"},{"location":"getting_started/#installing","title":"Installing","text":"<p>You can quickly install Furiosa Models by using <code>pip</code> as following:</p> <pre><code>pip install --upgrade pip setuptools wheel\npip install 'furiosa-models'\n</code></pre> <p>Info</p> <p>Older versions of wheel may reject the native-build wheels of furiosa-models. Please make sure of installing &amp; upgrading Python packaging tools before installing furiosa-models.</p> Building from Source Code (click to see) <p>Or you can build from the source code as following:</p> <pre><code>git clone https://github.com/furiosa-ai/furiosa-models\npip install ./furiosa-models\n</code></pre>"},{"location":"getting_started/#quick-example-and-guides","title":"Quick example and Guides","text":"<p>You can simply load a model and run through furiosa-sdk as the following:</p> <p>Info</p> <p>If you want to learn more about the installation of furiosa-sdk and how to use it, please refer to the followings:</p> <ul> <li>Driver, Firmware, and Runtime Installation</li> <li>Python SDK Installation and User Guide</li> <li>Furiosa SDK - Tutorial and Code Examples</li> </ul> <pre><code>from furiosa.models.vision import SSDMobileNet\nfrom furiosa.runtime.sync import create_runner\n\nimage = [\"tests/assets/cat.jpg\"]\n\nmobilenet = SSDMobileNet()\nwith create_runner(mobilenet.model_source()) as runner:\n    inputs, contexts = mobilenet.preprocess(image)\n    outputs = runner.run(inputs)\n    mobilenet.postprocess(outputs, contexts[0])\n</code></pre> <p>This example does:</p> <ol> <li>Load the SSDMobileNet model</li> <li>Create a <code>Runner</code>, which is one of the main classes of Furiosa Runtime, that can load an ONNX/tflite model onto NPU and run inferences.</li> <li>Run an inference with pre/post process functions.</li> </ol> <p>A <code>Model</code> instance is a Python object, including model artifacts, metadata, and its pre/postprocessors. You can learn more about <code>Model</code> object at Model object.</p> <p>Also, you can find all available models at Available Models. Each model page includes the details of the model, input and output tensors, and pre/post processings, and API reference.</p> <p>If you want to learn more about <code>Runner</code> in Furiosa Runtime, please refer to below links.</p> <ul> <li>Furiosa SDK - furiosa.runtime API Reference</li> <li>Furiosa SDK - furiosa.runtime.sync.create_runner Reference</li> <li>Furiosa SDK - Tutorial and Code Examples.</li> </ul>"},{"location":"model_object/","title":"Model Object","text":"<p>In the <code>furiosa-models</code> project, the <code>Model</code> is the primary class object, representing a neural network model. This document elucidates the offerings and uses of the <code>Model</code> object.</p>"},{"location":"model_object/#loading-a-pre-trained-model","title":"Loading a pre-trained model","text":"<p>To load a pre-trained neural network model, you need to invoke the <code>Model</code> object. As the sizes of pre-trained model weights can range from tens to hundreds of megabytes, the model images are not included in the Python package. The first time the model object is called, a pre-trained model will be fetched over the network. This process takes some time, typically a few seconds, depending on the models and network conditions. Once the model images are fetched, they will be cached on your local disk.</p> Load module <pre><code>from furiosa.models.types import Model\nfrom furiosa.models.vision import ResNet50\n\nmodel: Model = ResNet50()\n</code></pre> <p></p>"},{"location":"model_object/#accessing-artifacts-and-metadata","title":"Accessing artifacts and metadata","text":"<p>A Model object encompasses model artifacts, such as ONNX, TFLite, mapping from a tensor name to the tensor's min and max, and ENF.</p> <p>ENF is the serialization format of a compiled binary used in Furiosa SDK. Once you have the ENF file, you can reuse it to skip the compilation process, which can take up to several minutes. You can acquire the ENF binary from the model_source() method. In addition, a Model object contains various metadata attributes.</p> <p>Info</p> <p>If you want to learn more about the ENF please visit Furiosa SDK - Compiler - Using ENF files</p>"},{"location":"model_object/#furiosa.models.types.Model","title":"<code>furiosa.models.types.Model</code>","text":"<p>Represent the artifacts and metadata of a neural network model</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>a name of this model</p> <code>task_type</code> <code>ModelTaskType</code> <p>the task type of this model</p> <code>format</code> <code>Format</code> <p>the binary format type of model origin; e.g., ONNX, tflite</p> <code>family</code> <code>Optional[str]</code> <p>the model family</p> <code>version</code> <code>Optional[str]</code> <p>the model version</p> <code>metadata</code> <code>Optional[Metadata]</code> <p>the model metadata</p> <code>tags</code> <code>Optional[Tags]</code> <p>the model tags</p> <code>origin</code> <code>bytes</code> <p>an origin f32 binary in ONNX or tflite. It can be used for compiling this model with or without quantization and proper compiler configuration</p> <code>tensor_name_to_range</code> <code>Dict[str, List[float]]</code> <p>the calibration ranges of each tensor in origin</p> <code>preprocessor</code> <code>PreProcessor</code> <p>a preprocessor to preprocess input tensors</p> <code>postprocessor</code> <code>PostProcessor</code> <p>a postprocessor to postprocess output tensors</p>"},{"location":"model_object/#furiosa.models.types.Model.model_source","title":"<code>model_source(num_pe=2)</code>","text":"<p>Returns an executable binary for furiosa runtime and NPU. It can be     directly fed to <code>furiosa.runtime.create_runner</code>. If model binary is not compiled yet,     it will be quantized &amp; compiled automatically if possible</p> <p>Parameters:</p> Name Type Description Default <code>num_pe</code> <code>Literal[1, 2]</code> <p>number of PE to be used.</p> <code>2</code>"},{"location":"model_object/#furiosa.models.types.Model.postprocess","title":"<code>postprocess(*args, **kwargs)</code>","text":"<p>Postprocess output tensors</p>"},{"location":"model_object/#furiosa.models.types.Model.preprocess","title":"<code>preprocess(*args, **kwargs)</code>","text":"<p>Preprocess input tensors. Input of this function varies from model to model</p> <p>Returns:</p> Type Description <code>Tuple[Sequence[ArrayLike], Sequence[Context]]</code> <p>A tuple that contains list of preprocessed input tensors and contexts</p>"},{"location":"model_object/#furiosa.models.types.Model.resolve_all","title":"<code>resolve_all()</code>","text":"<p>Resolve all non-cached properties(origin, tensor_name_to_range, model_sources)</p>"},{"location":"model_object/#inferencing-with-runner-api","title":"Inferencing with Runner API","text":"<p>To create a Runner, you need to pass the ENF binary obtained from the <code>model_source()</code> method of the model object to the <code>furiosa.runtime.sync.create_runner</code> function. If you prefer an asynchronous Runner, you can use the <code>furiosa.runtime.create_runner</code> function instead. Passing the pre-compiled ENF binary allows you to perform inference directly without the compilation process. Alternatively, you can also manually quantize and compile the original f32 model with the provided calibration range.</p> <p>Info</p> <p>If you want to learn more about the installation of furiosa-sdk and how to use it, please follow the followings:</p> <ul> <li>Driver, Firmware, and Runtime Installation</li> <li>Python SDK Installation and User Guide</li> <li>Furiosa SDK - Tutorial and Code Examples</li> </ul> <p>Passing <code>Model.origin</code> to <code>create_runner()</code> allows users to start from source models in ONNX or tflite and customize models to their specific use-cases. This customization includes options such as specifying batch sizes and compiler configurations for optimization purposes. For additional information on Model.origin, please refer to Accessing artifacts and metadata.</p> <p>To work with f32 source models, calibration and quantization are essential steps. You can access pre-calibrated data directly from furiosa-models, simplifying the quantization process. If you prefer a manual quantization step for a model, you can install the <code>furiosa-quantizer</code> package, available at this package link. The <code>tensor_name_to_range</code> field within the model class contains this pre-calibrated data.</p> <p>Upon quantization, the output will be in FuriosaAI's Intermediate Representation (IR) format, which can then be passed to the Runner. At this stage, you have the option to specify the compiler configuration.</p> <p>After quantization, the output will be in the form of FuriosaAI's Intermediate Representation (IR) which can then be passed to the session. At this stage, the compiler configuration can be specified.</p> <p></p> <p>Example</p> Using pre-compiled ENF binary <pre><code>from furiosa.models.vision import SSDMobileNet\nfrom furiosa.runtime.sync import create_runner\n\nimage = [\"tests/assets/cat.jpg\"]\n\nmobilenet = SSDMobileNet()\nwith create_runner(mobilenet.model_source()) as runner:\n    inputs, contexts = mobilenet.preprocess(image)\n    outputs = runner.run(inputs)\n    mobilenet.postprocess(outputs, contexts[0])\n</code></pre> <p>Example</p> From ONNX <pre><code>from furiosa.models.vision import SSDMobileNet\nfrom furiosa.quantizer import quantize\nfrom furiosa.runtime.sync import create_runner\n\ncompiler_config = {\"lower_tabulated_dequantize\": True}\n\nimage = [\"tests/assets/cat.jpg\"]\n\nmobilenet = SSDMobileNet()\nonnx_model: bytes = mobilenet.origin\ntensor_name_to_range: dict = mobilenet.tensor_name_to_range\n\n# See https://furiosa-ai.github.io/docs/latest/en/api/python/furiosa.quantizer.html#furiosa.quantizer.quantize\n# for more details\nquantized_onnx = quantize(onnx_model, tensor_name_to_range)\n\nwith create_runner(quantized_onnx, compiler_config=compiler_config) as runner:\n    # Models in the Model Zoo have built-in optimizations that, by default,\n    # bypass normalization, quantization, and type conversion. If you compile\n    # and utilize these models without employing these optimizations, it's\n    # necessary to set up preprocessing steps to incorporate normalization and\n    # type casting. To accomplish this, you should introduce an extra parameter,\n    # `with_scaling=True`.\n    inputs, contexts = mobilenet.preprocess(image, with_scaling=True)\n    outputs = runner.run(inputs)\n    mobilenet.postprocess(outputs, contexts[0])\n</code></pre>"},{"location":"model_object/#prepostprocessing","title":"Pre/Postprocessing","text":"<p>There are gaps between model input/outputs and user applications' desired input and output data. In general, inputs and outputs of a neural network model are tensors. In applications, user sample data are images in standard formats like PNG or JPEG, and users also need to convert the output tensors to struct data for user applications.</p> <p>A <code>Model</code> object also provides both <code>preprocess()</code> and <code>postprocess()</code> methods. They are utilities to convert easily user inputs to the model's input tensors and output tensors to struct data which can be easily accessible by applications. If using pre-built pre/postprocessing methods, users can quickly start using <code>furiosa-models</code>.</p> <p>In sum, typical steps of a single inference is as the following, as also shown at examples.</p> <ol> <li>Call <code>preprocess()</code> with user inputs (e.g., image files)</li> <li>Pass an output of <code>preprocess()</code> to <code>Session.run()</code></li> <li>Pass the output of the model to <code>postprocess()</code></li> </ol> <p>Info</p> <p>Default postprocessing implementations are in Python. However, some models have the native postprocessing implemented in Rust and C++ and optimized for FuriosaAI Warboy and Intel/AMD CPUs. Python implementations can run on CPU and GPU as well, whereas the native postprocessor implementations works with only FuriosaAI NPU. Native implementations are designed to leverage FuriosaAI NPU's characteristics even for post-processing and maximize the latency and throughput by using modern CPU architecture, such as CPU cache, SIMD instructions and CPU pipelining. According to our benchmark, the native implementations show at most 70% lower latency.</p> <p>To use native post processor, please pass <code>postprocessor_type=Platform.RUST</code> to <code>Model()</code>.</p> <p>The following is an example to use native post processor for SSDMobileNet. You can find more details of each model page.</p> <p>Example</p> <pre><code>from furiosa.models.types import Platform\nfrom furiosa.models.vision import SSDMobileNet\nfrom furiosa.runtime.sync import create_runner\n\nimage = [\"tests/assets/cat.jpg\"]\n\nmobilenet = SSDMobileNet(postprocessor_type=Platform.RUST)\nwith create_runner(mobilenet.model_source()) as runner:\n    inputs, contexts = mobilenet.preprocess(image)\n    outputs = runner.run(inputs)\n    mobilenet.postprocess(outputs, contexts[0])\n</code></pre>"},{"location":"model_object/#see-also","title":"See Also","text":"<ul> <li>Furiosa SDK Documentation</li> </ul>"},{"location":"models/efficientnet_b0/","title":"EfficientNetB0","text":"<p>The EfficientNet originates from the \"EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks\" paper, which proposes a new compound scaling method that enables better performance on image classification tasks with fewer parameters. EfficientNet B0 is the smallest and most efficient model in the EfficientNet family, achieving state-of-the-art results on various image classification benchmarks with just 5.3 million parameters.</p>"},{"location":"models/efficientnet_b0/#overall","title":"Overall","text":"<ul> <li>Framework: PyTorch</li> <li>Model format: ONNX</li> <li>Model task: Image classification</li> <li>Source: github</li> </ul>"},{"location":"models/efficientnet_b0/#_1","title":"EfficientNetB0","text":"<p>Usages</p> Python Postprocessor <pre><code>from furiosa.models.vision import EfficientNetB0\nfrom furiosa.runtime.sync import create_runner\n\nimage = \"tests/assets/cat.jpg\"\n\neffnetb0 = EfficientNetB0()\nwith create_runner(effnetb0.model_source()) as runner:\n    inputs, _ = effnetb0.preprocess(image)\n    outputs = runner.run(inputs)\n    effnetb0.postprocess(outputs)\n</code></pre>"},{"location":"models/efficientnet_b0/#inputs","title":"Inputs","text":"<p>The input is a 3-channel image of 224x224 (height, width).</p> <ul> <li>Data Type: <code>numpy.float32</code></li> <li>Tensor Shape: <code>[1, 3, 224, 224]</code></li> <li>Memory Format: NCHW, where:<ul> <li>N - batch size</li> <li>C - number of channels</li> <li>H - image height</li> <li>W - image width</li> </ul> </li> <li>Color Order: BGR</li> <li>Optimal Batch Size (minimum: 1): &lt;= 16</li> </ul>"},{"location":"models/efficientnet_b0/#outputs","title":"Outputs","text":"<p>The output is a <code>numpy.float32</code> tensor with the shape (<code>[1,]</code>), including a class id. <code>postprocess()</code> transforms the class id to a label string.</p>"},{"location":"models/efficientnet_b0/#prepostprocessing","title":"Pre/Postprocessing","text":"<p><code>furiosa.models.vision.EfficientNetB0</code> class provides <code>preprocess</code> and <code>postprocess</code> methods that convert input images to input tensors and the model outputs to labels respectively. You can find examples at EfficientNetB0 Usage.</p>"},{"location":"models/efficientnet_b0/#furiosamodelsvisionefficientnetb0preprocess","title":"<code>furiosa.models.vision.EfficientNetB0.preprocess</code>","text":"<p>Read and preprocess an image located at image_path.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Union[str, Path, ArrayLike]</code> <p>A path of an image.</p> required <code>with_scaling</code> <code>bool</code> <p>Whether to apply model-specific techniques that involve scaling the model's input and converting its data type to float32. Refer to the code to gain a precise understanding of the techniques used. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, None]</code> <p>The first element of the tuple is a numpy array that meets the input requirements of the model. The second element of the tuple is unused in this model and has no value. To learn more information about the output numpy array, please refer to Inputs.</p>"},{"location":"models/efficientnet_b0/#furiosamodelsvisionefficientnetb0postprocess","title":"<code>furiosa.models.vision.EfficientNetB0.postprocess</code>","text":"<p>Convert the outputs of a model to a label string, such as car and cat.</p> <p>Parameters:</p> Name Type Description Default <code>model_outputs</code> <code>Sequence[ArrayLike]</code> <p>the outputs of the model. Please learn more about the output of model, please refer to Outputs.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A classified label, e.g., \"jigsaw puzzle\".</p>"},{"location":"models/efficientnet_b0/#notes-on-the-source-field-of-this-model","title":"Notes on the source field of this model","text":"<p>There is a significant update in sdk version 0.9.0, which involves that the Furiosa's quantization tool adopts DFG(Data Flow Graph) as its output format instead of onnx. DFG is an IR of FuriosaAI that supports more diverse quantization schemes than onnx and is more specialized for FuriosaAI\u2019s Warboy.</p> <p>The EfficientNetB0 we offer has been quantized with furiosa-sdk 0.9.0 and thus formatted in DFG. The ONNX file in the source field is the original f32 model, not yet quantized.</p> <p>In case you need to use a different batch size or start from scratch, you can either start from the DFG or use the original ONNX file (and repeat the quantization process).</p>"},{"location":"models/efficientnet_v2_s/","title":"EfficientNetV2-S","text":"<p>EfficientNetV2-S is the smallest and most efficient model in the EfficientNetV2 family. Introduced in the paper \"EfficientNetV2: Smaller Models and Faster Training\", EfficientNetV2-S achieves state-of-the-art performance on image classification tasks, and it can be trained much faster and has a smaller model size of up to 6.8 times when compared to previous state-of-the-art models. It uses a combination of advanced techniques such as Swish activation function, Squeeze-and-Excitation blocks, and efficient channel attention to optimize its performance and efficiency.</p>"},{"location":"models/efficientnet_v2_s/#overall","title":"Overall","text":"<ul> <li>Framework: PyTorch</li> <li>Model format: ONNX</li> <li>Model task: Image classification</li> <li>Source: torchvision</li> </ul>"},{"location":"models/efficientnet_v2_s/#_1","title":"EfficientNetV2-S","text":"<p>Usages</p> Python Postprocessor <pre><code>from furiosa.models.vision import EfficientNetV2s\nfrom furiosa.runtime.sync import create_runner\n\nimage = \"tests/assets/cat.jpg\"\n\neffnetv2s = EfficientNetV2s()\nwith create_runner(effnetv2s.model_source()) as runner:\n    inputs, _ = effnetv2s.preprocess(image)\n    outputs = runner.run(inputs)\n    effnetv2s.postprocess(outputs)\n</code></pre>"},{"location":"models/efficientnet_v2_s/#inputs","title":"Inputs","text":"<p>The input is a 3-channel image of 384x384 (height, width).</p> <ul> <li>Data Type: <code>numpy.float32</code></li> <li>Tensor Shape: <code>[1, 3, 384, 384]</code></li> <li>Memory Format: NCHW, where:<ul> <li>N - batch size</li> <li>C - number of channels</li> <li>H - image height</li> <li>W - image width</li> </ul> </li> <li>Color Order: BGR</li> <li>Optimal Batch Size (minimum: 1): &lt;= 8</li> </ul>"},{"location":"models/efficientnet_v2_s/#outputs","title":"Outputs","text":"<p>The output is a <code>numpy.float32</code> tensor with the shape (<code>[1,]</code>), including a class id. <code>postprocess()</code> transforms the class id to a label string.</p>"},{"location":"models/efficientnet_v2_s/#prepostprocessing","title":"Pre/Postprocessing","text":"<p><code>furiosa.models.vision.EfficientNetV2s</code> class provides <code>preprocess</code> and <code>postprocess</code> methods that convert input images to input tensors and the model outputs to labels respectively. You can find examples at EfficientNetV2-S Usage.</p>"},{"location":"models/efficientnet_v2_s/#furiosamodelsvisionefficientnetv2spreprocess","title":"<code>furiosa.models.vision.EfficientNetV2s.preprocess</code>","text":"<p>Read and preprocess an image located at image_path.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Union[str, Path, ArrayLike]</code> <p>A path of an image.</p> required <code>with_scaling</code> <code>bool</code> <p>Whether to apply model-specific techniques that involve scaling the model's input and converting its data type to float32. Refer to the code to gain a precise understanding of the techniques used. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, None]</code> <p>The first element of the tuple is a numpy array that meets the input requirements of the model. The second element of the tuple is unused in this model and has no value. To learn more information about the output numpy array, please refer to Inputs.</p>"},{"location":"models/efficientnet_v2_s/#furiosamodelsvisionefficientnetv2spostprocess","title":"<code>furiosa.models.vision.EfficientNetV2s.postprocess</code>","text":"<p>Convert the outputs of a model to a label string, such as car and cat.</p> <p>Parameters:</p> Name Type Description Default <code>model_outputs</code> <code>Sequence[ArrayLike]</code> <p>the outputs of the model. Please learn more about the output of model, please refer to Outputs.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A classified label, e.g., \"tabby, tabby cat\".</p>"},{"location":"models/efficientnet_v2_s/#notes-on-the-source-field-of-this-model","title":"Notes on the source field of this model","text":"<p>There is a significant update in sdk version 0.9.0, which involves that the Furiosa's quantization tool adopts DFG(Data Flow Graph) as its output format instead of onnx. DFG is an IR of FuriosaAI that supports more diverse quantization schemes than onnx and is more specialized for FuriosaAI\u2019s Warboy.</p> <p>The EfficientNetV2-S we offer has been quantized with furiosa-sdk 0.9.0 and thus formatted in DFG. The ONNX file in the source field is the original f32 model, not yet quantized.</p> <p>In case you need to use a different batch size or start from scratch, you can either start from the DFG or use the original ONNX file (and repeat the quantization process).</p>"},{"location":"models/resnet50_v1.5/","title":"ResNet50 v1.5","text":"<p>ResNet50 v1.5 backbone model trained on ImageNet (224x224). This model has been used since MLCommons v0.5.</p>"},{"location":"models/resnet50_v1.5/#overall","title":"Overall","text":"<ul> <li>Framework: PyTorch</li> <li>Model format: ONNX</li> <li>Model task: Image classification</li> <li>Source: This model is originated from ResNet50 v1.5 in ONNX available at MLCommons - Supported Models.</li> </ul>"},{"location":"models/resnet50_v1.5/#_1","title":"ResNet50 v1.5","text":"<p>Usages</p> Postprocessor <pre><code>from furiosa.models.vision import ResNet50\nfrom furiosa.runtime.sync import create_runner\n\nimage = \"tests/assets/cat.jpg\"\n\nresnet50 = ResNet50()\nwith create_runner(resnet50.model_source()) as runner:\n    inputs, _ = resnet50.preprocess(image)\n    outputs = runner.run(inputs)\n    resnet50.postprocess(outputs)\n</code></pre>"},{"location":"models/resnet50_v1.5/#inputs","title":"Inputs","text":"<p>The input is a 3-channel image of 224x224 (height, width).</p> <ul> <li>Data Type: <code>numpy.float32</code></li> <li>Tensor Shape: <code>[1, 3, 224, 224]</code></li> <li>Memory Format: NCHW, where:<ul> <li>N - batch size</li> <li>C - number of channels</li> <li>H - image height</li> <li>W - image width</li> </ul> </li> <li>Color Order: BGR</li> <li>Optimal Batch Size (minimum: 1): &lt;= 8</li> </ul>"},{"location":"models/resnet50_v1.5/#outputs","title":"Outputs","text":"<p>The output is a <code>numpy.float32</code> tensor with the shape (<code>[1,]</code>), including a class id. <code>postprocess()</code> transforms the class id to a label string.</p>"},{"location":"models/resnet50_v1.5/#prepostprocessing","title":"Pre/Postprocessing","text":"<p><code>furiosa.models.vision.ResNet50</code> class provides <code>preprocess</code> and <code>postprocess</code> methods that convert input images to input tensors and the model outputs to labels respectively. You can find examples at ResNet50 Usage.</p>"},{"location":"models/resnet50_v1.5/#furiosamodelsvisionresnet50preprocess","title":"<code>furiosa.models.vision.ResNet50.preprocess</code>","text":"<p>Convert an input image to a model input tensor</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Union[str, ArrayLike]</code> <p>A path of an image or an image loaded as a numpy array in BGR order.</p> required <code>with_scaling</code> <code>bool</code> <p>Whether to apply model-specific techniques that involve scaling the model's input and converting its data type to float32. Refer to the code to gain a precise understanding of the techniques used. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, None]</code> <p>The first element of the tuple is a numpy array that meets the input requirements of the ResNet50 model. The second element of the tuple is unused in this model and has no value. To learn more information about the output numpy array, please refer to Inputs.</p>"},{"location":"models/resnet50_v1.5/#furiosamodelsvisionresnet50postprocess","title":"<code>furiosa.models.vision.ResNet50.postprocess</code>","text":""},{"location":"models/ssd_mobilenet/","title":"SSD MobileNet v1","text":"<p>SSD MobileNet v1 backbone model trained on COCO (300x300). This model has been used since MLCommons v0.5.</p>"},{"location":"models/ssd_mobilenet/#overall","title":"Overall","text":"<ul> <li>Framework: PyTorch</li> <li>Model format: ONNX</li> <li>Model task: Object detection</li> <li>Source: This model is originated from SSD MobileNet v1 in ONNX available at MLCommons - Supported Models.</li> </ul>"},{"location":"models/ssd_mobilenet/#_1","title":"SSD MobileNet v1","text":"<p>Usages</p> Python PostprocessorNative Postprocessor <pre><code>from furiosa.models.vision import SSDMobileNet\nfrom furiosa.runtime.sync import create_runner\n\nimage = [\"tests/assets/cat.jpg\"]\n\nmobilenet = SSDMobileNet()\nwith create_runner(mobilenet.model_source()) as runner:\n    inputs, contexts = mobilenet.preprocess(image)\n    outputs = runner.run(inputs)\n    mobilenet.postprocess(outputs, contexts[0])\n</code></pre> <pre><code>from furiosa.models.types import Platform\nfrom furiosa.models.vision import SSDMobileNet\nfrom furiosa.runtime.sync import create_runner\n\nimage = [\"tests/assets/cat.jpg\"]\n\nmobilenet = SSDMobileNet(postprocessor_type=Platform.RUST)\nwith create_runner(mobilenet.model_source()) as runner:\n    inputs, contexts = mobilenet.preprocess(image)\n    outputs = runner.run(inputs)\n    mobilenet.postprocess(outputs, contexts[0])\n</code></pre>"},{"location":"models/ssd_mobilenet/#inputs","title":"Inputs","text":"<p>The input is a 3-channel image of 300x300 (height, width).</p> <ul> <li>Data Type: <code>numpy.float32</code></li> <li>Tensor Shape: <code>[1, 3, 300, 300]</code></li> <li>Memory Format: NCHW, where:<ul> <li>N - batch size</li> <li>C - number of channels</li> <li>H - image height</li> <li>W - image width</li> </ul> </li> <li>Color Order: RGB</li> <li>Optimal Batch Size (minimum: 1): &lt;= 8</li> </ul>"},{"location":"models/ssd_mobilenet/#outputs","title":"Outputs","text":"<p>The outputs are 12 <code>numpy.float32</code> tensors in various shapes as the following. You can refer to <code>postprocess()</code> function to learn how to decode boxes, classes, and confidence scores.</p> Tensor Shape Data Type Data Type Description 0 (1, 273, 19, 19) float32 NCHW 1 (1, 12, 19, 19) float32 NCHW 2 (1, 546, 10, 10) float32 NCHW 3 (1, 24, 10, 10) float32 NCHW 4 (1, 546, 5, 5) float32 NCHW 5 (1, 24, 5, 5) float32 NCHW 6 (1, 546, 3, 3) float32 NCHW 7 (1, 24, 3, 3) float32 NCHW 8 (1, 546, 2, 2) float32 NCHW 9 (1, 24, 2, 2) float32 NCHW 10 (1, 546, 1, 1) float32 NCHW 11 (1, 24, 1, 1) float32 NCHW"},{"location":"models/ssd_mobilenet/#prepostprocessing","title":"Pre/Postprocessing","text":"<p><code>furiosa.models.vision.SSDMobileNet</code> class provides <code>preprocess</code> and <code>postprocess</code> methods. <code>preprocess</code> method converts input images to input tensors, and <code>postprocess</code> method converts  model output tensors to a list of bounding boxes, scores and labels.  You can find examples at SSDMobileNet Usage.</p>"},{"location":"models/ssd_mobilenet/#furiosamodelsvisionssdmobilenetpreprocess","title":"<code>furiosa.models.vision.SSDMobileNet.preprocess</code>","text":"<p>Preprocess input images to a batch of input tensors.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>Sequence[Union[str, ndarray]]</code> <p>A list of paths of image files (e.g., JPEG, PNG) or a stacked image loaded as a numpy array in BGR order or gray order.</p> required <code>with_scaling</code> <code>bool</code> <p>Whether to apply model-specific techniques that involve scaling the model's input and converting its data type to float32. Refer to the code to gain a precise understanding of the techniques used. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[ArrayLike, List[Dict[str, Any]]]</code> <p>The first element is 3-channel images of 300x300 in NCHW format, and the second element is a list of context about the original image metadata. This context data should be passed and utilized during post-processing. To learn more about the outputs of preprocess (i.e., model inputs), please refer to Inputs.</p>"},{"location":"models/ssd_mobilenet/#furiosamodelsvisionssdmobilenetpostprocess","title":"<code>furiosa.models.vision.SSDMobileNet.postprocess</code>","text":"<p>Convert the outputs of this model to a list of bounding boxes, scores and labels</p> <p>Parameters:</p> Name Type Description Default <code>model_outputs</code> <code>Sequence[ndarray]</code> <p>the outputs of the model. To learn more about the output of model, please refer to Outputs.</p> required <code>contexts</code> <code>Sequence[Dict[str, Any]]</code> <p>context coming from <code>preprocess()</code></p> required <p>Returns:</p> Type Description <code>List[List[ObjectDetectionResult]]</code> <p>Detected Bounding Box and its score and label represented as <code>ObjectDetectionResult</code>. To learn more about <code>ObjectDetectionResult</code>, 'Definition of ObjectDetectionResult' can be found below.</p> Definitions of ObjectDetectionResult and LtrbBoundingBox Source code in <code>furiosa/models/vision/postprocess.py</code> <pre><code>@dataclass\nclass LtrbBoundingBox:\n    left: float\n    top: float\n    right: float\n    bottom: float\n\n    def __iter__(self) -&gt; Iterator[float]:\n        return iter([self.left, self.top, self.right, self.bottom])\n</code></pre> Source code in <code>furiosa/models/vision/postprocess.py</code> <pre><code>@dataclass\nclass ObjectDetectionResult:\n    boundingbox: LtrbBoundingBox\n    score: float\n    label: str\n    index: int\n</code></pre>"},{"location":"models/ssd_mobilenet/#native-postprocessor","title":"Native Postprocessor","text":"<p>This class provides another version of the postprocessing implementation which is highly optimized for NPU. The implementation leverages the NPU IO architecture and runtime.</p> <p>To use this implementation, when this model is called, the parameter <code>postprocessor_type=Platform.RUST</code> should be passed. The following is an example:</p> <p>Example</p> <pre><code>from furiosa.models.types import Platform\nfrom furiosa.models.vision import SSDMobileNet\nfrom furiosa.runtime.sync import create_runner\n\nimage = [\"tests/assets/cat.jpg\"]\n\nmobilenet = SSDMobileNet(postprocessor_type=Platform.RUST)\nwith create_runner(mobilenet.model_source()) as runner:\n    inputs, contexts = mobilenet.preprocess(image)\n    outputs = runner.run(inputs)\n    mobilenet.postprocess(outputs, contexts[0])\n</code></pre>"},{"location":"models/ssd_resnet34/","title":"SSD ResNet34","text":"<p>SSD ResNet34 backbone model trained on COCO (1200x1200). This model has been used since MLCommons v0.5.</p>"},{"location":"models/ssd_resnet34/#overall","title":"Overall","text":"<ul> <li>Framework: PyTorch</li> <li>Model format: ONNX</li> <li>Model task: Object detection</li> <li>Source: This model is originated from SSD ResNet34 in ONNX available at MLCommons - Supported Models.</li> </ul>"},{"location":"models/ssd_resnet34/#_1","title":"SSD ResNet34","text":"<p>Usages</p> Python PostprocessorNative Postprocessor <pre><code>from furiosa.models.vision import SSDResNet34\nfrom furiosa.runtime.sync import create_runner\n\nresnet34 = SSDResNet34(postprocessor_type=\"Python\")\n\nwith create_runner(resnet34.model_source()) as runner:\n    image, contexts = resnet34.preprocess([\"tests/assets/cat.jpg\"])\n    output = runner.run(image)\n    resnet34.postprocess(output, contexts=contexts)\n</code></pre> <pre><code>from furiosa.models.types import Platform\nfrom furiosa.models.vision import SSDResNet34\nfrom furiosa.runtime.sync import create_runner\n\nresnet34 = SSDResNet34(postprocessor_type=Platform.RUST)\n\nwith create_runner(resnet34.model_source()) as runner:\n    image, contexts = resnet34.preprocess([\"tests/assets/cat.jpg\"])\n    output = runner.run(image)\n    resnet34.postprocessor(output, contexts=contexts[0])\n</code></pre>"},{"location":"models/ssd_resnet34/#inputs","title":"Inputs","text":"<p>The input is a 3-channel image of 300x300 (height, width).</p> <ul> <li>Data Type: <code>numpy.float32</code></li> <li>Tensor Shape: <code>[1, 3, 1200, 1200]</code></li> <li>Memory Format: NCHW, where<ul> <li>N - batch size</li> <li>C - number of channels</li> <li>H - image height</li> <li>W - image width</li> </ul> </li> <li>Color Order: RGB</li> <li>Optimal Batch Size: 1</li> </ul>"},{"location":"models/ssd_resnet34/#outputs","title":"Outputs","text":"<p>The outputs are 12 <code>numpy.float32</code> tensors in various shapes as the following.  You can refer to <code>postprocess()</code> function to learn how to decode boxes, classes, and confidence scores.</p> Tensor Shape Data Type Data Type Description 0 (1, 324, 50, 50) float32 NCHW 1 (1, 486, 25, 25) float32 NCHW 2 (1, 486, 13, 13) float32 NCHW 3 (1, 486, 7, 7) float32 NCHW 4 (1, 324, 3, 3) float32 NCHW 5 (1, 324, 3, 3) float32 NCHW 6 (1, 16, 50, 50) float32 NCHW 7 (1, 24, 25, 25) float32 NCHW 8 (1, 24, 13, 13) float32 NCHW 9 (1, 24, 7, 7) float32 NCHW 10 (1, 16, 3, 3) float32 NCHW 11 (1, 16, 3, 3) float32 NCHW"},{"location":"models/ssd_resnet34/#prepostprocessing","title":"Pre/Postprocessing","text":"<p><code>furiosa.models.vision.SSDResNet34</code> class provides <code>preprocess</code> and <code>postprocess</code> methods. <code>preprocess</code> method converts input images to input tensors, and <code>postprocess</code> method converts  model output tensors to a list of bounding boxes, scores and labels.  You can find examples at SSDResNet34 Usage.</p>"},{"location":"models/ssd_resnet34/#furiosamodelsvisionssdresnet34preprocess","title":"<code>furiosa.models.vision.SSDResNet34.preprocess</code>","text":"<p>Preprocess input images to a batch of input tensors</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>Sequence[Union[str, ndarray]]</code> <p>A list of paths of image files (e.g., JPEG, PNG) or a stacked image loaded as a numpy array in BGR order or gray order.</p> required <code>with_scaling</code> <code>bool</code> <p>Whether to apply model-specific techniques that involve scaling the model's input and converting its data type to float32. Refer to the code to gain a precise understanding of the techniques used. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[ArrayLike, List[Dict[str, Any]]]</code> <p>The first element is a list of 3-channel images of 1200x1200 in NCHW format, and the second element is a list of context about the original image metadata. This context data should be passed and utilized during post-processing. To learn more about the outputs of preprocess (i.e., model inputs), please refer to Inputs.</p>"},{"location":"models/ssd_resnet34/#furiosamodelsvisionssdresnet34postprocess","title":"<code>furiosa.models.vision.SSDResNet34.postprocess</code>","text":"<p>Convert the outputs of this model to a list of bounding boxes, scores and labels</p> <p>Parameters:</p> Name Type Description Default <code>model_outputs</code> <code>Sequence[ndarray]</code> <p>the outputs of the model. To learn more about the output of model, please refer to Outputs.</p> required <code>contexts</code> <code>Sequence[Dict[str, Any]]</code> <p>context coming from <code>preprocess()</code></p> required <p>Returns:</p> Type Description <code>List[List[ObjectDetectionResult]]</code> <p>Detected Bounding Box and its score and label represented as <code>ObjectDetectionResult</code>. To learn more about <code>ObjectDetectionResult</code>, 'Definition of ObjectDetectionResult' can be found below.</p> Definition of ObjectDetectionResult and LtrbBoundingBox Source code in <code>furiosa/models/vision/postprocess.py</code> <pre><code>@dataclass\nclass LtrbBoundingBox:\n    left: float\n    top: float\n    right: float\n    bottom: float\n\n    def __iter__(self) -&gt; Iterator[float]:\n        return iter([self.left, self.top, self.right, self.bottom])\n</code></pre> Source code in <code>furiosa/models/vision/postprocess.py</code> <pre><code>@dataclass\nclass ObjectDetectionResult:\n    boundingbox: LtrbBoundingBox\n    score: float\n    label: str\n    index: int\n</code></pre>"},{"location":"models/ssd_resnet34/#native-postprocessor","title":"Native Postprocessor","text":"<p>This class provides another version of the postprocessing implementation which is highly optimized for NPU. The implementation leverages the NPU IO architecture and runtime.</p> <p>To use this implementation, when this model is called, the parameter <code>postprocessor_type=Platform.RUST</code> should be passed. The following is an example:</p> <p>Example</p> <pre><code>from furiosa.models.types import Platform\nfrom furiosa.models.vision import SSDResNet34\nfrom furiosa.runtime.sync import create_runner\n\nresnet34 = SSDResNet34(postprocessor_type=Platform.RUST)\n\nwith create_runner(resnet34.model_source()) as runner:\n    image, contexts = resnet34.preprocess([\"tests/assets/cat.jpg\"])\n    output = runner.run(image)\n    resnet34.postprocessor(output, contexts=contexts[0])\n</code></pre>"},{"location":"models/yolov5l/","title":"YOLOv5L","text":"<p>YOLOv5 is the one of the most popular object detection models. You can find more details at https://github.com/ultralytics/yolov5.</p>"},{"location":"models/yolov5l/#overall","title":"Overall","text":"<ul> <li>Framework: PyTorch</li> <li>Model format: ONNX</li> <li>Model task: Object Detection</li> <li>Source: https://github.com/ultralytics/yolov5</li> </ul>"},{"location":"models/yolov5l/#_1","title":"YOLOv5L","text":"<p>Usage</p> <pre><code>import cv2\nimport numpy as np\n\nfrom furiosa.models.vision import YOLOv5l\nfrom furiosa.runtime.sync import create_runner\n\nyolov5l = YOLOv5l()\n\nwith create_runner(yolov5l.model_source()) as runner:\n    image = cv2.imread(\"tests/assets/yolov5-test.jpg\")\n    inputs, contexts = yolov5l.preprocess([image])\n    output = runner.run(np.expand_dims(inputs[0], axis=0))\n    yolov5l.postprocess(output, contexts=contexts)\n</code></pre>"},{"location":"models/yolov5l/#inputs","title":"Inputs","text":"<p>The input is a 3-channel image of 640, 640 (height, width).</p> <ul> <li>Data Type: <code>numpy.uint8</code></li> <li>Tensor Shape: <code>[1, 640, 640, 3]</code></li> <li>Memory Format: NHWC, where<ul> <li>N - batch size</li> <li>H - image height</li> <li>W - image width</li> <li>C - number of channels</li> </ul> </li> <li>Color Order: RGB</li> <li>Optimal Batch Size (minimum: 1): &lt;= 2</li> </ul>"},{"location":"models/yolov5l/#outputs","title":"Outputs","text":"<p>The outputs are 3 <code>numpy.float32</code> tensors in various shapes as the following.  You can refer to <code>postprocess()</code> function to learn how to decode boxes, classes, and confidence scores.</p> Tensor Shape Data Type Data Type Description 0 (1, 45, 80, 80) float32 NCHW 1 (1, 45, 40, 40) float32 NCHW 2 (1, 45, 20, 20) float32 NCHW"},{"location":"models/yolov5l/#prepostprocessing","title":"Pre/Postprocessing","text":"<p><code>furiosa.models.vision.YOLOv5l</code> class provides <code>preprocess</code> and <code>postprocess</code> methods. <code>preprocess</code> method converts input images to input tensors, and <code>postprocess</code> method converts  model output tensors to a list of bounding boxes, scores and labels.  You can find examples at YOLOv5l Usage.</p>"},{"location":"models/yolov5l/#furiosamodelsvisionyolov5lpreprocess","title":"<code>furiosa.models.vision.YOLOv5l.preprocess</code>","text":"<p>Preprocess input images to a batch of input tensors</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>Sequence[Union[str, ndarray]]</code> <p>Color images have (NHWC: Batch, Height, Width, Channel) dimensions.</p> required <code>with_scaling</code> <code>bool</code> <p>Whether to apply model-specific techniques that involve scaling the model's input and converting its data type to float32. Refer to the code to gain a precise understanding of the techniques used. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, List[Dict[str, Any]]]</code> <p>a pre-processed image, scales and padded sizes(width,height) per images. The first element is a stacked numpy array containing a batch of images. To learn more about the outputs of preprocess (i.e., model inputs), please refer to YOLOv5l Inputs or YOLOv5m Inputs.</p> <p>The second element is a list of dict objects about the original images. Each dict object has the following keys. 'scale' key of the returned dict has a rescaled ratio per width(=target/width) and height(=target/height), and the 'pad' key has padded width and height pixels. Specially, the last dictionary element of returning tuple will be passed to postprocessing as a parameter to calculate predicted coordinates on normalized coordinates back to an input image coordinator.</p>"},{"location":"models/yolov5l/#furiosamodelsvisionyolov5lpostprocess","title":"<code>furiosa.models.vision.YOLOv5l.postprocess</code>","text":"<p>Convert the outputs of this model to a list of bounding boxes, scores and labels</p> <p>Parameters:</p> Name Type Description Default <code>model_outputs</code> <code>Sequence[ndarray]</code> <p>P3/8, P4/16, P5/32 features from yolov5l model. To learn more about the outputs of preprocess (i.e., model inputs), please refer to YOLOv5l Outputs or YOLOv5m Outputs.</p> required <code>contexts</code> <code>Sequence[Dict[str, Any]]</code> <p>A configuration for each image generated by the preprocessor. For example, it could be the reduction ratio of the image, the actual image width and height.</p> required <code>conf_thres</code> <code>float</code> <p>Confidence score threshold. The default to 0.25</p> <code>0.25</code> <code>iou_thres</code> <code>float</code> <p>IoU threshold value for the NMS processing. The default to 0.45.</p> <code>0.45</code> <p>Returns:</p> Type Description <code>List[List[ObjectDetectionResult]]</code> <p>Detected Bounding Box and its score and label represented as <code>ObjectDetectionResult</code>. The details of <code>ObjectDetectionResult</code> can be found below.</p> Definition of ObjectDetectionResult and LtrbBoundingBox Source code in <code>furiosa/models/vision/postprocess.py</code> <pre><code>@dataclass\nclass LtrbBoundingBox:\n    left: float\n    top: float\n    right: float\n    bottom: float\n\n    def __iter__(self) -&gt; Iterator[float]:\n        return iter([self.left, self.top, self.right, self.bottom])\n</code></pre> Source code in <code>furiosa/models/vision/postprocess.py</code> <pre><code>@dataclass\nclass ObjectDetectionResult:\n    boundingbox: LtrbBoundingBox\n    score: float\n    label: str\n    index: int\n</code></pre>"},{"location":"models/yolov5m/","title":"YOLOv5m","text":"<p>YOLOv5 is the one of the most popular object detection models. You can find more details at https://github.com/ultralytics/yolov5.</p>"},{"location":"models/yolov5m/#overall","title":"Overall","text":"<ul> <li>Framework: PyTorch</li> <li>Model format: ONNX</li> <li>Model task: Object Detection</li> <li>Source: https://github.com/ultralytics/yolov5.</li> </ul>"},{"location":"models/yolov5m/#_1","title":"YOLOv5m","text":"<p>Usage</p> <pre><code>import cv2\nimport numpy as np\n\nfrom furiosa.models.vision import YOLOv5m\nfrom furiosa.runtime.sync import create_runner\n\nyolov5m = YOLOv5m()\n\nwith create_runner(yolov5m.model_source()) as runner:\n    image = cv2.imread(\"tests/assets/yolov5-test.jpg\")\n    inputs, contexts = yolov5m.preprocess([image])\n    output = runner.run(np.expand_dims(inputs[0], axis=0))\n    yolov5m.postprocess(output, contexts=contexts)\n</code></pre>"},{"location":"models/yolov5m/#inputs","title":"Inputs","text":"<p>The input is a 3-channel image of 640, 640 (height, width).</p> <ul> <li>Data Type: <code>numpy.uint8</code></li> <li>Tensor Shape: <code>[1, 640, 640, 3]</code></li> <li>Memory Format: NHWC, where<ul> <li>N - batch size</li> <li>H - image height</li> <li>W - image width</li> <li>C - number of channels</li> </ul> </li> <li>Color Order: RGB</li> <li>Optimal Batch Size (minimum: 1): &lt;= 4</li> </ul>"},{"location":"models/yolov5m/#outputs","title":"Outputs","text":"<p>The outputs are 3 <code>numpy.float32</code> tensors in various shapes as the following.  You can refer to <code>postprocess()</code> function to learn how to decode boxes, classes, and confidence scores.</p> Tensor Shape Data Type Data Type Description 0 (1, 45, 80, 80) float32 NCHW 1 (1, 45, 40, 40) float32 NCHW 2 (1, 45, 20, 20) float32 NCHW"},{"location":"models/yolov5m/#prepostprocessing","title":"Pre/Postprocessing","text":"<p><code>furiosa.models.vision.YOLOv5m</code> class provides <code>preprocess</code> and <code>postprocess</code> methods. <code>preprocess</code> method converts input images to input tensors, and <code>postprocess</code> method converts  model output tensors to a list of bounding boxes, scores and labels.  You can find examples at YOLOv5m Usage.</p>"},{"location":"models/yolov5m/#furiosamodelsvisionyolov5mpreprocess","title":"<code>furiosa.models.vision.YOLOv5m.preprocess</code>","text":"<p>Preprocess input images to a batch of input tensors</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>Sequence[Union[str, ndarray]]</code> <p>Color images have (NHWC: Batch, Height, Width, Channel) dimensions.</p> required <code>with_scaling</code> <code>bool</code> <p>Whether to apply model-specific techniques that involve scaling the model's input and converting its data type to float32. Refer to the code to gain a precise understanding of the techniques used. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, List[Dict[str, Any]]]</code> <p>a pre-processed image, scales and padded sizes(width,height) per images. The first element is a stacked numpy array containing a batch of images. To learn more about the outputs of preprocess (i.e., model inputs), please refer to YOLOv5l Inputs or YOLOv5m Inputs.</p> <p>The second element is a list of dict objects about the original images. Each dict object has the following keys. 'scale' key of the returned dict has a rescaled ratio per width(=target/width) and height(=target/height), and the 'pad' key has padded width and height pixels. Specially, the last dictionary element of returning tuple will be passed to postprocessing as a parameter to calculate predicted coordinates on normalized coordinates back to an input image coordinator.</p>"},{"location":"models/yolov5m/#furiosamodelsvisionyolov5mpostprocess","title":"<code>furiosa.models.vision.YOLOv5m.postprocess</code>","text":"<p>Convert the outputs of this model to a list of bounding boxes, scores and labels</p> <p>Parameters:</p> Name Type Description Default <code>model_outputs</code> <code>Sequence[ndarray]</code> <p>P3/8, P4/16, P5/32 features from yolov5l model. To learn more about the outputs of preprocess (i.e., model inputs), please refer to YOLOv5l Outputs or YOLOv5m Outputs.</p> required <code>contexts</code> <code>Sequence[Dict[str, Any]]</code> <p>A configuration for each image generated by the preprocessor. For example, it could be the reduction ratio of the image, the actual image width and height.</p> required <code>conf_thres</code> <code>float</code> <p>Confidence score threshold. The default to 0.25</p> <code>0.25</code> <code>iou_thres</code> <code>float</code> <p>IoU threshold value for the NMS processing. The default to 0.45.</p> <code>0.45</code> <p>Returns:</p> Type Description <code>List[List[ObjectDetectionResult]]</code> <p>Detected Bounding Box and its score and label represented as <code>ObjectDetectionResult</code>. The details of <code>ObjectDetectionResult</code> can be found below.</p> Definition of ObjectDetectionResult and LtrbBoundingBox Source code in <code>furiosa/models/vision/postprocess.py</code> <pre><code>@dataclass\nclass LtrbBoundingBox:\n    left: float\n    top: float\n    right: float\n    bottom: float\n\n    def __iter__(self) -&gt; Iterator[float]:\n        return iter([self.left, self.top, self.right, self.bottom])\n</code></pre> Source code in <code>furiosa/models/vision/postprocess.py</code> <pre><code>@dataclass\nclass ObjectDetectionResult:\n    boundingbox: LtrbBoundingBox\n    score: float\n    label: str\n    index: int\n</code></pre>"},{"location":"models/yolov7_w6_pose/","title":"YOLOv7w6Pose","text":"<p>YOLOv7w6 Pose Estimation Model. You can find more details at https://github.com/WongKinYiu/yolov7#pose-estimation.</p>"},{"location":"models/yolov7_w6_pose/#overall","title":"Overall","text":"<ul> <li>Framework: PyTorch</li> <li>Model format: ONNX</li> <li>Model task: Pose Estimation</li> <li>Source: https://github.com/WongKinYiu/yolov7#pose-estimation.</li> </ul>"},{"location":"models/yolov7_w6_pose/#_1","title":"YOLOv7w6Pose","text":"<p>Usage</p> <pre><code>import cv2\nimport numpy as np\n\nfrom furiosa.models.vision import YOLOv7w6Pose\nfrom furiosa.runtime.sync import create_runner\n\nyolov5m = YOLOv7w6Pose()\n\nwith create_runner(yolov5m.model_source()) as runner:\n    image = cv2.imread(\"tests/assets/yolov5-test.jpg\")\n    inputs, contexts = yolov5m.preprocess([image])\n    output = runner.run(np.expand_dims(inputs[0], axis=0))\n    yolov5m.postprocess(output, contexts=contexts)\n</code></pre>"},{"location":"models/yolov7_w6_pose/#inputs","title":"Inputs","text":"<p>The input is a 3-channel image of 384, 640 (height, width).</p> <ul> <li>Data Type: <code>numpy.uint8</code></li> <li>Tensor Shape: <code>[1, 3, 384, 640]</code></li> <li>Memory Format: NHWC, where<ul> <li>N - batch size</li> <li>C - number of channels</li> <li>H - image height</li> <li>W - image width</li> </ul> </li> <li>Color Order: RGB</li> <li>Optimal Batch Size (minimum: 1): &lt;= 4</li> </ul>"},{"location":"models/yolov7_w6_pose/#outputs","title":"Outputs","text":"<p>The outputs are 3 <code>numpy.float32</code> tensors in various shapes as the following. You can refer to <code>postprocess()</code> function to learn how to decode boxes, classes, and confidence scores.</p> Tensor Shape Data Type Data Type Description 0 (1, 18, 48, 80) float32 NCHW 1 (1, 153, 48, 80) float32 NCHW 2 (1, 18, 24, 40) float32 NCHW 3 (1, 153, 24, 40) float32 NCHW 4 (1, 18, 12, 20) float32 NCHW 5 (1, 153, 12, 20) float32 NCHW 6 (1, 18, 6, 10) float32 NCHW 7 (1, 153, 6, 10) float32 NCHW"},{"location":"models/yolov7_w6_pose/#prepostprocessing","title":"Pre/Postprocessing","text":"<p><code>furiosa.models.vision.YOLOv7w6Pose</code> class provides <code>preprocess</code> and <code>postprocess</code> methods. <code>preprocess</code> method converts input images to input tensors, and <code>postprocess</code> method converts model output tensors to a list of <code>PoseEstimationResult</code>. You can find examples at YOLOv7w6Pose Usage.</p>"},{"location":"models/yolov7_w6_pose/#furiosamodelsvisionyolov7w6posepreprocess","title":"<code>furiosa.models.vision.YOLOv7w6Pose.preprocess</code>","text":"<p>Preprocess input images to a batch of input tensors</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>Sequence[Union[str, ndarray]]</code> <p>Color images have (NHWC: Batch, Height, Width, Channel) dimensions.</p> required <code>with_scaling</code> <code>bool</code> <p>Whether to apply model-specific techniques that involve scaling the model's input and converting its data type to float32. Refer to the code to gain a precise understanding of the techniques used. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, List[Dict[str, Any]]]</code> <p>a pre-processed image, scales and padded sizes(width,height) per images. The first element is a stacked numpy array containing a batch of images. To learn more about the outputs of preprocess (i.e., model inputs), please refer to YOLOv7w6Pose Inputs.</p> <p>The second element is a list of dict objects about the original images. Each dict object has the following keys. 'scale' key of the returned dict has a rescaled ratio per width(=target/width) and height(=target/height), and the 'pad' key has padded width and height pixels. Specially, the last dictionary element of returning tuple will be passed to postprocessing as a parameter to calculate predicted coordinates on normalized coordinates back to an input image coordinator.</p>"},{"location":"models/yolov7_w6_pose/#furiosamodelsvisionyolov7w6posepostprocess","title":"<code>furiosa.models.vision.YOLOv7w6Pose.postprocess</code>","text":"<p>Postprocess output tensors to a list of <code>PoseEstimationResult</code>. It transforms the model's output into a list of <code>PoseEstimationResult</code> instances. Each <code>PoseEstimationResult</code> contains information about the overall pose, including a bounding box, confidence score, and keypoint details such as nose, eyes, shoulders, etc. Please refer to the followings for more details.</p>"},{"location":"models/yolov7_w6_pose/#keypoint","title":"<code>Keypoint</code>","text":"<p>The <code>Keypoint</code> class represents a keypoint detected by the YOLOv7W6 Pose Estimation model. It contains the following attributes:</p> Attribute Description <code>x</code> The x-coordinate of the keypoint as a floating-point number. <code>y</code> The y-coordinate of the keypoint as a floating-point number. <code>confidence</code> Confidence score associated with the keypoint as a floating-point number. <p>See the source code for more details.</p> Source code in <code>furiosa/models/vision/yolov7_w6_pose/postprocess.py</code> <pre><code>class Keypoint(BaseModel):\n    x: float\n    y: float\n    confidence: float\n</code></pre>"},{"location":"models/yolov7_w6_pose/#poseestimationresult","title":"<code>PoseEstimationResult</code>","text":"<p>The <code>PoseEstimationResult</code> class represents the overall result of the YOLOv7W6 Pose Estimation model. It includes the following attributes:</p> Attribute Description <code>bounding_box</code> A list of four floating-point numbers representing the bounding box coordinates of the detected pose. <code>confidence</code> Confidence score associated with the overall pose estimation as a floating-point number. <code>nose</code> Instance of the <code>Keypoint</code> class representing the nose keypoint. <code>left_eye</code> Instance of the <code>Keypoint</code> class representing the left eye keypoint. <code>right_eye</code> Instance of the <code>Keypoint</code> class representing the right eye keypoint. <code>left_ear</code> Instance of the <code>Keypoint</code> class representing the left ear keypoint. <code>right_ear</code> Instance of the <code>Keypoint</code> class representing the right ear keypoint. <code>left_shoulder</code> Instance of the <code>Keypoint</code> class representing the left shoulder keypoint. <code>right_shoulder</code> Instance of the <code>Keypoint</code> class representing the right shoulder keypoint. <code>left_elbow</code> Instance of the <code>Keypoint</code> class representing the left elbow keypoint. <code>right_elbow</code> Instance of the <code>Keypoint</code> class representing the right elbow keypoint. <code>left_wrist</code> Instance of the <code>Keypoint</code> class representing the left wrist keypoint. <code>right_wrist</code> Instance of the <code>Keypoint</code> class representing the right wrist keypoint. <code>left_hip</code> Instance of the <code>Keypoint</code> class representing the left hip keypoint. <code>right_hip</code> Instance of the <code>Keypoint</code> class representing the right hip keypoint. <code>left_knee</code> Instance of the <code>Keypoint</code> class representing the left knee keypoint. <code>right_knee</code> Instance of the <code>Keypoint</code> class representing the right knee keypoint. <code>left_ankle</code> Instance of the <code>Keypoint</code> class representing the left ankle keypoint. <code>right_ankle</code> Instance of the <code>Keypoint</code> class representing the right ankle keypoint. <p>See the source code for more details.</p> Source code in <code>furiosa/models/vision/yolov7_w6_pose/postprocess.py</code> <pre><code>class PoseEstimationResult(BaseModel):\n    bounding_box: List[float]\n    confidence: float\n\n    nose: Keypoint\n    left_eye: Keypoint\n    right_eye: Keypoint\n    left_ear: Keypoint\n    right_ear: Keypoint\n    left_shoulder: Keypoint\n    right_shoulder: Keypoint\n    left_elbow: Keypoint\n    right_elbow: Keypoint\n    left_wrist: Keypoint\n    right_wrist: Keypoint\n    left_hip: Keypoint\n    right_hip: Keypoint\n    left_knee: Keypoint\n    right_knee: Keypoint\n    left_ankle: Keypoint\n    right_ankle: Keypoint\n</code></pre>"},{"location":"tutorials/navigate_models/","title":"Navigating Models from FuriosaAI Model Zoo","text":"In\u00a0[1]: Copied! <pre>from furiosa.models import vision\n\n\n# List of available vision models\nprint(dir(vision))\n</pre> from furiosa.models import vision   # List of available vision models print(dir(vision)) <pre>['EfficientNetB0', 'EfficientNetV2s', 'ResNet50', 'SSDMobileNet', 'SSDResNet34', 'YOLOv5l', 'YOLOv5m']\n</pre> In\u00a0[\u00a0]: Copied! <pre># Alternatively, use the Command line tool to list models\n! furiosa-models list\n</pre> # Alternatively, use the Command line tool to list models ! furiosa-models list Model name Model description Task type Available postprocesses ResNet50 MLCommons ResNet50 model Image Classification Python SSDMobileNet MLCommons MobileNet v1 model Object Detection Python, Rust SSDResNet34 MLCommons SSD ResNet34 model Object Detection Python, Rust YOLOv5l YOLOv5 Large model Object Detection Rust YOLOv5m YOLOv5 Medium model Object Detection Rust EfficientNetB0 EfficientNet B0 model Image Classification Python EfficientNetV2s EfficientNetV2-s model Image Classification Python <p>Now, let's instantiate a Model class from vision models and delve deeper into its attributes.</p> In\u00a0[3]: Copied! <pre>model = vision.ResNet50()\nprint(model)\n\n# Display the static fields of the model\nprint(\"Static fields:\", list(model.model_fields.keys()))\n\n# Show the lazy-loaded fields of the model\nprint(\"Lazy loaded fields:\", list(model.model_computed_fields.keys()))\n</pre> model = vision.ResNet50() print(model)  # Display the static fields of the model print(\"Static fields:\", list(model.model_fields.keys()))  # Show the lazy-loaded fields of the model print(\"Lazy loaded fields:\", list(model.model_computed_fields.keys())) <pre>name='ResNet50' task_type=&lt;ModelTaskType.IMAGE_CLASSIFICATION: 'IMAGE_CLASSIFICATION'&gt; format=&lt;Format.ONNX: 'ONNX'&gt; family='ResNet' version='v1.5' metadata=Metadata(description='ResNet50 v1.5 int8 ImageNet-1K', publication=Publication(authors=None, title=None, publisher=None, date=None, url='https://arxiv.org/abs/1512.03385.pdf')) tags=None\nStatic fields: ['name', 'task_type', 'format', 'family', 'version', 'metadata', 'tags', 'preprocessor', 'postprocessor']\nLazy loaded fields: ['origin', 'tensor_name_to_range']\n</pre> In\u00a0[4]: Copied! <pre># Moreover, you can access informative static fields using the Command line tool:\n! furiosa-models desc ResNet50\n</pre> # Moreover, you can access informative static fields using the Command line tool: ! furiosa-models desc ResNet50 <pre>libfuriosa_hal.so --- v0.11.0, built @ 43c901f\nname: ResNet50\nformat: ONNX\nfamily: ResNet\nversion: v1.5\nmetadata:\n  description: ResNet50 v1.5 int8 ImageNet-1K\n  publication:\n    url: https://arxiv.org/abs/1512.03385.pdf\ntask type: Image Classification\navailable postprocess versions: Python\n</pre> In\u00a0[5]: Copied! <pre>from furiosa.runtime.sync import create_runner\n\nmodel_source = model.model_source(num_pe=2)\n\n# Create a runner with the model source\nwith create_runner(model_source) as runner:\n    # Print model inputs metadata\n    print(runner.model.inputs())\n    # Run inferences, ...\n    ...\n</pre> from furiosa.runtime.sync import create_runner  model_source = model.model_source(num_pe=2)  # Create a runner with the model source with create_runner(model_source) as runner:     # Print model inputs metadata     print(runner.model.inputs())     # Run inferences, ...     ... <pre>libfuriosa_hal.so --- v0.11.0, built @ 43c901f\n:-) Finished in 0.000006756s\n</pre> <pre>[TensorDesc(shape=(1, 3, 224, 224), dtype=UINT8, format=NCHW, size=150528, len=150528)]\n</pre>"},{"location":"tutorials/navigate_models/#navigating-models-from-furiosaai-model-zoo","title":"Navigating Models from FuriosaAI Model Zoo\u00b6","text":""},{"location":"tutorials/navigate_models/#furiosaais-software-stack","title":"FuriosaAI's Software Stack\u00b6","text":"<p>FuriosaAI's software stack caters to a diverse range of deep learning models, with a primary focus on vision-related tasks. Within this stack, the FuriosaAI Compiler optimizes Deep Neural Network (DNN) models and generates executable code for the FuriosaAI NPU. It currently supports TFLite and ONNX models, utilizing the latest research and methods for optimization. The compiler efficiently accelerates various vision-related operators on the NPU while utilizing the CPU for unsupported operations.</p>"},{"location":"tutorials/navigate_models/#vision-models-and-beyond","title":"Vision Models and Beyond\u00b6","text":"<p>FuriosaAI's first-generation NPU, Warboy, is specialized for vision-related tasks. It accelerates popular vision models like ResNet50, SSD-MobileNet, and EfficientNet, while also enabling users to create custom models that utilize supported operators. This flexibility ensures the generation of highly optimized NPU-ready code for various vision tasks.</p>"},{"location":"tutorials/navigate_models/#exploring-vision-models","title":"Exploring Vision Models\u00b6","text":"<p>For easy exploration of vision models tailored for FuriosaAI's NPU, navigate to the <code>furiosa.models.vision</code> module. Here, you'll find a curated selection of models that have been optimized for efficient deployment on the FuriosaAI Warboy NPU.</p>"},{"location":"tutorials/navigate_models/#acquire-the-enf-binary-with-model_source","title":"Acquire the ENF Binary with <code>model_source()</code>\u00b6","text":"<p>FuriosaAI's Model object offers a method called <code>model_source()</code> which allows you to obtain the ENF (FuriosaAI Compiler-specific format) binary for a specific model. This ENF binary can be directly used for further processing or deployment without the need for recompilation. This is particularly beneficial when you want to save time and resources associated with the compilation process.</p> <p>Using <code>model_source()</code> is straightforward. You call this method on a Model object and, as a result, you receive the ENF binary. The <code>num_pe</code> parameter, which has a default value of 2, specifies the number of processing elements (PE) to use. You can set it to 1 if you want to use a single PE for the model. This flexibility allows you to optimize the model's deployment according to your specific requirements, whether it's for single-PE or fusioned-PE scenarios.</p> <p>Here's an example of how to use <code>model_source()</code>:</p>"},{"location":"tutorials/quantize_and_compile_model/","title":"Quantize and Compile Models","text":"In\u00a0[1]: Copied! <pre>from furiosa.models import vision\nfrom furiosa.quantizer import quantize\nfrom furiosa.runtime.sync import create_runner\n\nimport onnx\nimport numpy as np\n\nfrom time import perf_counter\n\nmodel = vision.ResNet50()\nf32_onnx_model = onnx.load_from_string(model.origin)\nquantized_onnx = quantize(f32_onnx_model, model.tensor_name_to_range)\n\nprint(\"Example field of calibration ranges:\", next(iter(model.tensor_name_to_range.items())))\n\nwith create_runner(quantized_onnx) as runner:\n    runner.model.print_summary()\n    input_tensor_desc = runner.model.inputs()\n    fake_input = [\n        np.asarray(np.random.randint(256, size=desc.shape), dtype=desc.dtype.numpy)\n        for desc in input_tensor_desc\n    ]\n    starting_time = perf_counter()\n    for _ in range(1000):\n        runner.run(fake_input)\n    print(\"Average inference time:\", perf_counter() - starting_time, \"ms\")\n</pre> from furiosa.models import vision from furiosa.quantizer import quantize from furiosa.runtime.sync import create_runner  import onnx import numpy as np  from time import perf_counter  model = vision.ResNet50() f32_onnx_model = onnx.load_from_string(model.origin) quantized_onnx = quantize(f32_onnx_model, model.tensor_name_to_range)  print(\"Example field of calibration ranges:\", next(iter(model.tensor_name_to_range.items())))  with create_runner(quantized_onnx) as runner:     runner.model.print_summary()     input_tensor_desc = runner.model.inputs()     fake_input = [         np.asarray(np.random.randint(256, size=desc.shape), dtype=desc.dtype.numpy)         for desc in input_tensor_desc     ]     starting_time = perf_counter()     for _ in range(1000):         runner.run(fake_input)     print(\"Average inference time:\", perf_counter() - starting_time, \"ms\") <pre>libfuriosa_hal.so --- v0.11.0, built @ 43c901f\nlibfuriosa_hal.so --- v0.11.0, built @ 43c901f\n</pre> <pre>Example field of calibration ranges: ('input_tensor:0', (-123.5584560111165, 150.34208860248327))\nInputs:\n{0: TensorDesc(shape=(1, 3, 224, 224), dtype=FLOAT32, format=NCHW, size=602112, len=150528)}\nOutputs:\n{0: TensorDesc(shape=(1,), dtype=INT64, format=?, size=8, len=1)}\nAverage inference time: 5.456097726011649 ms\n</pre> <p>According to performance tuning guide, we can remove input tensors' quantize operator to optimize the model.</p> <p>Please note that input tensors' data type has been changed from float32 to unsigned int 8.</p> In\u00a0[2]: Copied! <pre>from copy import deepcopy\nfrom furiosa.quantizer import ModelEditor, get_pure_input_names, TensorType\n\n\nmodel_wo_input_quantize = deepcopy(f32_onnx_model)\neditor = ModelEditor(model_wo_input_quantize)\nfor input_name in get_pure_input_names(model_wo_input_quantize):\n    editor.convert_input_type(input_name, TensorType.UINT8)\nquantized_onnx_wo_input_quantize = quantize(model_wo_input_quantize, model.tensor_name_to_range)\n\nwith create_runner(quantized_onnx_wo_input_quantize) as runner:\n    input_tensor_desc = runner.model.inputs()\n    runner.model.print_summary()\n    fake_input = [\n        np.random.randint(256, size=desc.shape, dtype=desc.dtype.numpy)\n        for desc in input_tensor_desc\n    ]\n    starting_time = perf_counter()\n    for _ in range(1000):\n        runner.run(fake_input)\n    print(\"Average inference time:\", perf_counter() - starting_time, \"ms\")\n</pre> from copy import deepcopy from furiosa.quantizer import ModelEditor, get_pure_input_names, TensorType   model_wo_input_quantize = deepcopy(f32_onnx_model) editor = ModelEditor(model_wo_input_quantize) for input_name in get_pure_input_names(model_wo_input_quantize):     editor.convert_input_type(input_name, TensorType.UINT8) quantized_onnx_wo_input_quantize = quantize(model_wo_input_quantize, model.tensor_name_to_range)  with create_runner(quantized_onnx_wo_input_quantize) as runner:     input_tensor_desc = runner.model.inputs()     runner.model.print_summary()     fake_input = [         np.random.randint(256, size=desc.shape, dtype=desc.dtype.numpy)         for desc in input_tensor_desc     ]     starting_time = perf_counter()     for _ in range(1000):         runner.run(fake_input)     print(\"Average inference time:\", perf_counter() - starting_time, \"ms\") <pre>Inputs:\n{0: TensorDesc(shape=(1, 3, 224, 224), dtype=UINT8, format=NCHW, size=150528, len=150528)}\nOutputs:\n{0: TensorDesc(shape=(1,), dtype=INT64, format=?, size=8, len=1)}\nAverage inference time: 2.715405730996281 ms\n</pre>"},{"location":"tutorials/quantize_and_compile_model/#quantize-and-compile-models","title":"Quantize and Compile Models\u00b6","text":"<p>Furiosa Model Zoo provides pre-compiled binaries that can be used directly with the NPU. However, we also offer the original model files and related metadata to allow for the application of different compiler options and calibration methods. In this document, we will explore the usage of the following two fields within the Model object:</p> <ul> <li><code>tensor_name_to_range</code></li> <li><code>origin</code></li> </ul> <p>For learn more about quantization and performance optimization, you can refer to the relevant SDK's documentation pages.</p> <ul> <li>Furiosa SDK - Quantization.</li> <li>Furiosa SDK - Model Optimization - Quantize.</li> </ul> <p>Now, we will run ResNet50 model without any further optimizations.</p>"},{"location":"tutorials/serving_with_furiosa_serving/","title":"Serving Example with Furiosa Serving","text":"<p>Furiosa Serving is a lightweight library based on FastAPI that allows you to run a model server on a Furiosa NPU.</p> <p>For more information about Furiosa Serving, you can visit the package link.</p>"},{"location":"tutorials/serving_with_furiosa_serving/#getting-started","title":"Getting Started","text":"<p>To get started with Furiosa Serving, you'll need to install the furiosa-serving library, create a ServeAPI (which is a <code>FastAPI</code> wrapper), and set up your model for serving. In this example, we'll walk you through the steps to create a simple ResNet50 server.</p> <p>First, you'll need to import the necessary modules and initialize a FastAPI app:</p> <pre><code>from tempfile import NamedTemporaryFile\nfrom typing import Dict, List\n\nfrom fastapi import FastAPI, File, UploadFile\nimport numpy as np\nimport uvicorn\n\nfrom furiosa.common.thread import synchronous\nfrom furiosa.models import vision\nfrom furiosa.serving import ServeAPI, ServeModel\n\nserve = ServeAPI()\napp: FastAPI = serve.app\n</code></pre>"},{"location":"tutorials/serving_with_furiosa_serving/#model-initialization","title":"Model Initialization","text":"<p>Next, you can initialize a vision model, such as ResNet50, for serving:</p> <pre><code>resnet50 = vision.ResNet50()\n\nmodel_file = NamedTemporaryFile()\nmodel_file.write(resnet50.model_source())\nmodel_file_path = model_file.name\nmodel: ServeModel = synchronous(serve.model(\"furiosart\"))(\n    'ResNet50', location=model_file_path\n)\n</code></pre> <p>Note</p> <p>ServeModel does not support in-memory model binaries for now. Instead, you can write the model into a temporary file and pass its path like example.</p>"},{"location":"tutorials/serving_with_furiosa_serving/#model-inference","title":"Model Inference","text":"<p>Now that you have your FastAPI app and model set up, you can define an endpoint for model inference. In this example, we create an endpoint that accepts an image file and performs inference using ResNet50:</p> <pre><code>@model.post(\"/infer\")\nasync def infer(image: UploadFile = File(...)) -&gt; Dict[str, str]:\n    # Model Zoo's preprocesses do not consider in-memory image file for now,\n    # so we write in-memory image into a temporary file and pass its path\n    image_file_path = NamedTemporaryFile()\n    image_file_path.write(await image.read())\n\n    tensors, _ctx = resnet50.preprocess(image_file_path.name)\n\n    # Infer from ServeModel\n    result: List[np.ndarray] = await model.predict(tensors)\n\n    response: str = resnet50.postprocess(result)\n\n    return {\"result\": response}\n</code></pre>"},{"location":"tutorials/serving_with_furiosa_serving/#running-the-server","title":"Running the Server","text":"<p>Finally, you can run the FastAPI server using uvicorn.</p> <pre><code># Run the server if the current Python script is called directly\nif __name__ == \"__main__\":\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n</code></pre> <p>Alternatively, you can run uvicorn server via internal app variable from ServeAPI instance like normal FastAPI application.</p> <pre><code>$ uvicorn main:app # or uvicorn main:serve.app\n</code></pre> <p>This example demonstrates the basic setup of a FastAPI server with Furiosa Serving for model inference. You can extend this example to add more functionality to your server as needed.</p> <p>For more information and advanced usage of Furiosa Serving, please refer to the Furiosa Serving documentation.</p> <p>You can find the full code example here.</p> <pre><code>from tempfile import NamedTemporaryFile\nfrom typing import Dict, List\n\nfrom fastapi import FastAPI, File, UploadFile\nimport numpy as np\nimport uvicorn\n\nfrom furiosa.common.thread import synchronous\nfrom furiosa.models import vision\nfrom furiosa.serving import ServeAPI, ServeModel\n\nserve = ServeAPI()\napp: FastAPI = serve.app\n\nresnet50 = vision.ResNet50()\n# ServeModel does not support in-memory model binary for now,\n# so we write model into temp file and pass its path\nmodel_file = NamedTemporaryFile()\nmodel_file.write(resnet50.model_source())\nmodel_file_path = model_file.name\n\nmodel: ServeModel = synchronous(serve.model(\"furiosart\"))('ResNet50', location=model_file_path)\n\n\n@model.post(\"/infer\")\nasync def infer(image: UploadFile = File(...)) -&gt; Dict[str, str]:\n    # Model Zoo's preprocesses do not consider in-memory image file for now\n    # (note that it's different from in-memory tensor)\n    # so we write in-memory image into temp file and pass its path\n    image_file_path = NamedTemporaryFile()\n    image_file_path.write(await image.read())\n\n    tensors, _ctx = resnet50.preprocess(image_file_path.name)\n\n    # Infer from ServeModel\n    result: List[np.ndarray] = await model.predict(tensors)\n\n    response: str = resnet50.postprocess(result)\n\n    return {\"result\": response}\n\n\n# Run the server if current Python script is called directly\nif __name__ == \"__main__\":\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n</code></pre>"}]}