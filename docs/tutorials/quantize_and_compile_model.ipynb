{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantize and Compile Models\n",
    "\n",
    "Furiosa Model Zoo provides pre-compiled binaries that can be used directly with the NPU. However, we also offer the original model files and related metadata to allow for the application of different compiler options and calibration methods. In this document, we will explore the usage of the following two fields within the Model object:\n",
    "- `tensor_name_to_range`\n",
    "- `origin`\n",
    "\n",
    "For learn more about quantization and performance optimization, you can refer to the relevant SDK's documentation pages.\n",
    "- [Furiosa SDK - Quantization](https://furiosa-ai.github.io/docs/latest/en/software/quantization.html).\n",
    "- [Furiosa SDK - Model Optimization - Quantize](https://furiosa-ai.github.io/docs/latest/en/software/performance.html#optimizing-quantize-operator).\n",
    "\n",
    "Now, we will run ResNet50 model without any further optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libfuriosa_hal.so --- v0.11.0, built @ 43c901f\n",
      "libfuriosa_hal.so --- v0.11.0, built @ 43c901f\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example field of calibration ranges: ('input_tensor:0', (-123.5584560111165, 150.34208860248327))\n",
      "Inputs:\n",
      "{0: TensorDesc(shape=(1, 3, 224, 224), dtype=FLOAT32, format=NCHW, size=602112, len=150528)}\n",
      "Outputs:\n",
      "{0: TensorDesc(shape=(1,), dtype=INT64, format=?, size=8, len=1)}\n",
      "Average inference time: 5.456097726011649 ms\n"
     ]
    }
   ],
   "source": [
    "from furiosa.models import vision\n",
    "from furiosa.quantizer import quantize\n",
    "from furiosa.runtime.sync import create_runner\n",
    "\n",
    "import onnx\n",
    "import numpy as np\n",
    "\n",
    "from time import perf_counter\n",
    "\n",
    "model = vision.ResNet50()\n",
    "f32_onnx_model = onnx.load_from_string(model.origin)\n",
    "quantized_onnx = quantize(f32_onnx_model, model.tensor_name_to_range)\n",
    "\n",
    "print(\"Example field of calibration ranges:\", next(iter(model.tensor_name_to_range.items())))\n",
    "\n",
    "with create_runner(quantized_onnx) as runner:\n",
    "    runner.model.print_summary()\n",
    "    input_tensor_desc = runner.model.inputs()\n",
    "    fake_input = [\n",
    "        np.asarray(np.random.randint(256, size=desc.shape), dtype=desc.dtype.numpy)\n",
    "        for desc in input_tensor_desc\n",
    "    ]\n",
    "    starting_time = perf_counter()\n",
    "    for _ in range(1000):\n",
    "        runner.run(fake_input)\n",
    "    print(\"Average inference time:\", perf_counter() - starting_time, \"ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to \n",
    "[performance tuning guide](https://furiosa-ai.github.io/docs/latest/ko/software/performance.html#quantize), we can remove input tensors' quantize operator to optimize the model.\n",
    "\n",
    "\n",
    "Please note that input tensors' data type has been changed from float32 to unsigned int 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      "{0: TensorDesc(shape=(1, 3, 224, 224), dtype=UINT8, format=NCHW, size=150528, len=150528)}\n",
      "Outputs:\n",
      "{0: TensorDesc(shape=(1,), dtype=INT64, format=?, size=8, len=1)}\n",
      "Average inference time: 2.715405730996281 ms\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "from furiosa.quantizer import ModelEditor, get_pure_input_names, TensorType\n",
    "\n",
    "\n",
    "model_wo_input_quantize = deepcopy(f32_onnx_model)\n",
    "editor = ModelEditor(model_wo_input_quantize)\n",
    "for input_name in get_pure_input_names(model_wo_input_quantize):\n",
    "    editor.convert_input_type(input_name, TensorType.UINT8)\n",
    "quantized_onnx_wo_input_quantize = quantize(model_wo_input_quantize, model.tensor_name_to_range)\n",
    "\n",
    "with create_runner(quantized_onnx_wo_input_quantize) as runner:\n",
    "    input_tensor_desc = runner.model.inputs()\n",
    "    runner.model.print_summary()\n",
    "    fake_input = [\n",
    "        np.random.randint(256, size=desc.shape, dtype=desc.dtype.numpy)\n",
    "        for desc in input_tensor_desc\n",
    "    ]\n",
    "    starting_time = perf_counter()\n",
    "    for _ in range(1000):\n",
    "        runner.run(fake_input)\n",
    "    print(\"Average inference time:\", perf_counter() - starting_time, \"ms\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
