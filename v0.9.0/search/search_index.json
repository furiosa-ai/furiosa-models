{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Furiosa Models","text":"<p><code>furiosa-models</code> is an open model zoo project for FuriosaAI NPU. It provides a set of public pre-trained, pre-quantized models for learning and demo purposes or for developing your applications.</p> <p><code>furiosa-models</code> also includes pre-packaged post/processing utilities, compiler configurations optimized for FuriosaAI NPU. However, all models are standard ONNX or tflite models, and they can run even on CPU and GPU as well.</p>"},{"location":"#releases","title":"Releases","text":"<ul> <li>v0.9.0 - 2023-TBD</li> <li>v0.8.0 - 2022-11-10</li> </ul>"},{"location":"#online-documentation","title":"Online Documentation","text":"<p>If you are new, you can start from Getting Started. You can also find the latest online documents, including programming guides, API references, and examples from the followings:</p> <ul> <li>Furiosa Models - Latest Documentation</li> <li>Model object</li> <li>Model List</li> <li>Command Tool</li> <li>Furiosa SDK - Tutorial and Code Examples</li> </ul>"},{"location":"#model-list","title":"Model List","text":"<p>The table summarizes all models available in <code>furiosa-models</code>. If you visit each model link, you can find details about loading a model, their input and output tensors, pre/post processings, and usage examples.</p> Model Task Size Accuracy ResNet50 Image Classification 25M 75.618% (ImageNet1K-val) EfficientNetB0 Image Classification 6.4M 72.47% (ImageNet1K-val) EfficientNetV2-S Image Classification 26M 83.498% (ImageNet1K-val) SSDMobileNet Object Detection 7.2M mAP 0.232 (COCO 2017-val) SSDResNet34 Object Detection 20M mAP 0.220 (COCO 2017-val) YOLOv5M Object Detection 21M mAP 0.272 (Bdd100k-val)* YOLOv5L Object Detection 46M mAP 0.284 (Bdd100k-val)* <p>*: The accuracy of the yolov5 f32 model trained with bdd100k-val dataset, is mAP 0.295 (for yolov5m) and mAP 0.316 (for yolov5l).</p>"},{"location":"#see-also","title":"See Also","text":"<ul> <li>Furiosa Models - Latest Documentation</li> <li>Furiosa Models - Github</li> <li>Furiosa SDK Documentation</li> </ul>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#090-tbu","title":"[0.9.0 - TBU]","text":""},{"location":"changelog/#new-features","title":"New Features","text":"<ul> <li>Add EfficientNetB0 model #121</li> <li>Add EfficientNetV2-S model #130</li> <li>Set default target as Warboy's production revision (B0) #125</li> </ul>"},{"location":"changelog/#improvements","title":"Improvements","text":"<ul> <li>Faster import for furiosa.models #117</li> <li>Replace yolov5's box decode implementation in Rust #109</li> <li>Remove Cpp postprocessor implementations #102</li> <li>Change packaging tool from setuptools-rust to flit #109</li> </ul>"},{"location":"changelog/#tasks","title":"Tasks","text":"<ul> <li>Release guide for developers #129</li> <li>Report regression test's result with PR comment #110</li> </ul>"},{"location":"changelog/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Fix CLI to properly report net inference time #112</li> <li>Update certain model sources with valid onnx #120</li> </ul>"},{"location":"changelog/#080-2022-11-10","title":"[0.8.0 - 2022-11-10]","text":""},{"location":"changelog/#new-features_1","title":"New Features","text":"<ul> <li>Add ResNet50 model</li> <li>Add SSD ResNet34 model</li> <li>Add SSD MobileNet model</li> <li>Add YOLOv5l model</li> <li>Add YOLOv5m model</li> </ul>"},{"location":"changelog/#improvements_1","title":"Improvements","text":"<ul> <li>Add native postprocessing implementation for ResNet50 #42</li> <li>Add native postprocessing implementation for SSD ResNet34 #45</li> <li>Add native postprocessing implementation for SSD MobileNet #16</li> <li>Refactor Model API to use classmethods to load models #66</li> <li>Make Model Zoo's ABC Model #78</li> <li>Retain only necessary python dependencies #81</li> <li>Improve enf_generator.sh and add enf, dfg models for e2e tests #33</li> <li>Add mkdocstrings to make API references #22</li> </ul>"},{"location":"changelog/#tasks_1","title":"Tasks","text":"<ul> <li>Regresstion Test #61</li> <li>Attach Tekton CI #41</li> <li>Yolov5 L/M e2e testing code #22</li> <li>Change the documentation layout and add resnet34, resnet50, mobilenet details #33</li> <li>Replace maturin with setuptools-rust #26</li> </ul>"},{"location":"changelog/#bug-fixes_1","title":"Bug Fixes","text":"<ul> <li>Resolve DVC directly #80</li> </ul>"},{"location":"command_tool/","title":"Command Tool","text":"<p>We provide a simple command line tool called <code>furiosa-models</code> to allow users to evaluate or run quickly one of models with FuriosaAI NPU.</p>"},{"location":"command_tool/#installing","title":"Installing","text":"<p>To install <code>furiosa-models</code> command, please refer to Installing. Then, <code>furiosa-models</code> command will be available.</p>"},{"location":"command_tool/#synopsis","title":"Synopsis","text":"<pre><code>furiosa-models [-h] {list, desc, bench} ...\n</code></pre> <p><code>furiosa-models</code> command has three subcommands: <code>list</code>, <code>desc</code>, and <code>bench</code>.</p>"},{"location":"command_tool/#subcommand-list","title":"Subcommand: list","text":"<p><code>list</code> subcommand prints out the list of models with attributes. You will be able to figure out what models are available.</p> <p>Example <pre><code>$ furiosa-models list\n\n+-----------------+------------------------------+----------------------+-------------------------+\n|   Model name    |      Model description       |      Task type       | Available postprocesses |\n+-----------------+------------------------------+----------------------+-------------------------+\n|    ResNet50     |   MLCommons ResNet50 model   | Image Classification |         Python          |\n|  SSDMobileNet   | MLCommons MobileNet v1 model |   Object Detection   |      Python, Rust       |\n|   SSDResNet34   | MLCommons SSD ResNet34 model |   Object Detection   |      Python, Rust       |\n|     YOLOv5l     |      YOLOv5 Large model      |   Object Detection   |         Python          |\n|     YOLOv5m     |     YOLOv5 Medium model      |   Object Detection   |         Python          |\n| EfficientNetB0  |    EfficientNet B0 model     | Image Classification |         Python          |\n| EfficientNetV2s |    EfficientNetV2-s model    | Image Classification |         Python          |\n+-----------------+------------------------------+----------------------+-------------------------+\n</code></pre></p>"},{"location":"command_tool/#subcommand-bench","title":"Subcommand: bench","text":"<p><code>bench</code> subcommand runs a specific model with a given path where the input sample data are located. It will print out the performance benchmark results like QPS.</p> <p>Example <pre><code>$ furiosa-models bench ResNet50 .\nlibfuriosa_hal.so --- v0.11.0, built @ 43c901f\nRunning 4 input samples ...\n----------------------------------------------------------------------\nWARN: the benchmark results may depend on the number of input samples,\nsizes of the images, and a machine where this benchmark is running.\n----------------------------------------------------------------------\n\n----------------------------------------------------------------------\nPreprocess -&gt; Inference -&gt; Postprocess\n----------------------------------------------------------------------\nTotal elapsed time: 618.86050 ms\nQPS: 6.46349\nAvg. elapsed time / sample: 154.71513 ms\n\n----------------------------------------------------------------------\nInference -&gt; Postprocess\n----------------------------------------------------------------------\nTotal elapsed time: 5.40490 ms\nQPS: 740.06865\nAvg. elapsed time / sample: 1.35123 ms\n\n----------------------------------------------------------------------\nInference\n----------------------------------------------------------------------\nTotal elapsed time: 5.05762 ms\nQPS: 790.88645\nAvg. elapsed time / sample: 1.26440 ms\n</code></pre></p>"},{"location":"command_tool/#subcommand-desc","title":"Subcommand: desc","text":"<p><code>desc</code> subcommand shows the details of a specific model.</p> <p>Example <pre><code>$ furiosa-models desc ResNet50\nfamily: ResNet\nformat: onnx\nmetadata:\n  description: ResNet50 v1.5 int8 ImageNet-1K\n  publication:\n    authors: null\n    date: null\n    publisher: null\n    title: null\n    url: https://arxiv.org/abs/1512.03385.pdf\nname: ResNet50\nversion: v1.5\ntask type: Image Classification\navailable postprocess versions: Python\n</code></pre></p>"},{"location":"developers-guide/","title":"Developer's guide","text":"<p>This documentation is for developers who want to contribute to Furiosa Models.</p>"},{"location":"developers-guide/#release-guide","title":"Release guide","text":""},{"location":"developers-guide/#preparing-for-release","title":"Preparing for Release","text":"<ul> <li> Select a correct release tag to mark the release: <code>x.y.z</code></li> <li> Update the code in the <code>main</code> branch to reflect the next development version.<ul> <li> <code>__version__</code> field in <code>furiosa/models/__init__.py</code></li> </ul> </li> <li> Create a dedicated release branch on github for the new tag.</li> </ul>"},{"location":"developers-guide/#pre-release-tasks","title":"Pre-Release Tasks","text":"<p>Before releasing the new version, ensure that the following tasks are completed:</p> <ul> <li> Update the code in the release branch to the appropriate version.</li> <li> Generate a rendered documentation for the new release.</li> <li> Write a changelog that describes the changes made in the new release.</li> <li> Test the building of wheels by <code>flit build</code> to ensure the release is functional.</li> </ul>"},{"location":"developers-guide/#releasing-the-new-version","title":"Releasing the New Version","text":"<ul> <li> Publish the package to PyPI with\u00a0<code>flit publish</code> command. \ud83c\udf89</li> </ul>"},{"location":"getting_started/","title":"Getting Started","text":"<p>This documentation explains how to install furiosa-models, how to use available models in furiosa-models, and how to explore the documents.</p>"},{"location":"getting_started/#prerequisites","title":"Prerequisites","text":"<p><code>furiosa-models</code> can be installed on various Linux distributions, but it has been tested on the followings:</p> <ul> <li>CentOS 7 or higher</li> <li>Debian bullseye or higher</li> <li>Ubuntu 20.04 or higher</li> </ul> <p>The following packages should be installed, but the followings are installed by default in most systems. So, only when you have any dependency issue, you need to install the following packages:</p> <ul> <li>libstdc++6</li> <li>libgomp</li> </ul>"},{"location":"getting_started/#installing","title":"Installing","text":"<p>You can quickly install Furiosa Models by using <code>pip</code> as following:</p> <pre><code>pip install --upgrade pip setuptools wheel\npip install 'furiosa-models'\n</code></pre> Building from Source Code (click to see) <p>Or you can build from the source code as following:</p> <pre><code>git clone https://github.com/furiosa-ai/furiosa-models\npip install .\n</code></pre>"},{"location":"getting_started/#quick-example-and-guides","title":"Quick example and Guides","text":"<p>You can simply load a model and run through furiosa-sdk as the following:</p> <p>Info</p> <p>If you want to learn more about the installation of furiosa-sdk and how to use it, please refer to the followings:</p> <ul> <li>Driver, Firmware, and Runtime Installation</li> <li>Python SDK Installation and User Guide</li> <li>Furiosa SDK - Tutorial and Code Examples</li> </ul> <pre><code>from furiosa.models.vision import SSDMobileNet\nfrom furiosa.runtime import session\nimage = [\"tests/assets/cat.jpg\"]\nmobilenet = SSDMobileNet.load()\nwith session.create(mobilenet.enf) as sess:\ninputs, contexts = mobilenet.preprocess(image)\noutputs = sess.run(inputs).numpy()\nmobilenet.postprocess(outputs, contexts)\n</code></pre> <p>This example does:</p> <ol> <li>Load the SSDMobileNet model</li> <li>Create a <code>session</code>, which is the main class of Furiosa Runtime, that can load an ONNX/tflite model onto NPU and run inferences.</li> <li>Run an inference with pre/post process functions.</li> </ol> <p>A <code>Model</code> instance is a Python object, including model artifacts, metadata, and its pre/postprocessors. You can learn more about <code>Model</code> object at Model object.</p> <p>Also, you can find all available models at Available Models. Each model page includes the details of the model, input and output tensors, and pre/post processings, and API reference.</p> <p>If you want to learn more about <code>furiosa.runtime.session</code> in Furiosa Runtime, please refer to Furiosa SDK - Tutorial and Code Examples.</p>"},{"location":"model_object/","title":"Model object","text":"<p>In <code>furiosa-models</code> project, <code>Model</code> is the first class object, and it represents a neural network model. This document explains what <code>Model</code> object offers and their usages.</p>"},{"location":"model_object/#loading-a-pre-trained-model","title":"Loading a pre-trained model","text":"<p>To load a pre-trained neural-network model, you need to call <code>load()</code> method. Since the sizes of pre-trained model weights vary from tens to hundreds megabytes, the model images are not included in Python package. When <code>load()</code> method is called, a pre-trained model will be fetched over the network. It takes some time (usually few seconds) depending on models and network conditions. Once the model images are fetched, they will be cached on a local disk.</p> <p>Non-blocking API <code>load_async()</code> also is available, and it can be used if your application is running through asynchronous executors (e.g., asyncio).</p> Blocking APINon-blocking API <pre><code>from furiosa.models.types import Model\nfrom furiosa.models.vision import ResNet50\nmodel: Model = ResNet50.load()\n</code></pre> <pre><code>import asyncio\nfrom furiosa.models.types import Model\nfrom furiosa.models.vision import ResNet50\nmodel: Model = asyncio.run(ResNet50.load_async())\n</code></pre> <p></p>"},{"location":"model_object/#accessing-artifacts-and-metadata","title":"Accessing artifacts and metadata","text":"<p>A <code>Model</code> object includes model artifacts, such as ONNX, tflite, calibration range in yaml format, and ENF.</p> <p>ENF format is FuriosaAI Compiler specific format. Once you have the ENF file, you can reuse it to omit the compilation process that take up to minutes. In addition, a <code>Model</code> object has various metadata. The followings are all attributes belonging to a single <code>Model</code> object.</p>"},{"location":"model_object/#furiosamodelstypesmodel","title":"<code>furiosa.models.types.Model</code>","text":"<p>Represent the artifacts and metadata of a neural network model</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>a name of this model</p> <code>format</code> <code>Format</code> <p>the binary format type of model source; e.g., ONNX, tflite</p> <code>source</code> <code>bytes</code> <p>a source binary in ONNX or tflite. It can be used for compiling this model with a custom compiler configuration.</p> <code>enf</code> <code>Optional[bytes]</code> <p>the executable binary for furiosa runtime and NPU</p> <code>calib_yaml</code> <code>Optional[str]</code> <p>the calibration ranges in yaml format for quantization</p> <code>version</code> <code>Optional[str]</code> <p>model version</p> <code>inputs</code> <code>Optional[List[ModelTensor]]</code> <p>data type and shape of input tensors</p> <code>outputs</code> <code>Optional[List[ModelTensor]]</code> <p>data type and shape of output tensors</p> <code>compiler_config</code> <code>Optional[List[ModelTensor]]</code> <p>a pre-defined compiler option</p> Source code in <code>furiosa/models/types.py</code> <pre><code>class Model(ABC, BaseModel):\n\"\"\"Represent the artifacts and metadata of a neural network model\n    Attributes:\n        name: a name of this model\n        format: the binary format type of model source; e.g., ONNX, tflite\n        source: a source binary in ONNX or tflite. It can be used for compiling this model\n            with a custom compiler configuration.\n        enf: the executable binary for furiosa runtime and NPU\n        calib_yaml: the calibration ranges in yaml format for quantization\n        version: model version\n        inputs: data type and shape of input tensors\n        outputs: data type and shape of output tensors\n        compiler_config: a pre-defined compiler option\n    \"\"\"\nclass Config(BaseConfig):\nextra: Extra = Extra.forbid\n# To allow Session, Processor type\narbitrary_types_allowed = True\nuse_enum_values = True\nname: str\nsource: bytes = Field(repr=False)\nformat: Format\nenf: Optional[bytes] = Field(repr=False)\ncalib_yaml: Optional[str] = Field(repr=False)\nfamily: Optional[str] = None\nversion: Optional[str] = None\nmetadata: Optional[Metadata] = None\ninputs: Optional[List[ModelTensor]] = []\noutputs: Optional[List[ModelTensor]] = []\npostprocessor_map: Optional[Dict[Platform, Type[PostProcessor]]] = None\npreprocessor: Optional[PreProcessor] = None\npostprocessor: Optional[PostProcessor] = None\n@staticmethod\n@abstractmethod\ndef get_artifact_name() -&gt; str:\n...\n@classmethod\n@abstractmethod\ndef load_aux(\ncls, artifacts: Dict[str, bytes], use_native: Optional[bool] = None, *args, **kwargs\n):\n...\n@classmethod\nasync def load_async(cls, use_native: Optional[bool] = None, *args, **kwargs) -&gt; 'Model':\nreturn cls.load_aux(\nawait load_artifacts(cls.get_artifact_name()), use_native, *args, **kwargs\n)\n@classmethod\ndef load(cls, use_native: Optional[bool] = None, *args, **kwargs) -&gt; 'Model':\nreturn cls.load_aux(\nsynchronous(load_artifacts)(cls.get_artifact_name()), use_native, *args, **kwargs\n)\ndef preprocess(self, *args, **kwargs) -&gt; Tuple[Sequence[npt.ArrayLike], Sequence[Context]]:\nassert self.preprocessor\nreturn self.preprocessor(*args, **kwargs)\ndef postprocess(self, *args, **kwargs):\nassert self.postprocessor\nreturn self.postprocessor(*args, **kwargs)\n</code></pre>"},{"location":"model_object/#inferencing-with-session-api","title":"Inferencing with Session API","text":"<p>To create a session, pass the <code>enf</code> field of the model object to the furiosa.runtime.session.create() function. Passing the pre-compiled <code>enf</code> allows you to perform inference directly without the compilation process. Alternatively, you can also manually quantize and compile the original f32 model with the provided calibration range.</p> <p>Info</p> <p>If you want to learn more about the installation of furiosa-sdk and how to use it, please follow the followings:</p> <ul> <li>Driver, Firmware, and Runtime Installation</li> <li>Python SDK Installation and User Guide</li> <li>Furiosa SDK - Tutorial and Code Examples</li> </ul> <p>Passing <code>Model.source</code> to <code>session.create()</code> allows users to start from source models in ONNX or tflite and customize models to their specific use-cases. This customization includes options such as specifying batch sizes and compiler configurations for optimization purposes. For additional information on Model.source, please refer to Accessing artifacts and metadata.</p> <p>To utilize f32 source models, calibration and quantization are necessary. Pre-calibrated data is readily available in Furiosa-models, facilitating direct access to the quantization process. The calib_range field of the model class represents this pre-calibrated data. After quantization, the output will be in the form of FuriosaAI's IR which can then be passed to the session. At this stage, the compiler configuration can be specified.</p> <p>Info</p> <p>The calibration range field is actually in yaml format but serialized in string type. To deserialize the calibration range, use <code>import yaml; yaml.full_load(calib_range)</code>.</p> <p></p> <p>Example</p> Using pre-compiled ENF binary <pre><code>from furiosa.models.vision import SSDMobileNet\nfrom furiosa.runtime import session\nimage = [\"tests/assets/cat.jpg\"]\nmobilenet = SSDMobileNet.load()\nwith session.create(mobilenet.enf) as sess:\ninputs, contexts = mobilenet.preprocess(image)\noutputs = sess.run(inputs).numpy()\nmobilenet.postprocess(outputs, contexts)\n</code></pre> <p>Example</p> From ONNX <pre><code>import yaml\nfrom furiosa.models.vision import SSDMobileNet\nfrom furiosa.quantizer import quantize\nfrom furiosa.runtime import session\ncompiler_config = {\"tabulate_dequantize\": True}\nimage = [\"tests/assets/cat.jpg\"]\nmobilenet = SSDMobileNet.load()\nonnx_model: bytes = mobilenet.source\ncalib_range: dict = yaml.full_load(mobilenet.calib_yaml)\n# See https://furiosa-ai.github.io/docs/latest/en/api/python/furiosa.quantizer.html#furiosa.quantizer.quantize\n# for more details\ndfg = quantize(onnx_model, calib_range, with_quantize=False)\nwith session.create(dfg, compiler_config=compiler_config) as sess:\ninputs, contexts = mobilenet.preprocess(image)\noutputs = sess.run(inputs).numpy()\nmobilenet.postprocess(outputs, contexts)\n</code></pre>"},{"location":"model_object/#prepostprocessing","title":"Pre/Postprocessing","text":"<p>There are gaps between model input/outputs and user applications' desired input and output data. In general, inputs and outputs of a neural network model are tensors. In applications, user sample data are images in standard formats like PNG or JPEG, and users also need to convert the output tensors to struct data for user applications.</p> <p>A <code>Model</code> object also provides both <code>preprocess()</code> and <code>postprocess()</code> methods. They are utilities to convert easily user inputs to the model's input tensors and output tensors to struct data which can be easily accessible by applications. If using pre-built pre/postprocessing methods, users can quickly start using <code>furiosa-models</code>.</p> <p>In sum, typical steps of a single inference is as the following, as also shown at examples.</p> <ol> <li>Call <code>preprocess()</code> with user inputs (e.g., image files)</li> <li>Pass an output of <code>preprocess()</code> to <code>Session.run()</code></li> <li>Pass the output of the model to <code>postprocess()</code></li> </ol> <p>Info</p> <p>Default postprocessing implementations are in Python. However, some models have the native postprocessing implemented in Rust and C++ and optimized for FuriosaAI Warboy and Intel/AMD CPUs. Python implementations can run on CPU and GPU as well, whereas the native postprocessor implementations works with only FuriosaAI NPU. Native implementations are designed to leverage FuriosaAI NPU's characteristics even for post-processing and maximize the latency and throughput by using modern CPU architecture, such as CPU cache, SIMD instructions and CPU pipelining. According to our benchmark, the native implementations show at most 70% lower latency.</p> <p>To use native post processor, please pass <code>use_native=True</code> to <code>Model.load()</code> or <code>Model.load_async()</code>. The following is an example to use native post processor for SSDMobileNet. You can find more details of each model page.</p> <p>Example</p> <pre><code>from furiosa.models.vision import SSDMobileNet\nfrom furiosa.runtime import session\nimage = [\"tests/assets/cat.jpg\"]\nmobilenet = SSDMobileNet.load(use_native=True)\nwith session.create(mobilenet.enf) as sess:\ninputs, contexts = mobilenet.preprocess(image)\noutputs = sess.run(inputs).numpy()\nmobilenet.postprocess(outputs, contexts[0])\n</code></pre>"},{"location":"model_object/#see-also","title":"See Also","text":"<ul> <li>Furiosa SDK Documentation</li> </ul>"},{"location":"models/efficientnet_b0/","title":"EfficientNetB0","text":"<p>The EfficientNet originates from the \"EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks\" paper, which proposes a new compound scaling method that enables better performance on image classification tasks with fewer parameters. EfficientNet B0 is the smallest and most efficient model in the EfficientNet family, achieving state-of-the-art results on various image classification benchmarks with just 5.3 million parameters.</p>"},{"location":"models/efficientnet_b0/#overall","title":"Overall","text":"<ul> <li>Framework: PyTorch</li> <li>Model format: ONNX</li> <li>Model task: Image classification</li> <li>Source: github</li> </ul>"},{"location":"models/efficientnet_b0/#_1","title":"EfficientNetB0","text":"<p>Usages</p> Python Postprocessor <pre><code>from furiosa.models.vision import EfficientNetB0\nfrom furiosa.runtime import session\nimage = \"tests/assets/cat.jpg\"\neffnetb0 = EfficientNetB0.load()\nwith session.create(effnetb0.enf) as sess:\ninputs, _ = effnetb0.preprocess(image)\noutputs = sess.run(inputs).numpy()\neffnetb0.postprocess(outputs)\n</code></pre>"},{"location":"models/efficientnet_b0/#inputs","title":"Inputs","text":"<p>The input is a 3-channel image of 224x224 (height, width).</p> <ul> <li>Data Type: <code>numpy.float32</code></li> <li>Tensor Shape: <code>[1, 3, 224, 224]</code></li> <li>Memory Format: NCHW, where:<ul> <li>N - batch size</li> <li>C - number of channels</li> <li>H - image height</li> <li>W - image width</li> </ul> </li> <li>Color Order: BGR</li> <li>Optimal Batch Size (minimum: 1): &lt;= 16</li> </ul>"},{"location":"models/efficientnet_b0/#outputs","title":"Outputs","text":"<p>The output is a <code>numpy.float32</code> tensor with the shape (<code>[1,]</code>), including a class id. <code>postprocess()</code> transforms the class id to a label string.</p>"},{"location":"models/efficientnet_b0/#prepostprocessing","title":"Pre/Postprocessing","text":"<p><code>furiosa.models.vision.EfficientNetB0</code> class provides <code>preprocess</code> and <code>postprocess</code> methods that convert input images to input tensors and the model outputs to labels respectively. You can find examples at EfficientNetB0 Usage.</p>"},{"location":"models/efficientnet_b0/#furiosamodelsvisionefficientnetb0preprocess","title":"<code>furiosa.models.vision.EfficientNetB0.preprocess</code>","text":"<p>Read and preprocess an image located at image_path.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Union[str, Path, npt.ArrayLike]</code> <p>A path of an image.</p> required <p>Returns:</p> Type Description <code>Tuple[np.ndarray, None]</code> <p>The first element of the tuple is a numpy array that meets the input requirements of the model. The second element of the tuple is unused in this model and has no value. To learn more information about the output numpy array, please refer to Inputs.</p>"},{"location":"models/efficientnet_b0/#furiosamodelsvisionefficientnetb0postprocess","title":"<code>furiosa.models.vision.EfficientNetB0.postprocess</code>","text":"<p>Convert the outputs of a model to a label string, such as car and cat.</p> <p>Parameters:</p> Name Type Description Default <code>model_outputs</code> <code>Sequence[npt.ArrayLike]</code> <p>the outputs of the model. Please learn more about the output of model, please refer to Outputs.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A classified label, e.g., \"jigsaw puzzle\".</p>"},{"location":"models/efficientnet_b0/#notes-on-the-source-field-of-this-model","title":"Notes on the source field of this model","text":"<p>There is a significant update in sdk version 0.9.0, which involves that the Furiosa's quantization tool adopts DFG(Data Flow Graph) as its output format instead of onnx. DFG is an IR of FuriosaAI that supports more diverse quantization schemes than onnx and is more specialized for FuriosaAI\u2019s Warboy.</p> <p>The EfficientNetB0 we offer has been quantized with furiosa-sdk 0.9.0 and thus formatted in DFG. The ONNX file in the source field is the original f32 model, not yet quantized.</p> <p>In case you need to use a different batch size or start from scratch, you can either start from the DFG or use the original ONNX file (and repeat the quantization process).</p>"},{"location":"models/efficientnet_v2_s/","title":"EfficientNetV2-S","text":"<p>EfficientNetV2-S is the smallest and most efficient model in the EfficientNetV2 family. Introduced in the paper \"EfficientNetV2: Smaller Models and Faster Training\", EfficientNetV2-S achieves state-of-the-art performance on image classification tasks, and it can be trained much faster and has a smaller model size of up to 6.8 times when compared to previous state-of-the-art models. It uses a combination of advanced techniques such as Swish activation function, Squeeze-and-Excitation blocks, and efficient channel attention to optimize its performance and efficiency.</p>"},{"location":"models/efficientnet_v2_s/#overall","title":"Overall","text":"<ul> <li>Framework: PyTorch</li> <li>Model format: ONNX</li> <li>Model task: Image classification</li> <li>Source: torchvision</li> </ul>"},{"location":"models/efficientnet_v2_s/#_1","title":"EfficientNetV2-S","text":"<p>Usages</p> Python Postprocessor <pre><code>from furiosa.models.vision import EfficientNetV2s\nfrom furiosa.runtime import session\nimage = \"tests/assets/cat.jpg\"\neffnetv2s = EfficientNetV2s.load()\nwith session.create(effnetv2s.enf) as sess:\ninputs, _ = effnetv2s.preprocess(image)\noutputs = sess.run(inputs).numpy()\neffnetv2s.postprocess(outputs)\n</code></pre>"},{"location":"models/efficientnet_v2_s/#inputs","title":"Inputs","text":"<p>The input is a 3-channel image of 384x384 (height, width).</p> <ul> <li>Data Type: <code>numpy.float32</code></li> <li>Tensor Shape: <code>[1, 3, 384, 384]</code></li> <li>Memory Format: NCHW, where:<ul> <li>N - batch size</li> <li>C - number of channels</li> <li>H - image height</li> <li>W - image width</li> </ul> </li> <li>Color Order: BGR</li> <li>Optimal Batch Size (minimum: 1): &lt;= 8</li> </ul>"},{"location":"models/efficientnet_v2_s/#outputs","title":"Outputs","text":"<p>The output is a <code>numpy.float32</code> tensor with the shape (<code>[1,]</code>), including a class id. <code>postprocess()</code> transforms the class id to a label string.</p>"},{"location":"models/efficientnet_v2_s/#prepostprocessing","title":"Pre/Postprocessing","text":"<p><code>furiosa.models.vision.EfficientNetV2s</code> class provides <code>preprocess</code> and <code>postprocess</code> methods that convert input images to input tensors and the model outputs to labels respectively. You can find examples at EfficientNetV2-S Usage.</p>"},{"location":"models/efficientnet_v2_s/#furiosamodelsvisionefficientnetv2spreprocess","title":"<code>furiosa.models.vision.EfficientNetV2s.preprocess</code>","text":"<p>Read and preprocess an image located at image_path.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Union[str, Path, npt.ArrayLike]</code> <p>A path of an image.</p> required <p>Returns:</p> Type Description <code>Tuple[np.ndarray, None]</code> <p>The first element of the tuple is a numpy array that meets the input requirements of the model. The second element of the tuple is unused in this model and has no value. To learn more information about the output numpy array, please refer to Inputs.</p>"},{"location":"models/efficientnet_v2_s/#furiosamodelsvisionefficientnetv2spostprocess","title":"<code>furiosa.models.vision.EfficientNetV2s.postprocess</code>","text":"<p>Convert the outputs of a model to a label string, such as car and cat.</p> <p>Parameters:</p> Name Type Description Default <code>model_outputs</code> <code>Sequence[npt.ArrayLike]</code> <p>the outputs of the model. Please learn more about the output of model, please refer to Outputs.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A classified label, e.g., \"tabby, tabby cat\".</p>"},{"location":"models/efficientnet_v2_s/#notes-on-the-source-field-of-this-model","title":"Notes on the source field of this model","text":"<p>There is a significant update in sdk version 0.9.0, which involves that the Furiosa's quantization tool adopts DFG(Data Flow Graph) as its output format instead of onnx. DFG is an IR of FuriosaAI that supports more diverse quantization schemes than onnx and is more specialized for FuriosaAI\u2019s Warboy.</p> <p>The EfficientNetV2-S we offer has been quantized with furiosa-sdk 0.9.0 and thus formatted in DFG. The ONNX file in the source field is the original f32 model, not yet quantized.</p> <p>In case you need to use a different batch size or start from scratch, you can either start from the DFG or use the original ONNX file (and repeat the quantization process).</p>"},{"location":"models/resnet50_v1.5/","title":"ResNet50 v1.5","text":"<p>ResNet50 v1.5 backbone model trained on ImageNet (224x224). This model has been used since MLCommons v0.5.</p>"},{"location":"models/resnet50_v1.5/#overall","title":"Overall","text":"<ul> <li>Framework: PyTorch</li> <li>Model format: ONNX</li> <li>Model task: Image classification</li> <li>Source: This model is originated from ResNet50 v1.5 in ONNX available at MLCommons - Supported Models.</li> </ul>"},{"location":"models/resnet50_v1.5/#_1","title":"ResNet50 v1.5","text":"<p>Usages</p> Postprocessor <pre><code>from furiosa.models.vision import ResNet50\nfrom furiosa.runtime import session\nimage = \"tests/assets/cat.jpg\"\nresnet50 = ResNet50.load()\nwith session.create(resnet50.enf) as sess:\ninputs, _ = resnet50.preprocess(image)\noutputs = sess.run(inputs).numpy()\nresnet50.postprocess(outputs)\n</code></pre>"},{"location":"models/resnet50_v1.5/#inputs","title":"Inputs","text":"<p>The input is a 3-channel image of 224x224 (height, width).</p> <ul> <li>Data Type: <code>numpy.float32</code></li> <li>Tensor Shape: <code>[1, 3, 224, 224]</code></li> <li>Memory Format: NCHW, where:<ul> <li>N - batch size</li> <li>C - number of channels</li> <li>H - image height</li> <li>W - image width</li> </ul> </li> <li>Color Order: BGR</li> <li>Optimal Batch Size (minimum: 1): &lt;= 8</li> </ul>"},{"location":"models/resnet50_v1.5/#outputs","title":"Outputs","text":"<p>The output is a <code>numpy.float32</code> tensor with the shape (<code>[1,]</code>), including a class id. <code>postprocess()</code> transforms the class id to a label string.</p>"},{"location":"models/resnet50_v1.5/#prepostprocessing","title":"Pre/Postprocessing","text":"<p><code>furiosa.models.vision.ResNet50</code> class provides <code>preprocess</code> and <code>postprocess</code> methods that convert input images to input tensors and the model outputs to labels respectively. You can find examples at ResNet50 Usage.</p>"},{"location":"models/resnet50_v1.5/#furiosamodelsvisionresnet50preprocess","title":"<code>furiosa.models.vision.ResNet50.preprocess</code>","text":"<p>Convert an input image to a model input tensor</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Union[str, npt.ArrayLike]</code> <p>A path of an image or an image loaded as a numpy array in BGR order.</p> required <p>Returns:</p> Type Description <code>Tuple[np.ndarray, None]</code> <p>The first element of the tuple is a numpy array that meets the input requirements of the ResNet50 model. The second element of the tuple is unused in this model and has no value. To learn more information about the output numpy array, please refer to Inputs.</p>"},{"location":"models/resnet50_v1.5/#furiosamodelsvisionresnet50postprocess","title":"<code>furiosa.models.vision.ResNet50.postprocess</code>","text":""},{"location":"models/ssd_mobilenet/","title":"SSD MobileNet v1","text":"<p>SSD MobileNet v1 backbone model trained on COCO (300x300). This model has been used since MLCommons v0.5.</p>"},{"location":"models/ssd_mobilenet/#overall","title":"Overall","text":"<ul> <li>Framework: PyTorch</li> <li>Model format: ONNX</li> <li>Model task: Object detection</li> <li>Source: This model is originated from SSD MobileNet v1 in ONNX available at MLCommons - Supported Models.</li> </ul>"},{"location":"models/ssd_mobilenet/#_1","title":"SSD MobileNet v1","text":"<p>Usages</p> Python PostprocessorNative Postprocessor <pre><code>from furiosa.models.vision import SSDMobileNet\nfrom furiosa.runtime import session\nimage = [\"tests/assets/cat.jpg\"]\nmobilenet = SSDMobileNet.load()\nwith session.create(mobilenet.enf) as sess:\ninputs, contexts = mobilenet.preprocess(image)\noutputs = sess.run(inputs).numpy()\nmobilenet.postprocess(outputs, contexts)\n</code></pre> <pre><code>from furiosa.models.vision import SSDMobileNet\nfrom furiosa.runtime import session\nimage = [\"tests/assets/cat.jpg\"]\nmobilenet = SSDMobileNet.load(use_native=True)\nwith session.create(mobilenet.enf) as sess:\ninputs, contexts = mobilenet.preprocess(image)\noutputs = sess.run(inputs).numpy()\nmobilenet.postprocess(outputs, contexts[0])\n</code></pre>"},{"location":"models/ssd_mobilenet/#inputs","title":"Inputs","text":"<p>The input is a 3-channel image of 300x300 (height, width).</p> <ul> <li>Data Type: <code>numpy.float32</code></li> <li>Tensor Shape: <code>[1, 3, 300, 300]</code></li> <li>Memory Format: NCHW, where:<ul> <li>N - batch size</li> <li>C - number of channels</li> <li>H - image height</li> <li>W - image width</li> </ul> </li> <li>Color Order: RGB</li> <li>Optimal Batch Size (minimum: 1): &lt;= 8</li> </ul>"},{"location":"models/ssd_mobilenet/#outputs","title":"Outputs","text":"<p>The outputs are 12 <code>numpy.float32</code> tensors in various shapes as the following. You can refer to <code>postprocess()</code> function to learn how to decode boxes, classes, and confidence scores.</p> Tensor Shape Data Type Data Type Description 0 (1, 273, 19, 19) float32 NCHW 1 (1, 12, 19, 19) float32 NCHW 2 (1, 546, 10, 10) float32 NCHW 3 (1, 24, 10, 10) float32 NCHW 4 (1, 546, 5, 5) float32 NCHW 5 (1, 24, 5, 5) float32 NCHW 6 (1, 546, 3, 3) float32 NCHW 7 (1, 24, 3, 3) float32 NCHW 8 (1, 546, 2, 2) float32 NCHW 9 (1, 24, 2, 2) float32 NCHW 10 (1, 546, 1, 1) float32 NCHW 11 (1, 24, 1, 1) float32 NCHW"},{"location":"models/ssd_mobilenet/#prepostprocessing","title":"Pre/Postprocessing","text":"<p><code>furiosa.models.vision.SSDMobileNet</code> class provides <code>preprocess</code> and <code>postprocess</code> methods. <code>preprocess</code> method converts input images to input tensors, and <code>postprocess</code> method converts  model output tensors to a list of bounding boxes, scores and labels.  You can find examples at SSDMobileNet Usage.</p>"},{"location":"models/ssd_mobilenet/#furiosamodelsvisionssdmobilenetpreprocess","title":"<code>furiosa.models.vision.SSDMobileNet.preprocess</code>","text":"<p>Preprocess input images to a batch of input tensors.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>Sequence[Union[str, np.ndarray]]</code> <p>A list of paths of image files (e.g., JPEG, PNG) or a stacked image loaded as a numpy array in BGR order or gray order.</p> required <p>Returns:</p> Type Description <code>Tuple[npt.ArrayLike, List[Dict[str, Any]]]</code> <p>The first element is 3-channel images of 300x300 in NCHW format, and the second element is a list of context about the original image metadata. This context data should be passed and utilized during post-processing. To learn more about the outputs of preprocess (i.e., model inputs), please refer to Inputs.</p>"},{"location":"models/ssd_mobilenet/#furiosamodelsvisionssdmobilenetpostprocess","title":"<code>furiosa.models.vision.SSDMobileNet.postprocess</code>","text":"<p>Convert the outputs of this model to a list of bounding boxes, scores and labels</p> <p>Parameters:</p> Name Type Description Default <code>model_outputs</code> <code>Sequence[numpy.ndarray]</code> <p>the outputs of the model. To learn more about the output of model, please refer to Outputs.</p> required <code>contexts</code> <code>Sequence[Dict[str, Any]]</code> <p>context coming from <code>preprocess()</code></p> required <p>Returns:</p> Type Description <code>List[List[ObjectDetectionResult]]</code> <p>Detected Bounding Box and its score and label represented as <code>ObjectDetectionResult</code>. To learn more about <code>ObjectDetectionResult</code>, 'Definition of ObjectDetectionResult' can be found below.</p> Definitions of ObjectDetectionResult and LtrbBoundingBox Source code in <code>furiosa/models/vision/postprocess.py</code> <pre><code>@dataclass\nclass LtrbBoundingBox:\nleft: float\ntop: float\nright: float\nbottom: float\ndef __iter__(self) -&gt; Iterator[float]:\nreturn iter([self.left, self.top, self.right, self.bottom])\n</code></pre> Source code in <code>furiosa/models/vision/postprocess.py</code> <pre><code>@dataclass\nclass ObjectDetectionResult:\nboundingbox: LtrbBoundingBox\nscore: float\nlabel: str\nindex: int\n</code></pre>"},{"location":"models/ssd_mobilenet/#native-postprocessor","title":"Native Postprocessor","text":"<p>This class provides another version of the postprocessing implementation which is highly optimized for NPU. The implementation leverages the NPU IO architecture and runtime.</p> <p>To use this implementation, when this model is loaded, the parameter <code>use_native=True</code> should be passed to <code>load()</code> or <code>load_aync()</code>. The following is an example:</p> <p>Example</p> <pre><code>from furiosa.models.vision import SSDMobileNet\nfrom furiosa.runtime import session\nimage = [\"tests/assets/cat.jpg\"]\nmobilenet = SSDMobileNet.load(use_native=True)\nwith session.create(mobilenet.enf) as sess:\ninputs, contexts = mobilenet.preprocess(image)\noutputs = sess.run(inputs).numpy()\nmobilenet.postprocess(outputs, contexts[0])\n</code></pre>"},{"location":"models/ssd_resnet34/","title":"SSD ResNet34","text":"<p>SSD ResNet34 backbone model trained on COCO (1200x1200). This model has been used since MLCommons v0.5.</p>"},{"location":"models/ssd_resnet34/#overall","title":"Overall","text":"<ul> <li>Framework: PyTorch</li> <li>Model format: ONNX</li> <li>Model task: Object detection</li> <li>Source: This model is originated from SSD ResNet34 in ONNX available at MLCommons - Supported Models.</li> </ul>"},{"location":"models/ssd_resnet34/#_1","title":"SSD ResNet34","text":"<p>Usages</p> Python PostprocessorNative Postprocessor <pre><code>from furiosa.models.vision import SSDResNet34\nfrom furiosa.runtime import session\nresnet34 = SSDResNet34.load()\nwith session.create(resnet34.enf) as sess:\nimage, contexts = resnet34.preprocess([\"tests/assets/cat.jpg\"])\noutput = sess.run(image).numpy()\nresnet34.postprocess(output, contexts=contexts)\n</code></pre> <pre><code>from furiosa.models.vision import SSDResNet34\nfrom furiosa.runtime import session\nresnet34 = SSDResNet34.load(use_native=True)\nwith session.create(resnet34.enf) as sess:\nimage, contexts = resnet34.preprocess([\"tests/assets/cat.jpg\"])\noutput = sess.run(image).numpy()\nresnet34.postprocessor(output, contexts=contexts[0])\n</code></pre>"},{"location":"models/ssd_resnet34/#inputs","title":"Inputs","text":"<p>The input is a 3-channel image of 300x300 (height, width).</p> <ul> <li>Data Type: <code>numpy.float32</code></li> <li>Tensor Shape: <code>[1, 3, 1200, 1200]</code></li> <li>Memory Format: NCHW, where<ul> <li>N - batch size</li> <li>C - number of channels</li> <li>H - image height</li> <li>W - image width</li> </ul> </li> <li>Color Order: RGB</li> <li>Optimal Batch Size: 1</li> </ul>"},{"location":"models/ssd_resnet34/#outputs","title":"Outputs","text":"<p>The outputs are 12 <code>numpy.float32</code> tensors in various shapes as the following.  You can refer to <code>postprocess()</code> function to learn how to decode boxes, classes, and confidence scores.</p> Tensor Shape Data Type Data Type Description 0 (1, 324, 50, 50) float32 NCHW 1 (1, 486, 25, 25) float32 NCHW 2 (1, 486, 13, 13) float32 NCHW 3 (1, 486, 7, 7) float32 NCHW 4 (1, 324, 3, 3) float32 NCHW 5 (1, 324, 3, 3) float32 NCHW 6 (1, 16, 50, 50) float32 NCHW 7 (1, 24, 25, 25) float32 NCHW 8 (1, 24, 13, 13) float32 NCHW 9 (1, 24, 7, 7) float32 NCHW 10 (1, 16, 3, 3) float32 NCHW 11 (1, 16, 3, 3) float32 NCHW"},{"location":"models/ssd_resnet34/#prepostprocessing","title":"Pre/Postprocessing","text":"<p><code>furiosa.models.vision.SSDResNet34</code> class provides <code>preprocess</code> and <code>postprocess</code> methods. <code>preprocess</code> method converts input images to input tensors, and <code>postprocess</code> method converts  model output tensors to a list of bounding boxes, scores and labels.  You can find examples at SSDResNet34 Usage.</p>"},{"location":"models/ssd_resnet34/#furiosamodelsvisionssdresnet34preprocess","title":"<code>furiosa.models.vision.SSDResNet34.preprocess</code>","text":"<p>Preprocess input images to a batch of input tensors</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>Sequence[Union[str, np.ndarray]]</code> <p>A list of paths of image files (e.g., JPEG, PNG) or a stacked image loaded as a numpy array in BGR order or gray order.</p> required <p>Returns:</p> Type Description <code>Tuple[npt.ArrayLike, List[Dict[str, Any]]]</code> <p>The first element is a list of 3-channel images of 1200x1200 in NCHW format, and the second element is a list of context about the original image metadata. This context data should be passed and utilized during post-processing. To learn more about the outputs of preprocess (i.e., model inputs), please refer to Inputs.</p>"},{"location":"models/ssd_resnet34/#furiosamodelsvisionssdresnet34postprocess","title":"<code>furiosa.models.vision.SSDResNet34.postprocess</code>","text":"<p>Convert the outputs of this model to a list of bounding boxes, scores and labels</p> <p>Parameters:</p> Name Type Description Default <code>model_outputs</code> <code>Sequence[np.ndarray]</code> <p>the outputs of the model. To learn more about the output of model, please refer to Outputs.</p> required <code>contexts</code> <code>Sequence[Dict[str, Any]]</code> <p>context coming from <code>preprocess()</code></p> required <p>Returns:</p> Type Description <code>List[List[ObjectDetectionResult]]</code> <p>Detected Bounding Box and its score and label represented as <code>ObjectDetectionResult</code>. To learn more about <code>ObjectDetectionResult</code>, 'Definition of ObjectDetectionResult' can be found below.</p> Definition of ObjectDetectionResult and LtrbBoundingBox Source code in <code>furiosa/models/vision/postprocess.py</code> <pre><code>@dataclass\nclass LtrbBoundingBox:\nleft: float\ntop: float\nright: float\nbottom: float\ndef __iter__(self) -&gt; Iterator[float]:\nreturn iter([self.left, self.top, self.right, self.bottom])\n</code></pre> Source code in <code>furiosa/models/vision/postprocess.py</code> <pre><code>@dataclass\nclass ObjectDetectionResult:\nboundingbox: LtrbBoundingBox\nscore: float\nlabel: str\nindex: int\n</code></pre>"},{"location":"models/ssd_resnet34/#native-postprocessor","title":"Native Postprocessor","text":"<p>This class provides another version of the postprocessing implementation which is highly optimized for NPU. The implementation leverages the NPU IO architecture and runtime.</p> <p>To use this implementation, when this model is loaded, the parameter <code>use_native=True</code> should be passed to <code>load()</code> or <code>load_aync()</code>. The following is an example:</p> <p>Example</p> <pre><code>from furiosa.models.vision import SSDResNet34\nfrom furiosa.runtime import session\nresnet34 = SSDResNet34.load(use_native=True)\nwith session.create(resnet34.enf) as sess:\nimage, contexts = resnet34.preprocess([\"tests/assets/cat.jpg\"])\noutput = sess.run(image).numpy()\nresnet34.postprocessor(output, contexts=contexts[0])\n</code></pre>"},{"location":"models/yolov5l/","title":"YOLOv5L","text":"<p>YOLOv5 is the one of the most popular object detection models. You can find more details at https://github.com/ultralytics/yolov5.</p>"},{"location":"models/yolov5l/#overall","title":"Overall","text":"<ul> <li>Framework: PyTorch</li> <li>Model format: ONNX</li> <li>Model task: Object Detection</li> <li>Source: https://github.com/ultralytics/yolov5</li> </ul>"},{"location":"models/yolov5l/#_1","title":"YOLOv5L","text":"<p>Usage</p> <pre><code>import cv2\nimport numpy as np\nfrom furiosa.models.vision import YOLOv5l\nfrom furiosa.runtime import session\nyolov5l = YOLOv5l.load()\nwith session.create(yolov5l.enf) as sess:\nimage = cv2.imread(\"tests/assets/yolov5-test.jpg\")\ninputs, contexts = yolov5l.preprocess([image])\noutput = sess.run(np.expand_dims(inputs[0], axis=0)).numpy()\nyolov5l.postprocess(output, contexts=contexts)\n</code></pre>"},{"location":"models/yolov5l/#inputs","title":"Inputs","text":"<p>The input is a 3-channel image of 640, 640 (height, width).</p> <ul> <li>Data Type: <code>numpy.uint8</code></li> <li>Tensor Shape: <code>[1, 640, 640, 3]</code></li> <li>Memory Format: NHWC, where<ul> <li>N - batch size</li> <li>H - image height</li> <li>W - image width</li> <li>C - number of channels</li> </ul> </li> <li>Color Order: RGB</li> <li>Optimal Batch Size (minimum: 1): &lt;= 2</li> </ul>"},{"location":"models/yolov5l/#outputs","title":"Outputs","text":"<p>The outputs are 3 <code>numpy.float32</code> tensors in various shapes as the following.  You can refer to <code>postprocess()</code> function to learn how to decode boxes, classes, and confidence scores.</p> Tensor Shape Data Type Data Type Description 0 (1, 45, 80, 80) float32 NCHW 1 (1, 45, 40, 40) float32 NCHW 2 (1, 45, 20, 20) float32 NCHW"},{"location":"models/yolov5l/#prepostprocessing","title":"Pre/Postprocessing","text":"<p><code>furiosa.models.vision.YOLOv5l</code> class provides <code>preprocess</code> and <code>postprocess</code> methods. <code>preprocess</code> method converts input images to input tensors, and <code>postprocess</code> method converts  model output tensors to a list of bounding boxes, scores and labels.  You can find examples at YOLOv5l Usage.</p>"},{"location":"models/yolov5l/#furiosamodelsvisionyolov5lpreprocess","title":"<code>furiosa.models.vision.YOLOv5l.preprocess</code>","text":"<p>Preprocess input images to a batch of input tensors</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>Sequence[Union[str, np.ndarray]]</code> <p>Color images have (NCHW: Batch, Channel, Height, Width) dimensions.</p> required <code>with_quantize</code> <code>bool</code> <p>Whether to put quantize operator in front of the model or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[np.ndarray, List[Dict[str, Any]]]</code> <p>a pre-processed image, scales and padded sizes(width,height) per images. The first element is a stacked numpy array containing a batch of images. To learn more about the outputs of preprocess (i.e., model inputs), please refer to YOLOv5l Inputs or YOLOv5m Inputs.</p> <p>The second element is a list of dict objects about the original images. Each dict object has the following keys. 'scale' key of the returned dict has a rescaled ratio per width(=target/width) and height(=target/height), and the 'pad' key has padded width and height pixels. Specially, the last dictionary element of returning tuple will be passed to postprocessing as a parameter to calculate predicted coordinates on normalized coordinates back to an input image coordinator.</p>"},{"location":"models/yolov5l/#furiosamodelsvisionyolov5lpostprocess","title":"<code>furiosa.models.vision.YOLOv5l.postprocess</code>","text":"<p>Convert the outputs of this model to a list of bounding boxes, scores and labels</p> <p>Parameters:</p> Name Type Description Default <code>model_outputs</code> <code>Sequence[np.ndarray]</code> <p>P3/8, P4/16, P5/32 features from yolov5l model. To learn more about the outputs of preprocess (i.e., model inputs), please refer to YOLOv5l Outputs or YOLOv5m Outputs.</p> required <code>contexts</code> <code>Sequence[Dict[str, Any]]</code> <p>A configuration for each image generated by the preprocessor. For example, it could be the reduction ratio of the image, the actual image width and height.</p> required <code>conf_thres</code> <code>float</code> <p>Confidence score threshold. The default to 0.25</p> <code>0.25</code> <code>iou_thres</code> <code>float</code> <p>IoU threshold value for the NMS processing. The default to 0.45.</p> <code>0.45</code> <p>Returns:</p> Type Description <code>List[List[ObjectDetectionResult]]</code> <p>Detected Bounding Box and its score and label represented as <code>ObjectDetectionResult</code>. The details of <code>ObjectDetectionResult</code> can be found below.</p> Definition of ObjectDetectionResult and LtrbBoundingBox Source code in <code>furiosa/models/vision/postprocess.py</code> <pre><code>@dataclass\nclass LtrbBoundingBox:\nleft: float\ntop: float\nright: float\nbottom: float\ndef __iter__(self) -&gt; Iterator[float]:\nreturn iter([self.left, self.top, self.right, self.bottom])\n</code></pre> Source code in <code>furiosa/models/vision/postprocess.py</code> <pre><code>@dataclass\nclass ObjectDetectionResult:\nboundingbox: LtrbBoundingBox\nscore: float\nlabel: str\nindex: int\n</code></pre>"},{"location":"models/yolov5m/","title":"YOLOv5m","text":"<p>YOLOv5 is the one of the most popular object detection models. You can find more details at https://github.com/ultralytics/yolov5.</p>"},{"location":"models/yolov5m/#overall","title":"Overall","text":"<ul> <li>Framework: PyTorch</li> <li>Model format: ONNX</li> <li>Model task: Object Detection</li> <li>Source: https://github.com/ultralytics/yolov5.</li> </ul>"},{"location":"models/yolov5m/#_1","title":"YOLOv5m","text":"<p>Usage</p> <pre><code>import cv2\nimport numpy as np\nfrom furiosa.models.vision import YOLOv5m\nfrom furiosa.runtime import session\nyolov5m = YOLOv5m.load()\nwith session.create(yolov5m.enf) as sess:\nimage = cv2.imread(\"tests/assets/yolov5-test.jpg\")\ninputs, contexts = yolov5m.preprocess([image])\noutput = sess.run(np.expand_dims(inputs[0], axis=0)).numpy()\nyolov5m.postprocess(output, contexts=contexts)\n</code></pre>"},{"location":"models/yolov5m/#inputs","title":"Inputs","text":"<p>The input is a 3-channel image of 640, 640 (height, width).</p> <ul> <li>Data Type: <code>numpy.uint8</code></li> <li>Tensor Shape: <code>[1, 640, 640, 3]</code></li> <li>Memory Format: NHWC, where<ul> <li>N - batch size</li> <li>H - image height</li> <li>W - image width</li> <li>C - number of channels</li> </ul> </li> <li>Color Order: RGB</li> <li>Optimal Batch Size (minimum: 1): &lt;= 4</li> </ul>"},{"location":"models/yolov5m/#outputs","title":"Outputs","text":"<p>The outputs are 3 <code>numpy.float32</code> tensors in various shapes as the following.  You can refer to <code>postprocess()</code> function to learn how to decode boxes, classes, and confidence scores.</p> Tensor Shape Data Type Data Type Description 0 (1, 45, 80, 80) float32 NCHW 1 (1, 45, 40, 40) float32 NCHW 2 (1, 45, 20, 20) float32 NCHW"},{"location":"models/yolov5m/#prepostprocessing","title":"Pre/Postprocessing","text":"<p><code>furiosa.models.vision.YOLOv5m</code> class provides <code>preprocess</code> and <code>postprocess</code> methods. <code>preprocess</code> method converts input images to input tensors, and <code>postprocess</code> method converts  model output tensors to a list of bounding boxes, scores and labels.  You can find examples at YOLOv5m Usage.</p>"},{"location":"models/yolov5m/#furiosamodelsvisionyolov5mpreprocess","title":"<code>furiosa.models.vision.YOLOv5m.preprocess</code>","text":"<p>Preprocess input images to a batch of input tensors</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>Sequence[Union[str, np.ndarray]]</code> <p>Color images have (NCHW: Batch, Channel, Height, Width) dimensions.</p> required <code>with_quantize</code> <code>bool</code> <p>Whether to put quantize operator in front of the model or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[np.ndarray, List[Dict[str, Any]]]</code> <p>a pre-processed image, scales and padded sizes(width,height) per images. The first element is a stacked numpy array containing a batch of images. To learn more about the outputs of preprocess (i.e., model inputs), please refer to YOLOv5l Inputs or YOLOv5m Inputs.</p> <p>The second element is a list of dict objects about the original images. Each dict object has the following keys. 'scale' key of the returned dict has a rescaled ratio per width(=target/width) and height(=target/height), and the 'pad' key has padded width and height pixels. Specially, the last dictionary element of returning tuple will be passed to postprocessing as a parameter to calculate predicted coordinates on normalized coordinates back to an input image coordinator.</p>"},{"location":"models/yolov5m/#furiosamodelsvisionyolov5mpostprocess","title":"<code>furiosa.models.vision.YOLOv5m.postprocess</code>","text":"<p>Convert the outputs of this model to a list of bounding boxes, scores and labels</p> <p>Parameters:</p> Name Type Description Default <code>model_outputs</code> <code>Sequence[np.ndarray]</code> <p>P3/8, P4/16, P5/32 features from yolov5l model. To learn more about the outputs of preprocess (i.e., model inputs), please refer to YOLOv5l Outputs or YOLOv5m Outputs.</p> required <code>contexts</code> <code>Sequence[Dict[str, Any]]</code> <p>A configuration for each image generated by the preprocessor. For example, it could be the reduction ratio of the image, the actual image width and height.</p> required <code>conf_thres</code> <code>float</code> <p>Confidence score threshold. The default to 0.25</p> <code>0.25</code> <code>iou_thres</code> <code>float</code> <p>IoU threshold value for the NMS processing. The default to 0.45.</p> <code>0.45</code> <p>Returns:</p> Type Description <code>List[List[ObjectDetectionResult]]</code> <p>Detected Bounding Box and its score and label represented as <code>ObjectDetectionResult</code>. The details of <code>ObjectDetectionResult</code> can be found below.</p> Definition of ObjectDetectionResult and LtrbBoundingBox Source code in <code>furiosa/models/vision/postprocess.py</code> <pre><code>@dataclass\nclass LtrbBoundingBox:\nleft: float\ntop: float\nright: float\nbottom: float\ndef __iter__(self) -&gt; Iterator[float]:\nreturn iter([self.left, self.top, self.right, self.bottom])\n</code></pre> Source code in <code>furiosa/models/vision/postprocess.py</code> <pre><code>@dataclass\nclass ObjectDetectionResult:\nboundingbox: LtrbBoundingBox\nscore: float\nlabel: str\nindex: int\n</code></pre>"}]}