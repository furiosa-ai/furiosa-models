{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Furiosa Models furiosa-models is an open model zoo project for FuriosaAI NPU. It provides a set of public pre-trained, pre-quantized models for learning and demo purposes or for developing your applications. furiosa-models also includes pre-packaged post/processing utilities, compiler configurations optimized for FuriosaAI NPU. However, all models are standard ONNX or tflite models, and they can run even on CPU and GPU as well. Releases v0.8.0 - 2022-11-10 Online Documentation If you are new, you can start from Getting Started . You can also find the latest online documents, including programming guide, API reference, examples from the followings: Furiosa Models - Documentation Model object and its Examples Model List Furiosa SDK - Tutorial and Code Examples Model List The table summarizes all models available in furiosa-models . If you visit each model link, you can find details about loading a model, their input and output tensors, and pre/post processings, and examples. Model Task Size Accuracy ResNet50 Image Classification 25M 76.002% (ImageNet1K-val) SSDMobileNet Object Detection 7.2M mAP 0.228 (COCO 2017-val) SSDResNet34 Object Detection 20M mAP 0.220 (COCO 2017-val) YOLOv5M Object Detection 21M mAP 0.280 YOLOv5L Object Detection 46M mAP 0.295 See Also Furiosa Models - Documentation Furiosa Models - Github Furiosa SDK Documentation","title":"Overview"},{"location":"#furiosa-models","text":"furiosa-models is an open model zoo project for FuriosaAI NPU. It provides a set of public pre-trained, pre-quantized models for learning and demo purposes or for developing your applications. furiosa-models also includes pre-packaged post/processing utilities, compiler configurations optimized for FuriosaAI NPU. However, all models are standard ONNX or tflite models, and they can run even on CPU and GPU as well.","title":"Furiosa Models"},{"location":"#releases","text":"v0.8.0 - 2022-11-10","title":"Releases"},{"location":"#online-documentation","text":"If you are new, you can start from Getting Started . You can also find the latest online documents, including programming guide, API reference, examples from the followings: Furiosa Models - Documentation Model object and its Examples Model List Furiosa SDK - Tutorial and Code Examples","title":"Online Documentation"},{"location":"#model-list","text":"The table summarizes all models available in furiosa-models . If you visit each model link, you can find details about loading a model, their input and output tensors, and pre/post processings, and examples. Model Task Size Accuracy ResNet50 Image Classification 25M 76.002% (ImageNet1K-val) SSDMobileNet Object Detection 7.2M mAP 0.228 (COCO 2017-val) SSDResNet34 Object Detection 20M mAP 0.220 (COCO 2017-val) YOLOv5M Object Detection 21M mAP 0.280 YOLOv5L Object Detection 46M mAP 0.295","title":"Model List"},{"location":"#see-also","text":"Furiosa Models - Documentation Furiosa Models - Github Furiosa SDK Documentation","title":"See Also"},{"location":"changelog/","text":"Changelog [0.8.0] New Features Add ResNet50 model Add SSD ResNet34 model Add SSD MobileNet model Add YOLOv5l model Add YOLOv5m model Improvements Add native postprocessing implementation for ResNet50 #42 Add native postprocessing implementation for SSD ResNet34 #45 Add native postprocessing implementation for SSD MobileNet #16 Refactor Model API to use classmethods to load models #66 Make Model Zoo's ABC Model #78 Retain only necessary python dependencies #81 Improve enf_generator.sh and add enf, dfg models for e2e tests #33 Add mkdocstrings to make API references #22 Tasks Regresstion Test #61 Attach Tekton CI #41 Yolov5 L/M e2e testing code #22 Change the documentation layout and add resnet34, resnet50, mobilenet details #33 Replace maturin with setuptools-rust #26 Bug Fixes Resolve DVC directly #80","title":"Changelog"},{"location":"changelog/#changelog","text":"","title":"Changelog"},{"location":"changelog/#080","text":"","title":"[0.8.0]"},{"location":"changelog/#new-features","text":"Add ResNet50 model Add SSD ResNet34 model Add SSD MobileNet model Add YOLOv5l model Add YOLOv5m model","title":"New Features"},{"location":"changelog/#improvements","text":"Add native postprocessing implementation for ResNet50 #42 Add native postprocessing implementation for SSD ResNet34 #45 Add native postprocessing implementation for SSD MobileNet #16 Refactor Model API to use classmethods to load models #66 Make Model Zoo's ABC Model #78 Retain only necessary python dependencies #81 Improve enf_generator.sh and add enf, dfg models for e2e tests #33 Add mkdocstrings to make API references #22","title":"Improvements"},{"location":"changelog/#tasks","text":"Regresstion Test #61 Attach Tekton CI #41 Yolov5 L/M e2e testing code #22 Change the documentation layout and add resnet34, resnet50, mobilenet details #33 Replace maturin with setuptools-rust #26","title":"Tasks"},{"location":"changelog/#bug-fixes","text":"Resolve DVC directly #80","title":"Bug Fixes"},{"location":"getting_started/","text":"Getting Started This documentation explains how to install furiosa-models, how to use available models in furiosa-models, and how to explore the documents. Prerequisites furiosa-models can be installed on various Linux distributions, but it has been tested on the followings: CentOS 7 or higher Debian buster or higher Ubuntu 18.04 or higher The following packages should be installed, but the followings are installed by default in most systems. So, only when you have any dependency issue, you need to install the following packages: libstdc++6 libgomp Installing You can quickly install Furiosa Models by using pip as following: pip install 'furiosa-models' Building from Source Code (click to see) Or you can build from the source code as following: Building furiosa-models requires additional prerequisites: rust-toolchain (please refer to rustup ) git clone https://github.com/furiosa-ai/furiosa-models pip install . Quick example and Guides You can simply load a model and run through furiosa-sdk as following: from furiosa.models.vision import SSDMobileNet from furiosa.runtime import session image = [ \"tests/assets/cat.jpg\" ] mobilenet = SSDMobileNet . load () with session . create ( mobilenet ) as sess : inputs , contexts = mobilenet . preprocess ( image ) outputs = sess . run ( inputs ) . numpy () mobilenet . postprocess ( outputs , contexts ) This example does: Loads the SSDMobileNet model Creates a session which is the main class of Furiosa Runtime which actually loads an ONNX/tflite model to NPU and run inferences Runs an inference with pre/post processings. A Model instance is a Python object, including model artifacts, metadata, and its pre/post processors. You can learn more about Model object at Model object . Also, you can find all available models at Available Models . Each model page includes the details of the model, input and output tensors, and pre/post processings, and API reference. If you want to learn more about furiosa.runtime.session in Furiosa Runtime, please refer to Furiosa SDK - Tutorial and Code Examples .","title":"Getting Started"},{"location":"getting_started/#getting-started","text":"This documentation explains how to install furiosa-models, how to use available models in furiosa-models, and how to explore the documents.","title":"Getting Started"},{"location":"getting_started/#prerequisites","text":"furiosa-models can be installed on various Linux distributions, but it has been tested on the followings: CentOS 7 or higher Debian buster or higher Ubuntu 18.04 or higher The following packages should be installed, but the followings are installed by default in most systems. So, only when you have any dependency issue, you need to install the following packages: libstdc++6 libgomp","title":"Prerequisites"},{"location":"getting_started/#installing","text":"You can quickly install Furiosa Models by using pip as following: pip install 'furiosa-models' Building from Source Code (click to see) Or you can build from the source code as following: Building furiosa-models requires additional prerequisites: rust-toolchain (please refer to rustup ) git clone https://github.com/furiosa-ai/furiosa-models pip install .","title":"Installing"},{"location":"getting_started/#quick-example-and-guides","text":"You can simply load a model and run through furiosa-sdk as following: from furiosa.models.vision import SSDMobileNet from furiosa.runtime import session image = [ \"tests/assets/cat.jpg\" ] mobilenet = SSDMobileNet . load () with session . create ( mobilenet ) as sess : inputs , contexts = mobilenet . preprocess ( image ) outputs = sess . run ( inputs ) . numpy () mobilenet . postprocess ( outputs , contexts ) This example does: Loads the SSDMobileNet model Creates a session which is the main class of Furiosa Runtime which actually loads an ONNX/tflite model to NPU and run inferences Runs an inference with pre/post processings. A Model instance is a Python object, including model artifacts, metadata, and its pre/post processors. You can learn more about Model object at Model object . Also, you can find all available models at Available Models . Each model page includes the details of the model, input and output tensors, and pre/post processings, and API reference. If you want to learn more about furiosa.runtime.session in Furiosa Runtime, please refer to Furiosa SDK - Tutorial and Code Examples .","title":"Quick example and Guides"},{"location":"model_object/","text":"Model object In furiosa-models project, Model is the first class object, and it represents a neural network model. This document explains what Model object offers and their usages. Loading a pre-trained model To load a pre-trained neural-network model, you need to call load() method. Since the sizes of pre-trained model weights vary from tens to hundreds megabytes, the model images are not included in Python package. When load() method is called, a pre-trained model will be fetched over network. It takes some time (usually few seconds) depending on models and environments. Once the model images are fetched, they will be cached on a local disk. Non-blocking API load_async() also is available, and it can be used if your application is running through asynchronous executors (e.g., asyncio). Blocking API Non-blocking API from furiosa.models.types import Model from furiosa.models.vision import ResNet50 model : Model = ResNet50 . load () import asyncio from furiosa.models.types import Model from furiosa.models.vision import ResNet50 model : Model = asyncio . run ( ResNet50 . load_async ()) Accessing artifacts and metadata A Model object includes model artifacts, such as ONNX, tflite, DFG, and ENF. DFG and ENF are FuriosaAI Compiler specific formats. Both formats are used for pre-compiled binary, and they are used to skip compilation times that take up to minutes. In addition, a Model object has various metadata. The followings are all attributes belonging to a single Model object. furiosa.registry.Model Represent the artifacts and metadata of a neural network model Attributes: Name Type Description name str a name of this model format Format the binary format type of model source; e.g., ONNX, tflite source bytes a source binary in ONNX or tflite. It can be used for compiling this model with a custom compiler configuration. dfg Optional [ bytes ] an intermediate representation of furiosa-compiler. Native post processor implementation uses dfg binary. Users don't need to use dfg directly. enf Optional [ bytes ] the executable binary for furiosa runtime and NPU version Optional [ str ] model version inputs Optional [ List [ ModelTensor ]] data type and shape of input tensors outputs Optional [ List [ ModelTensor ]] data type and shape of output tensors compiler_config Optional [ Dict ] a pre-defined compiler option Source code in furiosa/registry/model.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 class Model ( BaseModel ): \"\"\"Represent the artifacts and metadata of a neural network model Attributes: name: a name of this model format: the binary format type of model source; e.g., ONNX, tflite source: a source binary in ONNX or tflite. It can be used for compiling this model with a custom compiler configuration. dfg: an intermediate representation of furiosa-compiler. Native post processor implementation uses dfg binary. Users don't need to use `dfg` directly. enf: the executable binary for furiosa runtime and NPU version: model version inputs: data type and shape of input tensors outputs: data type and shape of output tensors compiler_config: a pre-defined compiler option \"\"\" # class Config(BaseConfig): # # Non pydantic attribute allowed # # https://pydantic-docs.helpmanual.io/usage/types/#arbitrary-types-allowed # arbitrary_types_allowed = True name : str source : bytes = Field ( repr = False ) format : Format dfg : Optional [ bytes ] = Field ( repr = False ) enf : Optional [ bytes ] = Field ( repr = False ) family : Optional [ str ] = None version : Optional [ str ] = None metadata : Optional [ Metadata ] = None inputs : Optional [ List [ ModelTensor ]] = [] outputs : Optional [ List [ ModelTensor ]] = [] compiler_config : Optional [ Dict ] = None Inferencing with Session API To load a model to FuriosaAI NPU, you need to create a session instance with a Model object through Furiosa SDK. As we mentioned above, even a single Model object has multiple model artifacts, such as a ONNX model and an ENF (FuriosaAI's compiled program binary). If an Model object is passed to session.create() , Session API chooses the ENF (FuriosaAI's Executable NPU Format) by default. In this case, session.create() doesn't involve any compilation because it uses the pre-compiled ENF binary. Info If you want to learn more about various options and features of Session API, please refer to Furiosa SDK - Tutorial and Code Examples . Users still can compile source models like ONNX or tflite if passing Model.source to session.create() . Compiling models will take some time up to minutes, but it allows to specify batch size and compiler configs, leading to more optimizations depending on user use-cases. To learn more about Model.source , please refer to Accessing artifacts and metadata . Example Using ENF binary Using ONNX model from furiosa.models.vision import SSDMobileNet from furiosa.runtime import session image = [ \"tests/assets/cat.jpg\" ] mobilenet = SSDMobileNet . load () with session . create ( mobilenet ) as sess : inputs , contexts = mobilenet . preprocess ( image ) outputs = sess . run ( inputs ) . numpy () mobilenet . postprocess ( outputs , contexts ) from furiosa.models.vision import SSDMobileNet from furiosa.runtime import session images = [ \"tests/assets/cat.jpg\" , \"tests/assets/cat.jpg\" ] mobilenet = SSDMobileNet . load () with session . create ( mobilenet . source , batch_size = 2 ) as sess : inputs , context = mobilenet . preprocess ( images ) outputs = sess . run ( inputs ) . numpy () mobilenet . postprocess ( outputs , context = context ) Pre/Postprocessing There are gaps between model input/outputs and user applications' desired input and output data. In general, inputs and outputs of a neural network model are tensors. In applications, user sample data are images in standard formats like PNG or JPEG, and users also need to convert the output tensors to struct data for user applications. A Model object also provides both preprocess() and postprocess() methods. They are utilities to convert easily user inputs to the model's input tensors and output tensors to struct data which can be easily accessible by applications. If using pre-built pre/postprocessing methods, users can quickly start using furiosa-models . In sum, typical steps of a single inference is as the following, as also shown at examples . Call preprocess() with user inputs (e.g., image files) Pass an output of preprocess() to Session.run() Pass the output of the model to postprocess() Info Default postprocessing implementations are in Python. However, some models have the native postprocessing implemented in Rust and C++ and optimized for FuriosaAI Warboy and Intel/AMD CPUs. Python implementations can run on CPU and GPU as well, whereas the native postprocessor implementations works with only FuriosaAI NPU. Native implementations are designed to leverage FuriosaAI NPU's characteristics even for post-processing and maximize the latency and throughput by using modern CPU architecture, such as CPU cache, SIMD instructions and CPU pipelining. According to our benchmark, the native implementations show at most 70% lower latency. To use native post processor, please pass use_native=True to Model.load() or Model.load_async() . The following is an example to use native post processor for SSDMobileNet . You can find more details of each mode page. Example from furiosa.models.vision import SSDMobileNet from furiosa.runtime import session image = [ \"tests/assets/cat.jpg\" ] mobilenet = SSDMobileNet . load ( use_native = True ) with session . create ( mobilenet ) as sess : inputs , contexts = mobilenet . preprocess ( image ) outputs = sess . run ( inputs ) . numpy () mobilenet . postprocess ( outputs , contexts [ 0 ]) See Also Furiosa SDK Documentation","title":"Model object"},{"location":"model_object/#model-object","text":"In furiosa-models project, Model is the first class object, and it represents a neural network model. This document explains what Model object offers and their usages.","title":"Model object"},{"location":"model_object/#loading-a-pre-trained-model","text":"To load a pre-trained neural-network model, you need to call load() method. Since the sizes of pre-trained model weights vary from tens to hundreds megabytes, the model images are not included in Python package. When load() method is called, a pre-trained model will be fetched over network. It takes some time (usually few seconds) depending on models and environments. Once the model images are fetched, they will be cached on a local disk. Non-blocking API load_async() also is available, and it can be used if your application is running through asynchronous executors (e.g., asyncio). Blocking API Non-blocking API from furiosa.models.types import Model from furiosa.models.vision import ResNet50 model : Model = ResNet50 . load () import asyncio from furiosa.models.types import Model from furiosa.models.vision import ResNet50 model : Model = asyncio . run ( ResNet50 . load_async ())","title":"Loading a pre-trained model"},{"location":"model_object/#accessing-artifacts-and-metadata","text":"A Model object includes model artifacts, such as ONNX, tflite, DFG, and ENF. DFG and ENF are FuriosaAI Compiler specific formats. Both formats are used for pre-compiled binary, and they are used to skip compilation times that take up to minutes. In addition, a Model object has various metadata. The followings are all attributes belonging to a single Model object.","title":"Accessing artifacts and metadata"},{"location":"model_object/#furiosaregistrymodel","text":"Represent the artifacts and metadata of a neural network model Attributes: Name Type Description name str a name of this model format Format the binary format type of model source; e.g., ONNX, tflite source bytes a source binary in ONNX or tflite. It can be used for compiling this model with a custom compiler configuration. dfg Optional [ bytes ] an intermediate representation of furiosa-compiler. Native post processor implementation uses dfg binary. Users don't need to use dfg directly. enf Optional [ bytes ] the executable binary for furiosa runtime and NPU version Optional [ str ] model version inputs Optional [ List [ ModelTensor ]] data type and shape of input tensors outputs Optional [ List [ ModelTensor ]] data type and shape of output tensors compiler_config Optional [ Dict ] a pre-defined compiler option Source code in furiosa/registry/model.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 class Model ( BaseModel ): \"\"\"Represent the artifacts and metadata of a neural network model Attributes: name: a name of this model format: the binary format type of model source; e.g., ONNX, tflite source: a source binary in ONNX or tflite. It can be used for compiling this model with a custom compiler configuration. dfg: an intermediate representation of furiosa-compiler. Native post processor implementation uses dfg binary. Users don't need to use `dfg` directly. enf: the executable binary for furiosa runtime and NPU version: model version inputs: data type and shape of input tensors outputs: data type and shape of output tensors compiler_config: a pre-defined compiler option \"\"\" # class Config(BaseConfig): # # Non pydantic attribute allowed # # https://pydantic-docs.helpmanual.io/usage/types/#arbitrary-types-allowed # arbitrary_types_allowed = True name : str source : bytes = Field ( repr = False ) format : Format dfg : Optional [ bytes ] = Field ( repr = False ) enf : Optional [ bytes ] = Field ( repr = False ) family : Optional [ str ] = None version : Optional [ str ] = None metadata : Optional [ Metadata ] = None inputs : Optional [ List [ ModelTensor ]] = [] outputs : Optional [ List [ ModelTensor ]] = [] compiler_config : Optional [ Dict ] = None","title":"furiosa.registry.Model"},{"location":"model_object/#inferencing-with-session-api","text":"To load a model to FuriosaAI NPU, you need to create a session instance with a Model object through Furiosa SDK. As we mentioned above, even a single Model object has multiple model artifacts, such as a ONNX model and an ENF (FuriosaAI's compiled program binary). If an Model object is passed to session.create() , Session API chooses the ENF (FuriosaAI's Executable NPU Format) by default. In this case, session.create() doesn't involve any compilation because it uses the pre-compiled ENF binary. Info If you want to learn more about various options and features of Session API, please refer to Furiosa SDK - Tutorial and Code Examples . Users still can compile source models like ONNX or tflite if passing Model.source to session.create() . Compiling models will take some time up to minutes, but it allows to specify batch size and compiler configs, leading to more optimizations depending on user use-cases. To learn more about Model.source , please refer to Accessing artifacts and metadata . Example Using ENF binary Using ONNX model from furiosa.models.vision import SSDMobileNet from furiosa.runtime import session image = [ \"tests/assets/cat.jpg\" ] mobilenet = SSDMobileNet . load () with session . create ( mobilenet ) as sess : inputs , contexts = mobilenet . preprocess ( image ) outputs = sess . run ( inputs ) . numpy () mobilenet . postprocess ( outputs , contexts ) from furiosa.models.vision import SSDMobileNet from furiosa.runtime import session images = [ \"tests/assets/cat.jpg\" , \"tests/assets/cat.jpg\" ] mobilenet = SSDMobileNet . load () with session . create ( mobilenet . source , batch_size = 2 ) as sess : inputs , context = mobilenet . preprocess ( images ) outputs = sess . run ( inputs ) . numpy () mobilenet . postprocess ( outputs , context = context )","title":"Inferencing with Session API"},{"location":"model_object/#prepostprocessing","text":"There are gaps between model input/outputs and user applications' desired input and output data. In general, inputs and outputs of a neural network model are tensors. In applications, user sample data are images in standard formats like PNG or JPEG, and users also need to convert the output tensors to struct data for user applications. A Model object also provides both preprocess() and postprocess() methods. They are utilities to convert easily user inputs to the model's input tensors and output tensors to struct data which can be easily accessible by applications. If using pre-built pre/postprocessing methods, users can quickly start using furiosa-models . In sum, typical steps of a single inference is as the following, as also shown at examples . Call preprocess() with user inputs (e.g., image files) Pass an output of preprocess() to Session.run() Pass the output of the model to postprocess() Info Default postprocessing implementations are in Python. However, some models have the native postprocessing implemented in Rust and C++ and optimized for FuriosaAI Warboy and Intel/AMD CPUs. Python implementations can run on CPU and GPU as well, whereas the native postprocessor implementations works with only FuriosaAI NPU. Native implementations are designed to leverage FuriosaAI NPU's characteristics even for post-processing and maximize the latency and throughput by using modern CPU architecture, such as CPU cache, SIMD instructions and CPU pipelining. According to our benchmark, the native implementations show at most 70% lower latency. To use native post processor, please pass use_native=True to Model.load() or Model.load_async() . The following is an example to use native post processor for SSDMobileNet . You can find more details of each mode page. Example from furiosa.models.vision import SSDMobileNet from furiosa.runtime import session image = [ \"tests/assets/cat.jpg\" ] mobilenet = SSDMobileNet . load ( use_native = True ) with session . create ( mobilenet ) as sess : inputs , contexts = mobilenet . preprocess ( image ) outputs = sess . run ( inputs ) . numpy () mobilenet . postprocess ( outputs , contexts [ 0 ])","title":"Pre/Postprocessing"},{"location":"model_object/#see-also","text":"Furiosa SDK Documentation","title":"See Also"},{"location":"models/resnet50_v1.5/","text":"ResNet50 v1.5 ResNet50 v1.5 backbone model trained on ImageNet (224x224). This model has been used since MLCommons v0.5. Overall Framework: PyTorch Model format: ONNX Model task: Image classification Source: This model is originated from ResNet50 v1.5 in ONNX available at MLCommons - Supported Models . Usages Python Postprocessor Native Postprocessor from furiosa.models.vision import ResNet50 from furiosa.runtime import session image = \"tests/assets/cat.jpg\" resnet50 = ResNet50 . load () with session . create ( resnet50 ) as sess : inputs , _ = resnet50 . preprocess ( image ) outputs = sess . run ( inputs ) . numpy () resnet50 . postprocess ( outputs ) from furiosa.models.vision import ResNet50 from furiosa.runtime import session image = \"tests/assets/cat.jpg\" resnet50 = ResNet50 . load ( use_native = True ) with session . create ( resnet50 ) as sess : inputs , _ = resnet50 . preprocess ( image ) outputs = sess . run ( inputs ) . numpy () resnet50 . postprocess ( outputs ) Inputs of Model The input is a 3-channel image of 224x224 (height, width). Data Type: numpy.float32 Tensor Shape: [1, 3, 224, 224] Memory Format: NCHW, where: N - batch size C - number of channels H - image height W - image width NPU Optimal Batch Size: <= 8 Output of Model The output is a numpy.float32 tensor with the shape ( [1,] ), including a class id. postprocess() can transform the class id to a single label. Pre/Postprocessing furiosa.models.vision.ResNet50 class provides preprocess and postprocess methods that convert input images to input tensors and the model outputs to labels respectively. You can find examples at ResNet50 Usage . furiosa.models.vision.ResNet50.preprocess Preprocess an input image to an input tensor of ResNet50. This function can take a standard image file (e.g., jpg, gif, png) and return a numpy array. Parameters: Name Type Description Default image A path of an image or an image loaded as numpy from cv2.imread() required furiosa.models.vision.ResNet50.postprocess Convert the outputs of a model to a label string, such as car and cat. Parameters: Name Type Description Default model_outputs Sequence [ npt . ArrayLike ] the outputs of the model required Native Postprocessor This class provides another version of the postprocessing implementation which is highly optimized for NPU. The implementation leverages the NPU IO architecture and runtime. To use this implementation, when this model is loaded, the parameter use_native=True should be passed to load() or load_aync() . The following is an example: Example from furiosa.models.vision import ResNet50 from furiosa.runtime import session image = \"tests/assets/cat.jpg\" resnet50 = ResNet50 . load ( use_native = True ) with session . create ( resnet50 ) as sess : inputs , _ = resnet50 . preprocess ( image ) outputs = sess . run ( inputs ) . numpy () resnet50 . postprocess ( outputs )","title":"ResNet50 v1.5"},{"location":"models/resnet50_v1.5/#resnet50-v15","text":"ResNet50 v1.5 backbone model trained on ImageNet (224x224). This model has been used since MLCommons v0.5.","title":"ResNet50 v1.5"},{"location":"models/resnet50_v1.5/#overall","text":"Framework: PyTorch Model format: ONNX Model task: Image classification Source: This model is originated from ResNet50 v1.5 in ONNX available at MLCommons - Supported Models .","title":"Overall"},{"location":"models/resnet50_v1.5/#_1","text":"Usages Python Postprocessor Native Postprocessor from furiosa.models.vision import ResNet50 from furiosa.runtime import session image = \"tests/assets/cat.jpg\" resnet50 = ResNet50 . load () with session . create ( resnet50 ) as sess : inputs , _ = resnet50 . preprocess ( image ) outputs = sess . run ( inputs ) . numpy () resnet50 . postprocess ( outputs ) from furiosa.models.vision import ResNet50 from furiosa.runtime import session image = \"tests/assets/cat.jpg\" resnet50 = ResNet50 . load ( use_native = True ) with session . create ( resnet50 ) as sess : inputs , _ = resnet50 . preprocess ( image ) outputs = sess . run ( inputs ) . numpy () resnet50 . postprocess ( outputs )","title":""},{"location":"models/resnet50_v1.5/#inputs-of-model","text":"The input is a 3-channel image of 224x224 (height, width). Data Type: numpy.float32 Tensor Shape: [1, 3, 224, 224] Memory Format: NCHW, where: N - batch size C - number of channels H - image height W - image width NPU Optimal Batch Size: <= 8","title":"Inputs of Model"},{"location":"models/resnet50_v1.5/#output-of-model","text":"The output is a numpy.float32 tensor with the shape ( [1,] ), including a class id. postprocess() can transform the class id to a single label.","title":"Output of Model"},{"location":"models/resnet50_v1.5/#prepostprocessing","text":"furiosa.models.vision.ResNet50 class provides preprocess and postprocess methods that convert input images to input tensors and the model outputs to labels respectively. You can find examples at ResNet50 Usage .","title":"Pre/Postprocessing"},{"location":"models/resnet50_v1.5/#furiosamodelsvisionresnet50preprocess","text":"Preprocess an input image to an input tensor of ResNet50. This function can take a standard image file (e.g., jpg, gif, png) and return a numpy array. Parameters: Name Type Description Default image A path of an image or an image loaded as numpy from cv2.imread() required","title":"furiosa.models.vision.ResNet50.preprocess"},{"location":"models/resnet50_v1.5/#furiosamodelsvisionresnet50postprocess","text":"Convert the outputs of a model to a label string, such as car and cat. Parameters: Name Type Description Default model_outputs Sequence [ npt . ArrayLike ] the outputs of the model required","title":"furiosa.models.vision.ResNet50.postprocess"},{"location":"models/resnet50_v1.5/#native-postprocessor","text":"This class provides another version of the postprocessing implementation which is highly optimized for NPU. The implementation leverages the NPU IO architecture and runtime. To use this implementation, when this model is loaded, the parameter use_native=True should be passed to load() or load_aync() . The following is an example: Example from furiosa.models.vision import ResNet50 from furiosa.runtime import session image = \"tests/assets/cat.jpg\" resnet50 = ResNet50 . load ( use_native = True ) with session . create ( resnet50 ) as sess : inputs , _ = resnet50 . preprocess ( image ) outputs = sess . run ( inputs ) . numpy () resnet50 . postprocess ( outputs )","title":"Native Postprocessor"},{"location":"models/ssd_mobilenet/","text":"SSD MobileNet v1 SSD MobileNet v1 backbone model trained on COCO (300x300). This model has been used since MLCommons v0.5. Overall Framework: PyTorch Model format: ONNX Model task: Object detection Source: This model is originated from SSD MobileNet v1 in ONNX available at MLCommons - Supported Models . Usages Python Postprocessor Native Postprocessor from furiosa.models.vision import SSDMobileNet from furiosa.runtime import session image = [ \"tests/assets/cat.jpg\" ] mobilenet = SSDMobileNet . load () with session . create ( mobilenet ) as sess : inputs , contexts = mobilenet . preprocess ( image ) outputs = sess . run ( inputs ) . numpy () mobilenet . postprocess ( outputs , contexts ) from furiosa.models.vision import SSDMobileNet from furiosa.runtime import session image = [ \"tests/assets/cat.jpg\" ] mobilenet = SSDMobileNet . load ( use_native = True ) with session . create ( mobilenet ) as sess : inputs , contexts = mobilenet . preprocess ( image ) outputs = sess . run ( inputs ) . numpy () mobilenet . postprocess ( outputs , contexts [ 0 ]) Inputs of Model The input is a 3-channel image of 300x300 (height, width). Data Type: numpy.float32 Tensor Shape: [1, 3, 300, 300] Memory Format: NCHW, where: N - batch size C - number of channels H - image height W - image width Optimal Batch Size: <= 8 Outputs of Model The outputs are 12 numpy.float32 tensors in various shapes as the following. You can refer to postprocess() function to learn how to decode boxes, classes, and confidence scores. Tensor Shape Data Type Data Type Description 0 (1, 273, 19, 19) float32 NCHW 1 (1, 12, 19, 19) float32 NCHW 2 (1, 546, 10, 10) float32 NCHW 3 (1, 24, 10, 10) float32 NCHW 4 (1, 546, 5, 5) float32 NCHW 5 (1, 24, 5, 5) float32 NCHW 6 (1, 546, 3, 3) float32 NCHW 7 (1, 24, 3, 3) float32 NCHW 8 (1, 546, 2, 2) float32 NCHW 9 (1, 24, 2, 2) float32 NCHW 10 (1, 546, 1, 1) float32 NCHW 11 (1, 24, 1, 1) float32 NCHW Pre/Postprocessing furiosa.models.vision.SSDMobileNet class provides preprocess and postprocess methods. preprocess method converts input images to input tensors, and postprocess method converts model output tensors to a list of bounding boxes, scores and labels. You can find examples at SSDMobileNet Usage . furiosa.models.vision.SSDMobileNet.preprocess Preprocess input images to a batch of input tensors. When the image file paths are passed, the image files should be standard image format, such as jpg, gif, png. Parameters: Name Type Description Default images A list of paths of image files or a stacked image loaded as numpy through cv2.imread() required Returns: Type Description Tuple [ npt . ArrayLike , List [ Dict [ str , Any ]]] 3-channel images of 300x300 in NCHW format. Please find the details at 'Inputs of Model' section. furiosa.models.vision.SSDMobileNet.postprocess Convert the outputs of this model to a list of bounding boxes, scores and labels Parameters: Name Type Description Default model_outputs Sequence [ numpy . ndarray ] the outputs of the model required context context coming from preprocess() required Returns: Type Description List [ List [ ObjectDetectionResult ]] Detected Bounding Box and its score and label represented as ObjectDetectionResult . To learn more about ObjectDetectionResult , 'Definition of ObjectDetectionResult' can be found below. Definition of ObjectDetectionResult Source code in furiosa/models/vision/postprocess.py 32 33 34 35 36 37 38 39 40 @dataclass class LtrbBoundingBox : left : float top : float right : float bottom : float def __iter__ ( self ) -> Iterator [ float ]: return iter ([ self . left , self . top , self . right , self . bottom ]) Source code in furiosa/models/vision/postprocess.py 89 90 91 92 93 94 @dataclass class ObjectDetectionResult : boundingbox : LtrbBoundingBox score : float label : str index : int Native Postprocessor This class provides another version of the postprocessing implementation which is highly optimized for NPU. The implementation leverages the NPU IO architecture and runtime. To use this implementation, when this model is loaded, the parameter use_native=True should be passed to load() or load_aync() . The following is an example: Example from furiosa.models.vision import SSDMobileNet from furiosa.runtime import session image = [ \"tests/assets/cat.jpg\" ] mobilenet = SSDMobileNet . load ( use_native = True ) with session . create ( mobilenet ) as sess : inputs , contexts = mobilenet . preprocess ( image ) outputs = sess . run ( inputs ) . numpy () mobilenet . postprocess ( outputs , contexts [ 0 ])","title":"SSD MobileNet v1"},{"location":"models/ssd_mobilenet/#ssd-mobilenet-v1","text":"SSD MobileNet v1 backbone model trained on COCO (300x300). This model has been used since MLCommons v0.5.","title":"SSD MobileNet v1"},{"location":"models/ssd_mobilenet/#overall","text":"Framework: PyTorch Model format: ONNX Model task: Object detection Source: This model is originated from SSD MobileNet v1 in ONNX available at MLCommons - Supported Models .","title":"Overall"},{"location":"models/ssd_mobilenet/#_1","text":"Usages Python Postprocessor Native Postprocessor from furiosa.models.vision import SSDMobileNet from furiosa.runtime import session image = [ \"tests/assets/cat.jpg\" ] mobilenet = SSDMobileNet . load () with session . create ( mobilenet ) as sess : inputs , contexts = mobilenet . preprocess ( image ) outputs = sess . run ( inputs ) . numpy () mobilenet . postprocess ( outputs , contexts ) from furiosa.models.vision import SSDMobileNet from furiosa.runtime import session image = [ \"tests/assets/cat.jpg\" ] mobilenet = SSDMobileNet . load ( use_native = True ) with session . create ( mobilenet ) as sess : inputs , contexts = mobilenet . preprocess ( image ) outputs = sess . run ( inputs ) . numpy () mobilenet . postprocess ( outputs , contexts [ 0 ])","title":""},{"location":"models/ssd_mobilenet/#inputs-of-model","text":"The input is a 3-channel image of 300x300 (height, width). Data Type: numpy.float32 Tensor Shape: [1, 3, 300, 300] Memory Format: NCHW, where: N - batch size C - number of channels H - image height W - image width Optimal Batch Size: <= 8","title":"Inputs of Model"},{"location":"models/ssd_mobilenet/#outputs-of-model","text":"The outputs are 12 numpy.float32 tensors in various shapes as the following. You can refer to postprocess() function to learn how to decode boxes, classes, and confidence scores. Tensor Shape Data Type Data Type Description 0 (1, 273, 19, 19) float32 NCHW 1 (1, 12, 19, 19) float32 NCHW 2 (1, 546, 10, 10) float32 NCHW 3 (1, 24, 10, 10) float32 NCHW 4 (1, 546, 5, 5) float32 NCHW 5 (1, 24, 5, 5) float32 NCHW 6 (1, 546, 3, 3) float32 NCHW 7 (1, 24, 3, 3) float32 NCHW 8 (1, 546, 2, 2) float32 NCHW 9 (1, 24, 2, 2) float32 NCHW 10 (1, 546, 1, 1) float32 NCHW 11 (1, 24, 1, 1) float32 NCHW","title":"Outputs of Model"},{"location":"models/ssd_mobilenet/#prepostprocessing","text":"furiosa.models.vision.SSDMobileNet class provides preprocess and postprocess methods. preprocess method converts input images to input tensors, and postprocess method converts model output tensors to a list of bounding boxes, scores and labels. You can find examples at SSDMobileNet Usage .","title":"Pre/Postprocessing"},{"location":"models/ssd_mobilenet/#furiosamodelsvisionssdmobilenetpreprocess","text":"Preprocess input images to a batch of input tensors. When the image file paths are passed, the image files should be standard image format, such as jpg, gif, png. Parameters: Name Type Description Default images A list of paths of image files or a stacked image loaded as numpy through cv2.imread() required Returns: Type Description Tuple [ npt . ArrayLike , List [ Dict [ str , Any ]]] 3-channel images of 300x300 in NCHW format. Please find the details at 'Inputs of Model' section.","title":"furiosa.models.vision.SSDMobileNet.preprocess"},{"location":"models/ssd_mobilenet/#furiosamodelsvisionssdmobilenetpostprocess","text":"Convert the outputs of this model to a list of bounding boxes, scores and labels Parameters: Name Type Description Default model_outputs Sequence [ numpy . ndarray ] the outputs of the model required context context coming from preprocess() required Returns: Type Description List [ List [ ObjectDetectionResult ]] Detected Bounding Box and its score and label represented as ObjectDetectionResult . To learn more about ObjectDetectionResult , 'Definition of ObjectDetectionResult' can be found below. Definition of ObjectDetectionResult Source code in furiosa/models/vision/postprocess.py 32 33 34 35 36 37 38 39 40 @dataclass class LtrbBoundingBox : left : float top : float right : float bottom : float def __iter__ ( self ) -> Iterator [ float ]: return iter ([ self . left , self . top , self . right , self . bottom ]) Source code in furiosa/models/vision/postprocess.py 89 90 91 92 93 94 @dataclass class ObjectDetectionResult : boundingbox : LtrbBoundingBox score : float label : str index : int","title":"furiosa.models.vision.SSDMobileNet.postprocess"},{"location":"models/ssd_mobilenet/#native-postprocessor","text":"This class provides another version of the postprocessing implementation which is highly optimized for NPU. The implementation leverages the NPU IO architecture and runtime. To use this implementation, when this model is loaded, the parameter use_native=True should be passed to load() or load_aync() . The following is an example: Example from furiosa.models.vision import SSDMobileNet from furiosa.runtime import session image = [ \"tests/assets/cat.jpg\" ] mobilenet = SSDMobileNet . load ( use_native = True ) with session . create ( mobilenet ) as sess : inputs , contexts = mobilenet . preprocess ( image ) outputs = sess . run ( inputs ) . numpy () mobilenet . postprocess ( outputs , contexts [ 0 ])","title":"Native Postprocessor"},{"location":"models/ssd_resnet34/","text":"SSD ResNet34 SSD ResNet34 backbone model trained on COCO (1200x1200). This model has been used since MLCommons v0.5. Overall Framework: PyTorch Model format: ONNX Model task: Object detection Source: This model is originated from SSD ResNet34 in ONNX available at MLCommons - Supported Models . Usages Python Postprocessor Native Postprocessor from furiosa.models.vision import SSDResNet34 from furiosa.runtime import session resnet34 = SSDResNet34 . load () with session . create ( resnet34 ) as sess : image , context = resnet34 . preprocess ([ \"tests/assets/cat.jpg\" ]) output = sess . run ( image ) . numpy () resnet34 . postprocess ( output , context = context ) from furiosa.models.vision import SSDResNet34 from furiosa.runtime import session resnet34 = SSDResNet34 . load ( use_native = True ) with session . create ( resnet34 ) as sess : image , context = resnet34 . preprocess ([ \"tests/assets/cat.jpg\" ]) output = sess . run ( image ) . numpy () resnet34 . postprocessor ( output , context = context [ 0 ]) Model inputs The input is a 3-channel image of 300x300 (height, width). Data Type: numpy.float32 Tensor Shape: [1, 3, 1200, 1200] Memory Format: NCHW, where N - batch size C - number of channels H - image height W - image width Optimal Batch Size: 1 Outputs The outputs are 12 numpy.float32 tensors in various shapes as the following. You can refer to postprocess() function to learn how to decode boxes, classes, and confidence scores. Tensor Shape Data Type Data Type Description 0 (1, 324, 50, 50) float32 NCHW 1 (1, 486, 25, 25) float32 NCHW 2 (1, 486, 13, 13) float32 NCHW 3 (1, 486, 7, 7) float32 NCHW 4 (1, 324, 3, 3) float32 NCHW 5 (1, 324, 3, 3) float32 NCHW 6 (1, 16, 50, 50) float32 NCHW 7 (1, 24, 25, 25) float32 NCHW 8 (1, 24, 13, 13) float32 NCHW 9 (1, 24, 7, 7) float32 NCHW 10 (1, 16, 3, 3) float32 NCHW 11 (1, 16, 3, 3) float32 NCHW Pre/Postprocessing furiosa.models.vision.SSDResNet34 class provides preprocess and postprocess methods. preprocess method converts input images to input tensors, and postprocess method converts model output tensors to a list of bounding boxes, scores and labels. You can find examples at SSDResNet34 Usage . furiosa.models.vision.SSDResNet34.preprocess Preprocess input images to a batch of input tensors. When the image file paths are passed, the image files should be standard image format, such as jpg, gif, png. Parameters: Name Type Description Default images A list of paths of image files or a stacked image loaded as numpy through cv2.imread() required Returns: Type Description Tuple [ npt . ArrayLike , List [ Dict [ str , Any ]]] 3-channel images of 1200x1200 in NCHW format. Please find the details at 'Inputs of Model' section. furiosa.models.vision.SSDResNet34.postprocess Preprocess input images to a batch of input tensors. When the image file paths are passed, the image files should be standard image format, such as jpg, gif, png. Parameters: Name Type Description Default images A list of paths of image files or a stacked image loaded as numpy through cv2.imread() required Returns: Type Description Tuple [ npt . ArrayLike , List [ Dict [ str , Any ]]] 3-channel images of 1200x1200 in NCHW format. Please find the details at 'Inputs of Model' section. Native Postprocessor This class provides another version of the postprocessing implementation which is highly optimized for NPU. The implementation leverages the NPU IO architecture and runtime. To use this implementation, when this model is loaded, the parameter use_native=True should be passed to load() or load_aync() . The following is an example: Example from furiosa.models.vision import SSDResNet34 from furiosa.runtime import session resnet34 = SSDResNet34 . load ( use_native = True ) with session . create ( resnet34 ) as sess : image , context = resnet34 . preprocess ([ \"tests/assets/cat.jpg\" ]) output = sess . run ( image ) . numpy () resnet34 . postprocessor ( output , context = context [ 0 ])","title":"SSD ResNet34"},{"location":"models/ssd_resnet34/#ssd-resnet34","text":"SSD ResNet34 backbone model trained on COCO (1200x1200). This model has been used since MLCommons v0.5.","title":"SSD ResNet34"},{"location":"models/ssd_resnet34/#overall","text":"Framework: PyTorch Model format: ONNX Model task: Object detection Source: This model is originated from SSD ResNet34 in ONNX available at MLCommons - Supported Models .","title":"Overall"},{"location":"models/ssd_resnet34/#_1","text":"Usages Python Postprocessor Native Postprocessor from furiosa.models.vision import SSDResNet34 from furiosa.runtime import session resnet34 = SSDResNet34 . load () with session . create ( resnet34 ) as sess : image , context = resnet34 . preprocess ([ \"tests/assets/cat.jpg\" ]) output = sess . run ( image ) . numpy () resnet34 . postprocess ( output , context = context ) from furiosa.models.vision import SSDResNet34 from furiosa.runtime import session resnet34 = SSDResNet34 . load ( use_native = True ) with session . create ( resnet34 ) as sess : image , context = resnet34 . preprocess ([ \"tests/assets/cat.jpg\" ]) output = sess . run ( image ) . numpy () resnet34 . postprocessor ( output , context = context [ 0 ])","title":""},{"location":"models/ssd_resnet34/#model-inputs","text":"The input is a 3-channel image of 300x300 (height, width). Data Type: numpy.float32 Tensor Shape: [1, 3, 1200, 1200] Memory Format: NCHW, where N - batch size C - number of channels H - image height W - image width Optimal Batch Size: 1","title":"Model inputs"},{"location":"models/ssd_resnet34/#outputs","text":"The outputs are 12 numpy.float32 tensors in various shapes as the following. You can refer to postprocess() function to learn how to decode boxes, classes, and confidence scores. Tensor Shape Data Type Data Type Description 0 (1, 324, 50, 50) float32 NCHW 1 (1, 486, 25, 25) float32 NCHW 2 (1, 486, 13, 13) float32 NCHW 3 (1, 486, 7, 7) float32 NCHW 4 (1, 324, 3, 3) float32 NCHW 5 (1, 324, 3, 3) float32 NCHW 6 (1, 16, 50, 50) float32 NCHW 7 (1, 24, 25, 25) float32 NCHW 8 (1, 24, 13, 13) float32 NCHW 9 (1, 24, 7, 7) float32 NCHW 10 (1, 16, 3, 3) float32 NCHW 11 (1, 16, 3, 3) float32 NCHW","title":"Outputs"},{"location":"models/ssd_resnet34/#prepostprocessing","text":"furiosa.models.vision.SSDResNet34 class provides preprocess and postprocess methods. preprocess method converts input images to input tensors, and postprocess method converts model output tensors to a list of bounding boxes, scores and labels. You can find examples at SSDResNet34 Usage .","title":"Pre/Postprocessing"},{"location":"models/ssd_resnet34/#furiosamodelsvisionssdresnet34preprocess","text":"Preprocess input images to a batch of input tensors. When the image file paths are passed, the image files should be standard image format, such as jpg, gif, png. Parameters: Name Type Description Default images A list of paths of image files or a stacked image loaded as numpy through cv2.imread() required Returns: Type Description Tuple [ npt . ArrayLike , List [ Dict [ str , Any ]]] 3-channel images of 1200x1200 in NCHW format. Please find the details at 'Inputs of Model' section.","title":"furiosa.models.vision.SSDResNet34.preprocess"},{"location":"models/ssd_resnet34/#furiosamodelsvisionssdresnet34postprocess","text":"Preprocess input images to a batch of input tensors. When the image file paths are passed, the image files should be standard image format, such as jpg, gif, png. Parameters: Name Type Description Default images A list of paths of image files or a stacked image loaded as numpy through cv2.imread() required Returns: Type Description Tuple [ npt . ArrayLike , List [ Dict [ str , Any ]]] 3-channel images of 1200x1200 in NCHW format. Please find the details at 'Inputs of Model' section.","title":"furiosa.models.vision.SSDResNet34.postprocess"},{"location":"models/ssd_resnet34/#native-postprocessor","text":"This class provides another version of the postprocessing implementation which is highly optimized for NPU. The implementation leverages the NPU IO architecture and runtime. To use this implementation, when this model is loaded, the parameter use_native=True should be passed to load() or load_aync() . The following is an example: Example from furiosa.models.vision import SSDResNet34 from furiosa.runtime import session resnet34 = SSDResNet34 . load ( use_native = True ) with session . create ( resnet34 ) as sess : image , context = resnet34 . preprocess ([ \"tests/assets/cat.jpg\" ]) output = sess . run ( image ) . numpy () resnet34 . postprocessor ( output , context = context [ 0 ])","title":"Native Postprocessor"},{"location":"models/yolov5l/","text":"YOLOv5L YOLOv5 is the one of the most popular object detection models developed by Ultralytics . You can find more details at https://github.com/ultralytics/yolov5. Overall Framework: PyTorch Model format: ONNX Model task: Object Detection Source: This model is originated from https://github.com/ultralytics/yolov5 Usage import cv2 import numpy as np from furiosa.models.vision import YOLOv5l from furiosa.runtime import session yolov5l = YOLOv5l . load () with session . create ( yolov5l ) as sess : image = cv2 . imread ( \"tests/assets/yolov5-test.jpg\" ) inputs , context = yolov5l . preprocess ([ image ]) output = sess . run ( np . expand_dims ( inputs [ 0 ], axis = 0 )) . numpy () yolov5l . postprocess ( output , context = context ) Model inputs The input is a 3-channel image of 640, 640 (height, width). Data Type: numpy.uint8 Tensor Shape: [1, 640, 640, 3] Memory Format: NHWC, where N - batch size H - image height W - image width C - number of channels Optimal Batch Size: 1 Outputs The outputs are 3 numpy.float32 tensors in various shapes as the following. You can refer to postprocess() function to learn how to decode boxes, classes, and confidence scores. Tensor Shape Data Type Data Type Description 0 (1, 45, 80, 80) float32 NCHW 1 (1, 45, 40, 40) float32 NCHW 2 (1, 45, 20, 20) float32 NCHW Pre/Post processing furiosa.models.vision.YOLOv5l class provides preprocess and postprocess methods. preprocess method converts input images to input tensors, and postprocess method converts model output tensors to a list of bounding boxes, scores and labels. You can find examples at YOLOv5l Usage . furiosa.models.vision.YOLOv5l.preprocess Preprocess a batch of images in numpy Parameters: Name Type Description Default images Sequence [ np . ndarray ] Color images have (NCHW: Batch, Channel, Height, Width) dimensions. required input_color_format str 'rgb' (Red, Green, Blue) or 'bgr' (Blue, Green, Red). required Returns: Type Description Tuple [ np . ndarray , List [ Dict [ str , Any ]]] a pre-processed image, scales and padded sizes(width,height) per images. The first element is a preprocessed image, and the second element is a dictionary object to be used for postprocess. 'scale' key of the returned dict has a rescaled ratio per width(=target/width) and height(=target/height), and the 'pad' key has padded width and height pixels. Specially, the last dictionary element of returing tuple will be passed to postprocessing as a parameter to calculate predicted coordinates on normalized coordinates back to an input image coordinator. furiosa.models.vision.YOLOv5l.postprocess Convert the outputs of this model to a list of bounding boxes, scores and labels Parameters: Name Type Description Default model_outputs Sequence [ np . array ] P3/8, P4/16, P5/32 features from yolov5l model. required context Dict [ str , Any ] A configuration for each image generated by the preprocessor. For example, it could be the reduction ratio of the image, the actual image width and height. required conf_threshold float Confidence score threshold. The default to 0.25 required iou_thres float IoU threshold value for the NMS processing. The default to 0.45. 0.45 Returns: Type Description List [ List [ ObjectDetectionResult ]] Detected Bounding Box and its score and label represented as ObjectDetectionResult . To learn more about ObjectDetectionResult , 'Definition of ObjectDetectionResult' can be found below. Definition of ObjectDetectionResult Source code in furiosa/models/vision/postprocess.py 32 33 34 35 36 37 38 39 40 @dataclass class LtrbBoundingBox : left : float top : float right : float bottom : float def __iter__ ( self ) -> Iterator [ float ]: return iter ([ self . left , self . top , self . right , self . bottom ]) Source code in furiosa/models/vision/postprocess.py 89 90 91 92 93 94 @dataclass class ObjectDetectionResult : boundingbox : LtrbBoundingBox score : float label : str index : int","title":"YOLOv5L"},{"location":"models/yolov5l/#yolov5l","text":"YOLOv5 is the one of the most popular object detection models developed by Ultralytics . You can find more details at https://github.com/ultralytics/yolov5.","title":"YOLOv5L"},{"location":"models/yolov5l/#overall","text":"Framework: PyTorch Model format: ONNX Model task: Object Detection Source: This model is originated from https://github.com/ultralytics/yolov5","title":"Overall"},{"location":"models/yolov5l/#_1","text":"Usage import cv2 import numpy as np from furiosa.models.vision import YOLOv5l from furiosa.runtime import session yolov5l = YOLOv5l . load () with session . create ( yolov5l ) as sess : image = cv2 . imread ( \"tests/assets/yolov5-test.jpg\" ) inputs , context = yolov5l . preprocess ([ image ]) output = sess . run ( np . expand_dims ( inputs [ 0 ], axis = 0 )) . numpy () yolov5l . postprocess ( output , context = context )","title":""},{"location":"models/yolov5l/#model-inputs","text":"The input is a 3-channel image of 640, 640 (height, width). Data Type: numpy.uint8 Tensor Shape: [1, 640, 640, 3] Memory Format: NHWC, where N - batch size H - image height W - image width C - number of channels Optimal Batch Size: 1","title":"Model inputs"},{"location":"models/yolov5l/#outputs","text":"The outputs are 3 numpy.float32 tensors in various shapes as the following. You can refer to postprocess() function to learn how to decode boxes, classes, and confidence scores. Tensor Shape Data Type Data Type Description 0 (1, 45, 80, 80) float32 NCHW 1 (1, 45, 40, 40) float32 NCHW 2 (1, 45, 20, 20) float32 NCHW","title":"Outputs"},{"location":"models/yolov5l/#prepost-processing","text":"furiosa.models.vision.YOLOv5l class provides preprocess and postprocess methods. preprocess method converts input images to input tensors, and postprocess method converts model output tensors to a list of bounding boxes, scores and labels. You can find examples at YOLOv5l Usage .","title":"Pre/Post processing"},{"location":"models/yolov5l/#furiosamodelsvisionyolov5lpreprocess","text":"Preprocess a batch of images in numpy Parameters: Name Type Description Default images Sequence [ np . ndarray ] Color images have (NCHW: Batch, Channel, Height, Width) dimensions. required input_color_format str 'rgb' (Red, Green, Blue) or 'bgr' (Blue, Green, Red). required Returns: Type Description Tuple [ np . ndarray , List [ Dict [ str , Any ]]] a pre-processed image, scales and padded sizes(width,height) per images. The first element is a preprocessed image, and the second element is a dictionary object to be used for postprocess. 'scale' key of the returned dict has a rescaled ratio per width(=target/width) and height(=target/height), and the 'pad' key has padded width and height pixels. Specially, the last dictionary element of returing tuple will be passed to postprocessing as a parameter to calculate predicted coordinates on normalized coordinates back to an input image coordinator.","title":"furiosa.models.vision.YOLOv5l.preprocess"},{"location":"models/yolov5l/#furiosamodelsvisionyolov5lpostprocess","text":"Convert the outputs of this model to a list of bounding boxes, scores and labels Parameters: Name Type Description Default model_outputs Sequence [ np . array ] P3/8, P4/16, P5/32 features from yolov5l model. required context Dict [ str , Any ] A configuration for each image generated by the preprocessor. For example, it could be the reduction ratio of the image, the actual image width and height. required conf_threshold float Confidence score threshold. The default to 0.25 required iou_thres float IoU threshold value for the NMS processing. The default to 0.45. 0.45 Returns: Type Description List [ List [ ObjectDetectionResult ]] Detected Bounding Box and its score and label represented as ObjectDetectionResult . To learn more about ObjectDetectionResult , 'Definition of ObjectDetectionResult' can be found below. Definition of ObjectDetectionResult Source code in furiosa/models/vision/postprocess.py 32 33 34 35 36 37 38 39 40 @dataclass class LtrbBoundingBox : left : float top : float right : float bottom : float def __iter__ ( self ) -> Iterator [ float ]: return iter ([ self . left , self . top , self . right , self . bottom ]) Source code in furiosa/models/vision/postprocess.py 89 90 91 92 93 94 @dataclass class ObjectDetectionResult : boundingbox : LtrbBoundingBox score : float label : str index : int","title":"furiosa.models.vision.YOLOv5l.postprocess"},{"location":"models/yolov5m/","text":"YOLOv5m YOLOv5 is the one of the most popular object detection models developed by Ultralytics . You can find more details at https://github.com/ultralytics/yolov5. Overall Framework: PyTorch Model format: ONNX Model task: Object Detection Source: This model is originated from https://github.com/ultralytics/yolov5. Usage import cv2 import numpy as np from furiosa.models.vision import YOLOv5m from furiosa.runtime import session yolov5m = YOLOv5m . load () with session . create ( yolov5m ) as sess : image = cv2 . imread ( \"tests/assets/yolov5-test.jpg\" ) inputs , context = yolov5m . preprocess ([ image ]) output = sess . run ( np . expand_dims ( inputs [ 0 ], axis = 0 )) . numpy () yolov5m . postprocess ( output , context = context ) Model inputs The input is a 3-channel image of 640, 640 (height, width). Data Type: numpy.uint8 Tensor Shape: [1, 640, 640, 3] Memory Format: NHWC, where N - batch size H - image height W - image width C - number of channels Optimal Batch Size: 1 Outputs The outputs are 3 numpy.float32 tensors in various shapes as the following. You can refer to postprocess() function to learn how to decode boxes, classes, and confidence scores. Tensor Shape Data Type Data Type Description 0 (1, 45, 80, 80) float32 NCHW 1 (1, 45, 40, 40) float32 NCHW 2 (1, 45, 20, 20) float32 NCHW Pre/Post processing furiosa.models.vision.YOLOv5m class provides preprocess and postprocess methods. preprocess method converts input images to input tensors, and postprocess method converts model output tensors to a list of bounding boxes, scores and labels. You can find examples at YOLOv5m Usage . furiosa.models.vision.YOLOv5m.preprocess Preprocess a batch of images in numpy Parameters: Name Type Description Default images Sequence [ np . ndarray ] Color images have (NCHW: Batch, Channel, Height, Width) dimensions. required input_color_format str 'rgb' (Red, Green, Blue) or 'bgr' (Blue, Green, Red). required Returns: Type Description Tuple [ np . ndarray , List [ Dict [ str , Any ]]] a pre-processed image, scales and padded sizes(width,height) per images. The first element is a preprocessed image, and the second element is a dictionary object to be used for postprocess. 'scale' key of the returned dict has a rescaled ratio per width(=target/width) and height(=target/height), and the 'pad' key has padded width and height pixels. Specially, the last dictionary element of returing tuple will be passed to postprocessing as a parameter to calculate predicted coordinates on normalized coordinates back to an input image coordinator. furiosa.models.vision.ResNet34.postprocess Convert the outputs of this model to a list of bounding boxes, scores and labels Parameters: Name Type Description Default model_outputs Sequence [ np . array ] P3/8, P4/16, P5/32 features from yolov5l model. required context Dict [ str , Any ] A configuration for each image generated by the preprocessor. For example, it could be the reduction ratio of the image, the actual image width and height. required conf_threshold float Confidence score threshold. The default to 0.25 required iou_thres float IoU threshold value for the NMS processing. The default to 0.45. 0.45 Returns: Type Description List [ List [ ObjectDetectionResult ]] Detected Bounding Box and its score and label represented as ObjectDetectionResult . To learn more about ObjectDetectionResult , 'Definition of ObjectDetectionResult' can be found below. Definition of ObjectDetectionResult Source code in furiosa/models/vision/postprocess.py 32 33 34 35 36 37 38 39 40 @dataclass class LtrbBoundingBox : left : float top : float right : float bottom : float def __iter__ ( self ) -> Iterator [ float ]: return iter ([ self . left , self . top , self . right , self . bottom ]) Source code in furiosa/models/vision/postprocess.py 89 90 91 92 93 94 @dataclass class ObjectDetectionResult : boundingbox : LtrbBoundingBox score : float label : str index : int","title":"YOLOv5m"},{"location":"models/yolov5m/#yolov5m","text":"YOLOv5 is the one of the most popular object detection models developed by Ultralytics . You can find more details at https://github.com/ultralytics/yolov5.","title":"YOLOv5m"},{"location":"models/yolov5m/#overall","text":"Framework: PyTorch Model format: ONNX Model task: Object Detection Source: This model is originated from https://github.com/ultralytics/yolov5.","title":"Overall"},{"location":"models/yolov5m/#_1","text":"Usage import cv2 import numpy as np from furiosa.models.vision import YOLOv5m from furiosa.runtime import session yolov5m = YOLOv5m . load () with session . create ( yolov5m ) as sess : image = cv2 . imread ( \"tests/assets/yolov5-test.jpg\" ) inputs , context = yolov5m . preprocess ([ image ]) output = sess . run ( np . expand_dims ( inputs [ 0 ], axis = 0 )) . numpy () yolov5m . postprocess ( output , context = context )","title":""},{"location":"models/yolov5m/#model-inputs","text":"The input is a 3-channel image of 640, 640 (height, width). Data Type: numpy.uint8 Tensor Shape: [1, 640, 640, 3] Memory Format: NHWC, where N - batch size H - image height W - image width C - number of channels Optimal Batch Size: 1","title":"Model inputs"},{"location":"models/yolov5m/#outputs","text":"The outputs are 3 numpy.float32 tensors in various shapes as the following. You can refer to postprocess() function to learn how to decode boxes, classes, and confidence scores. Tensor Shape Data Type Data Type Description 0 (1, 45, 80, 80) float32 NCHW 1 (1, 45, 40, 40) float32 NCHW 2 (1, 45, 20, 20) float32 NCHW","title":"Outputs"},{"location":"models/yolov5m/#prepost-processing","text":"furiosa.models.vision.YOLOv5m class provides preprocess and postprocess methods. preprocess method converts input images to input tensors, and postprocess method converts model output tensors to a list of bounding boxes, scores and labels. You can find examples at YOLOv5m Usage .","title":"Pre/Post processing"},{"location":"models/yolov5m/#furiosamodelsvisionyolov5mpreprocess","text":"Preprocess a batch of images in numpy Parameters: Name Type Description Default images Sequence [ np . ndarray ] Color images have (NCHW: Batch, Channel, Height, Width) dimensions. required input_color_format str 'rgb' (Red, Green, Blue) or 'bgr' (Blue, Green, Red). required Returns: Type Description Tuple [ np . ndarray , List [ Dict [ str , Any ]]] a pre-processed image, scales and padded sizes(width,height) per images. The first element is a preprocessed image, and the second element is a dictionary object to be used for postprocess. 'scale' key of the returned dict has a rescaled ratio per width(=target/width) and height(=target/height), and the 'pad' key has padded width and height pixels. Specially, the last dictionary element of returing tuple will be passed to postprocessing as a parameter to calculate predicted coordinates on normalized coordinates back to an input image coordinator.","title":"furiosa.models.vision.YOLOv5m.preprocess"},{"location":"models/yolov5m/#furiosamodelsvisionresnet34postprocess","text":"Convert the outputs of this model to a list of bounding boxes, scores and labels Parameters: Name Type Description Default model_outputs Sequence [ np . array ] P3/8, P4/16, P5/32 features from yolov5l model. required context Dict [ str , Any ] A configuration for each image generated by the preprocessor. For example, it could be the reduction ratio of the image, the actual image width and height. required conf_threshold float Confidence score threshold. The default to 0.25 required iou_thres float IoU threshold value for the NMS processing. The default to 0.45. 0.45 Returns: Type Description List [ List [ ObjectDetectionResult ]] Detected Bounding Box and its score and label represented as ObjectDetectionResult . To learn more about ObjectDetectionResult , 'Definition of ObjectDetectionResult' can be found below. Definition of ObjectDetectionResult Source code in furiosa/models/vision/postprocess.py 32 33 34 35 36 37 38 39 40 @dataclass class LtrbBoundingBox : left : float top : float right : float bottom : float def __iter__ ( self ) -> Iterator [ float ]: return iter ([ self . left , self . top , self . right , self . bottom ]) Source code in furiosa/models/vision/postprocess.py 89 90 91 92 93 94 @dataclass class ObjectDetectionResult : boundingbox : LtrbBoundingBox score : float label : str index : int","title":"furiosa.models.vision.ResNet34.postprocess"}]}